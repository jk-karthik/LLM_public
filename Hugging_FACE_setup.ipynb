{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Better Way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer,pipeline,LogitsProcessor\n",
    "import torch\n",
    "import torch\n",
    "import torchaudio\n",
    "import torchgen\n",
    "import torchvision\n",
    "# import bitsandbytes\n",
    "import accelerate\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.functional import softmax\n",
    "import warnings\n",
    "\n",
    "# To ignore all warnings:\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook: Testing Ollama Functionality\n",
    "\n",
    "**Objective.** Explore Llama 3.2B model functionality provided through HuggingFace\n",
    "<!-- **Data.** <sources, date range, N obs, key filters>   -->\n",
    "<!-- **Method.** <models/algorithms + key hyperparams>   -->\n",
    "\n",
    "\n",
    "<!-- **Takeaways.** <what to do with the result>  \n",
    "**Limitations.** <assumptions, data gaps, caveats>  \n",
    "**Repro.** `python -m pip install -r requirements.txt`  \n",
    "`papermill this.ipynb out.ipynb -p seed 42 -p start 2018-01-01 -p end 2025-08-01` -->\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print(tokenizer.chat_template)\n",
    "\n",
    "# def get_probability_distribution(prompt):\n",
    "#     input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(\"mps\")\n",
    "#     with torch.no_grad():\n",
    "#         outputs = model(input_ids)\n",
    "#         logits = outputs.logits[:, -1, :]\n",
    "#         probabilities = F.softmax(logits, dim=-1)\n",
    "#     return {\"probabilities\": probabilities, \"input_ids\": input_ids}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PipeLine Text generation, Forcing answers using chat templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d66bc603eea43e9b6f17d8b02b6a383",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-3B-Instruct\")\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    tokenizer= tokenizer,\n",
    "    model=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device=\"mps\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a probability expert who always gives estimate of events\"},\n",
    "        {\"role\": \"user\", \"content\": \"What is the probability of having an economic recession in the United States this year (2025)?Just give me an estimate and nothing else with no additional text.\"}\n",
    "    ]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False, # Return a string\n",
    "        add_generation_prompt=True \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\n"
     ]
    }
   ],
   "source": [
    "outputs = pipe(\n",
    "        prompt,\n",
    "        max_new_tokens=100,\n",
    "        do_sample=False, # Need for temperature = 0.0\n",
    "        temperature=0.0,\n",
    "        # top_p=1.0,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Response:\n",
      "20-30%\n"
     ]
    }
   ],
   "source": [
    "# 7. Print the generated text\n",
    "\n",
    "if outputs and isinstance(outputs, list) and 'generated_text' in outputs[0]:\n",
    "\n",
    "    full_text = outputs[0]['generated_text']\n",
    "    \n",
    "\n",
    "    # For Llama 3 Instruct, the response starts after \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "    response_start_marker = \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "    response_start_index = full_text.rfind(response_start_marker)\n",
    "\n",
    "    if response_start_index != -1:\n",
    "        response = full_text[response_start_index + len(response_start_marker):].strip()\n",
    "        print(\"Generated Response:\")\n",
    "        print(response)\n",
    "    else:\n",
    "      \n",
    "        if full_text.startswith(prompt):\n",
    "             response = full_text[len(prompt):].strip()\n",
    "             print(\"Generated Response (fallback extraction):\")\n",
    "             print(response)\n",
    "        else:\n",
    "             print(\"Generated Text (could not isolate response):\")\n",
    "             print(full_text) # Print the whole thing \n",
    "else:\n",
    "    print(\"No output or unexpected output format:\", outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complement Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a probability expert who always gives estimate of events\"},\n",
    "        {\"role\": \"user\", \"content\": \"What is the probability of not having an economic recession in the United States this year (2025)?Just give me an estimate and nothing else with no additional text.\"}\n",
    "    ]\n",
    "\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False, # Return a string\n",
    "        add_generation_prompt=True \n",
    ")\n",
    "\n",
    "outputs = pipe(\n",
    "        prompt,\n",
    "        max_new_tokens=100,\n",
    "        do_sample=False, # Need for temperature = 0.0\n",
    "        temperature=0.0,\n",
    "        # top_p=1.0,\n",
    "    )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Response:\n",
      "20-30%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if outputs and isinstance(outputs, list) and 'generated_text' in outputs[0]:\n",
    "\n",
    "    full_text = outputs[0]['generated_text']\n",
    "    \n",
    "\n",
    "    # For Llama 3 Instruct, the response starts after \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "    response_start_marker = \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "    response_start_index = full_text.rfind(response_start_marker)\n",
    "\n",
    "    if response_start_index != -1:\n",
    "        response = full_text[response_start_index + len(response_start_marker):].strip()\n",
    "        print(\"Generated Response:\")\n",
    "        print(response)\n",
    "    else:\n",
    "      \n",
    "        if full_text.startswith(prompt):\n",
    "             response = full_text[len(prompt):].strip()\n",
    "             print(\"Generated Response (fallback extraction):\")\n",
    "             print(response)\n",
    "        else:\n",
    "             print(\"Generated Text (could not isolate response):\")\n",
    "             print(full_text) # Print the whole thing \n",
    "else:\n",
    "    print(\"No output or unexpected output format:\", outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Sensitivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    # Added \"probabilistic\" to the system message\n",
    "        {\"role\": \"system\", \"content\": \"You are a probability expert who only answers in terms of probabilty estimates, with no additional text.\"},\n",
    "        {\"role\": \"user\", \"content\": \"What is the probability of an economic recession in the United States this year (2025)?\"}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Response:\n",
      "0.35%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False, # Return a string\n",
    "        add_generation_prompt=True \n",
    ")\n",
    "\n",
    "\n",
    "outputs = pipe(\n",
    "        prompt,\n",
    "        max_new_tokens=100,\n",
    "        do_sample=False, # Need for temperature = 0.0\n",
    "        temperature=0.0,\n",
    "        # top_p=1.0,\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "# 7. Print the generated text\n",
    "\n",
    "if outputs and isinstance(outputs, list) and 'generated_text' in outputs[0]:\n",
    "\n",
    "    full_text = outputs[0]['generated_text']\n",
    "    \n",
    "\n",
    "    # For Llama 3 Instruct, the response starts after \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "    response_start_marker = \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "    response_start_index = full_text.rfind(response_start_marker)\n",
    "\n",
    "    if response_start_index != -1:\n",
    "        response = full_text[response_start_index + len(response_start_marker):].strip()\n",
    "        print(\"Generated Response:\")\n",
    "        print(response)\n",
    "    else:\n",
    "      \n",
    "        if full_text.startswith(prompt):\n",
    "             response = full_text[len(prompt):].strip()\n",
    "             print(\"Generated Response (fallback extraction):\")\n",
    "             print(response)\n",
    "        else:\n",
    "             print(\"Generated Text (could not isolate response):\")\n",
    "             print(full_text) # Print the whole thing if unsure\n",
    "else:\n",
    "    print(\"No output or unexpected output format:\", outputs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing different questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"What is the probability of an economic recession in the United States this year (2025)?\",\n",
    "    \"How likely is it that interest rates will be cut by the Federal Reserve in 2025?\",\n",
    "    \"What are the chances of a major stock market correction in 2025?\",\n",
    "    \"What is the probability of inflation falling below 2% in the US in 2025?\",\n",
    "    \"How likely is it that unemployment will rise above 6% this year?\"\n",
    "]\n",
    "\n",
    "# Format all prompts using chat template\n",
    "prompts = []\n",
    "for question in questions:\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are a probability expert who only answers in terms of probabilty estimates, with no additional text.\"},\n",
    "        {\"role\": \"user\", \"content\": question}\n",
    "    ]\n",
    "    formatted_prompt = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True\n",
    "    )\n",
    "    prompts.append(formatted_prompt)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# Generate responses in batch\n",
    "outputs = pipe(\n",
    "    prompts,\n",
    "    max_new_tokens=100,\n",
    "    do_sample=False,\n",
    "    temperature=0.0,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated Responses:\n",
      "\n",
      "Q1: What is the probability of an economic recession in the United States this year (2025)?\n",
      "A1: 0.35%\n",
      "\n",
      "Q2: How likely is it that interest rates will be cut by the Federal Reserve in 2025?\n",
      "A2: Approximately 34.7%\n",
      "\n",
      "Q3: What are the chances of a major stock market correction in 2025?\n",
      "A3: Approximately 34.7%\n",
      "\n",
      "Q4: What is the probability of inflation falling below 2% in the US in 2025?\n",
      "A4: I estimate the probability of inflation falling below 2% in the US in 2025 at 12.4%.\n",
      "\n",
      "Q5: How likely is it that unemployment will rise above 6% this year?\n",
      "A5: Approximately 34.6%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Extract and print responses\n",
    "response_start_marker = \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n",
    "\n",
    "print(\"\\nGenerated Responses:\\n\")\n",
    "for idx, output in enumerate(outputs):\n",
    "    full_text = output[0]['generated_text']\n",
    "    response_start_index = full_text.rfind(response_start_marker)\n",
    "\n",
    "    print(f\"Q{idx+1}: {questions[idx]}\")\n",
    "    if response_start_index != -1:\n",
    "        response = full_text[response_start_index + len(response_start_marker):].strip()\n",
    "        print(f\"A{idx+1}: {response}\\n\")\n",
    "    else:\n",
    "        # Fallback\n",
    "        if full_text.startswith(prompts[idx]):\n",
    "            response = full_text[len(prompts[idx]):].strip()\n",
    "            print(f\"A{idx+1}: {response} (extracted using fallback)\\n\")\n",
    "        else:\n",
    "            print(f\"A{idx+1}: Could not parse response. Raw output:\\n{full_text}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "del pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use generation to get the Logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e0e0c4c05c848e4a5454f50acd34216",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import torch\n",
    "# from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_id = \"meta-llama/Llama-3.2-3B-Instruct\"          # any Llama‑2/3 chat checkpoint works\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, padding_side=\"left\")\n",
    "model     = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16).to(\"mps\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    # Added \"probabilistic\" to the system message\n",
    "        {\"role\": \"system\", \"content\": \"You are a probability expert who only answers in terms of probabilty estimates, with no additional text.\"},\n",
    "        {\"role\": \"user\", \"content\": \"What is the probability of having an economic recession in the United States this year (2025)?\"}\n",
    "    ]\n",
    "\n",
    "#  Format the conversation with the model‑specific chat template\n",
    "prompt_ids = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,          \n",
    "    return_tensors=\"pt\"\n",
    ").to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "From v4.47 onwards, when a model cache is to be returned, `generate` will return a `Cache` instance instead by default (as opposed to the legacy tuple of tuples format). If you want to keep returning the legacy format, please set `return_legacy_cache=True`.\n"
     ]
    }
   ],
   "source": [
    "# Generate while asking for the raw scores (logits) at every step\n",
    "out = model.generate(\n",
    "    prompt_ids,\n",
    "    max_new_tokens=40,\n",
    "    return_dict_in_generate=True,\n",
    "    output_scores=True,      # gives one logits tensor per generated step\n",
    "    do_sample=False,\n",
    "    temperature=0.0,                 \n",
    ")\n",
    "\n",
    "\n",
    "#  Convert those logits to token‑wise probabilities\n",
    "#     compute_transition_scores aligns each score tensor with the token it chose\n",
    "log_probs = model.compute_transition_scores(\n",
    "    out.sequences, out.scores, normalize_logits=True         # True → log p_i, not raw logits\n",
    ")     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           I  0.1900\n",
      "   Ġestimate  0.2939\n",
      "        Ġthe  0.5681\n",
      "Ġprobability  0.9912\n",
      "         Ġof  0.8675\n",
      "         Ġan  0.4365\n",
      "   Ġeconomic  0.9982\n",
      "  Ġrecession  0.9999\n",
      "         Ġin  0.9962\n",
      "        Ġthe  0.9998\n",
      "     ĠUnited  0.8475\n",
      "     ĠStates  0.9996\n",
      "       Ġthis  0.7722\n",
      "       Ġyear  0.9999\n",
      "          Ġ(  0.8431\n",
      "         202  0.9984\n",
      "           5  0.9834\n",
      "           )  0.9786\n",
      "         Ġto  0.3735\n",
      "         Ġbe  0.9997\n",
      "     Ġaround  0.4796\n",
      "           Ġ  0.9979\n",
      "          12  0.0896\n",
      "           .  0.3700\n",
      "           4  0.2632\n",
      "          %.  0.8635\n",
      "  <|eot_id|>  0.9793\n"
     ]
    }
   ],
   "source": [
    "   \n",
    "gen_ids   = out.sequences[0, prompt_ids.shape[-1]:]          # only the new tokens\n",
    "probs     = log_probs[0].exp()                               # p_i = e^{log p_i}\n",
    "tokens    = tokenizer.convert_ids_to_tokens(gen_ids)\n",
    "\n",
    "for tok, p in zip(tokens, probs.tolist()):\n",
    "    print(f\"{tok:>12s}  {p:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I estimate the probability of an economic recession in the United States this year (2025) to be around 12.4%.<|eot_id|>'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(gen_ids)  # decode the generated tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### above logits for each generation setp(5 steps above) is conditioned on previous input+ generated tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 27])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# one logit tensor per generated step\n",
    "log_probs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{{- bos_token }}\\n{%- if custom_tools is defined %}\\n    {%- set tools = custom_tools %}\\n{%- endif %}\\n{%- if not tools_in_user_message is defined %}\\n    {%- set tools_in_user_message = true %}\\n{%- endif %}\\n{%- if not date_string is defined %}\\n    {%- if strftime_now is defined %}\\n        {%- set date_string = strftime_now(\"%d %b %Y\") %}\\n    {%- else %}\\n        {%- set date_string = \"26 Jul 2024\" %}\\n    {%- endif %}\\n{%- endif %}\\n{%- if not tools is defined %}\\n    {%- set tools = none %}\\n{%- endif %}\\n\\n{#- This block extracts the system message, so we can slot it into the right place. #}\\n{%- if messages[0][\\'role\\'] == \\'system\\' %}\\n    {%- set system_message = messages[0][\\'content\\']|trim %}\\n    {%- set messages = messages[1:] %}\\n{%- else %}\\n    {%- set system_message = \"\" %}\\n{%- endif %}\\n\\n{#- System message #}\\n{{- \"<|start_header_id|>system<|end_header_id|>\\\\n\\\\n\" }}\\n{%- if tools is not none %}\\n    {{- \"Environment: ipython\\\\n\" }}\\n{%- endif %}\\n{{- \"Cutting Knowledge Date: December 2023\\\\n\" }}\\n{{- \"Today Date: \" + date_string + \"\\\\n\\\\n\" }}\\n{%- if tools is not none and not tools_in_user_message %}\\n    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\\n    {{- \\'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.\\' }}\\n    {{- \"Do not use variables.\\\\n\\\\n\" }}\\n    {%- for t in tools %}\\n        {{- t | tojson(indent=4) }}\\n        {{- \"\\\\n\\\\n\" }}\\n    {%- endfor %}\\n{%- endif %}\\n{{- system_message }}\\n{{- \"<|eot_id|>\" }}\\n\\n{#- Custom tools are passed in a user message with some extra guidance #}\\n{%- if tools_in_user_message and not tools is none %}\\n    {#- Extract the first user message so we can plug it in here #}\\n    {%- if messages | length != 0 %}\\n        {%- set first_user_message = messages[0][\\'content\\']|trim %}\\n        {%- set messages = messages[1:] %}\\n    {%- else %}\\n        {{- raise_exception(\"Cannot put tools in the first user message when there\\'s no first user message!\") }}\\n{%- endif %}\\n    {{- \\'<|start_header_id|>user<|end_header_id|>\\\\n\\\\n\\' -}}\\n    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\\n    {{- \"with its proper arguments that best answers the given prompt.\\\\n\\\\n\" }}\\n    {{- \\'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.\\' }}\\n    {{- \"Do not use variables.\\\\n\\\\n\" }}\\n    {%- for t in tools %}\\n        {{- t | tojson(indent=4) }}\\n        {{- \"\\\\n\\\\n\" }}\\n    {%- endfor %}\\n    {{- first_user_message + \"<|eot_id|>\"}}\\n{%- endif %}\\n\\n{%- for message in messages %}\\n    {%- if not (message.role == \\'ipython\\' or message.role == \\'tool\\' or \\'tool_calls\\' in message) %}\\n        {{- \\'<|start_header_id|>\\' + message[\\'role\\'] + \\'<|end_header_id|>\\\\n\\\\n\\'+ message[\\'content\\'] | trim + \\'<|eot_id|>\\' }}\\n    {%- elif \\'tool_calls\\' in message %}\\n        {%- if not message.tool_calls|length == 1 %}\\n            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\\n        {%- endif %}\\n        {%- set tool_call = message.tool_calls[0].function %}\\n        {{- \\'<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n\\' -}}\\n        {{- \\'{\"name\": \"\\' + tool_call.name + \\'\", \\' }}\\n        {{- \\'\"parameters\": \\' }}\\n        {{- tool_call.arguments | tojson }}\\n        {{- \"}\" }}\\n        {{- \"<|eot_id|>\" }}\\n    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\\n        {{- \"<|start_header_id|>ipython<|end_header_id|>\\\\n\\\\n\" }}\\n        {%- if message.content is mapping or message.content is iterable %}\\n            {{- message.content | tojson }}\\n        {%- else %}\\n            {{- message.content }}\\n        {%- endif %}\\n        {{- \"<|eot_id|>\" }}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- \\'<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n\\' }}\\n{%- endif %}\\n'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 3.8594,  6.9062,  5.1875,  ..., -0.8398, -0.8398, -0.8398]],\n",
       "        device='mps:0'),\n",
       " tensor([[ 3.5938,  4.3125,  2.8750,  ..., -2.8438, -2.8438, -2.8438]],\n",
       "        device='mps:0'),\n",
       " tensor([[ 3.6875,  2.2344,  0.0199,  ..., -1.3906, -1.3906, -1.3906]],\n",
       "        device='mps:0'),\n",
       " tensor([[ 0.1982,  0.2773,  0.1855,  ..., -1.1016, -1.1016, -1.1016]],\n",
       "        device='mps:0'),\n",
       " tensor([[ 4.6875,  2.7500,  0.2695,  ..., -0.5742, -0.5742, -0.5781]],\n",
       "        device='mps:0'),\n",
       " tensor([[2.1875, 2.2969, 0.8125,  ..., 1.0703, 1.0703, 1.0703]],\n",
       "        device='mps:0'),\n",
       " tensor([[ 1.2344,  3.9375,  1.6016,  ..., -1.0625, -1.0625, -1.0625]],\n",
       "        device='mps:0'),\n",
       " tensor([[ 2.6719,  4.5312,  1.3516,  ..., -2.4688, -2.4688, -2.4688]],\n",
       "        device='mps:0'),\n",
       " tensor([[ 4.5000,  5.7500,  2.6875,  ..., -0.1484, -0.1484, -0.1484]],\n",
       "        device='mps:0'),\n",
       " tensor([[1.7422, 2.0625, 2.2656,  ..., 1.3828, 1.3828, 1.3828]],\n",
       "        device='mps:0'),\n",
       " tensor([[2.3906, 2.7031, 4.4688,  ..., 0.7148, 0.7148, 0.7148]],\n",
       "        device='mps:0'),\n",
       " tensor([[ 2.3438,  1.6328,  0.1021,  ..., -2.4375, -2.4375, -2.4375]],\n",
       "        device='mps:0'),\n",
       " tensor([[5.1250, 6.5938, 2.0469,  ..., 0.4453, 0.4453, 0.4453]],\n",
       "        device='mps:0'),\n",
       " tensor([[4.0625, 4.3438, 3.6562,  ..., 0.0547, 0.0542, 0.0549]],\n",
       "        device='mps:0'),\n",
       " tensor([[5.8750, 3.5781, 0.5977,  ..., 0.6836, 0.6836, 0.6836]],\n",
       "        device='mps:0'),\n",
       " tensor([[ 1.2266,  1.9453,  2.4844,  ..., -0.7109, -0.7109, -0.7109]],\n",
       "        device='mps:0'),\n",
       " tensor([[ 3.5938,  5.9062,  6.4688,  ..., -1.3359, -1.3359, -1.3359]],\n",
       "        device='mps:0'),\n",
       " tensor([[2.6250, 6.8125, 3.3125,  ..., 2.0312, 2.0312, 2.0312]],\n",
       "        device='mps:0'),\n",
       " tensor([[ 2.8750, -0.3789, -0.7539,  ...,  0.4395,  0.4395,  0.4395]],\n",
       "        device='mps:0'),\n",
       " tensor([[3.5781, 2.7031, 0.1709,  ..., 0.1318, 0.1309, 0.1318]],\n",
       "        device='mps:0'),\n",
       " tensor([[ 3.9844,  2.1406, -1.6016,  ..., -0.6445, -0.6445, -0.6445]],\n",
       "        device='mps:0'),\n",
       " tensor([[ 4.7500,  3.4688, -1.2422,  ...,  0.3965,  0.3965,  0.3965]],\n",
       "        device='mps:0'),\n",
       " tensor([[3.6875, 2.8438, 4.9375,  ..., 1.5078, 1.5078, 1.5078]],\n",
       "        device='mps:0'),\n",
       " tensor([[4.8125, 2.9219, 4.1875,  ..., 1.1484, 1.1484, 1.1484]],\n",
       "        device='mps:0'),\n",
       " tensor([[ 0.6836, -2.5156,  5.5625,  ...,  4.5938,  4.5938,  4.5938]],\n",
       "        device='mps:0'),\n",
       " tensor([[4.9062, 3.3125, 5.2812,  ..., 1.7188, 1.7188, 1.7188]],\n",
       "        device='mps:0'),\n",
       " tensor([[2.6094, 1.5859, 2.6250,  ..., 1.8594, 1.8594, 1.8594]],\n",
       "        device='mps:0'))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 1:\n",
      "             I  P=0.1900\n",
      "        Approx  P=0.1676\n",
      "           Est  P=0.0897\n",
      "             0  P=0.0792\n",
      "     Estimated  P=0.0699\n",
      "\n",
      "Step 2:\n",
      "     Ġestimate  P=0.2939\n",
      "          Ġcan  P=0.2593\n",
      "       Ġcannot  P=0.2289\n",
      "            'm  P=0.0842\n",
      "            'd  P=0.0616\n",
      "\n",
      "Step 3:\n",
      "          Ġthe  P=0.5681\n",
      "            Ġa  P=0.1436\n",
      "           Ġit  P=0.0871\n",
      "             Ġ  P=0.0528\n",
      "         Ġthis  P=0.0466\n",
      "\n",
      "Step 4:\n",
      "  Ġprobability  P=0.9912\n",
      "             Ġ  P=0.0041\n",
      "       Ġannual  P=0.0017\n",
      "   Ġlikelihood  P=0.0006\n",
      "       Ġchance  P=0.0005\n",
      "\n",
      "Step 5:\n",
      "           Ġof  P=0.8675\n",
      "           Ġat  P=0.0807\n",
      "           Ġto  P=0.0381\n",
      "           Ġas  P=0.0096\n",
      "           Ġis  P=0.0017\n",
      "\n",
      "Step 6:\n",
      "           Ġan  P=0.4365\n",
      "       Ġhaving  P=0.3400\n",
      "            Ġa  P=0.2062\n",
      "          Ġthe  P=0.0149\n",
      "    Ġrecession  P=0.0007\n",
      "\n",
      "Step 7:\n",
      "     Ġeconomic  P=0.9982\n",
      "     ĠAmerican  P=0.0006\n",
      "           ĠUS  P=0.0005\n",
      "      Ġeconomy  P=0.0002\n",
      "      economic  P=0.0000\n",
      "\n",
      "Step 8:\n",
      "    Ġrecession  P=0.9999\n",
      "       Ġrecess  P=0.0001\n",
      "     Ġdownturn  P=0.0000\n",
      "   Ġrecurrence  P=0.0000\n",
      "          Ġrec  P=0.0000\n",
      "\n",
      "Step 9:\n",
      "           Ġin  P=0.9962\n",
      "    Ġoccurring  P=0.0019\n",
      "     Ġstarting  P=0.0015\n",
      "    Ġhappening  P=0.0001\n",
      "         Ġthis  P=0.0001\n",
      "\n",
      "Step 10:\n",
      "          Ġthe  P=0.9998\n",
      "             Ġ  P=0.0002\n",
      "           ĠUS  P=0.0000\n",
      "       ĠUnited  P=0.0000\n",
      "            ĠU  P=0.0000\n",
      "\n",
      "Step 11:\n",
      "       ĠUnited  P=0.8475\n",
      "           ĠUS  P=0.1473\n",
      "            ĠU  P=0.0050\n",
      "          ĠUSA  P=0.0002\n",
      "         Ġnext  P=0.0000\n",
      "\n",
      "Step 12:\n",
      "       ĠStates  P=0.9996\n",
      "            ĠS  P=0.0004\n",
      "       Ġstates  P=0.0000\n",
      "           ĠSt  P=0.0000\n",
      "        ĠState  P=0.0000\n",
      "\n",
      "Step 13:\n",
      "         Ġthis  P=0.7722\n",
      "           Ġin  P=0.1184\n",
      "           Ġby  P=0.0718\n",
      "          Ġfor  P=0.0233\n",
      "       Ġbefore  P=0.0041\n",
      "\n",
      "Step 14:\n",
      "         Ġyear  P=0.9999\n",
      "            Ġ(  P=0.0000\n",
      "     Ġcalendar  P=0.0000\n",
      "             Ġ  P=0.0000\n",
      "          year  P=0.0000\n",
      "\n",
      "Step 15:\n",
      "            Ġ(  P=0.8431\n",
      "           Ġat  P=0.0784\n",
      "           Ġto  P=0.0420\n",
      "           Ġas  P=0.0198\n",
      "           Ġis  P=0.0094\n",
      "\n",
      "Step 16:\n",
      "           202  P=0.9984\n",
      "           not  P=0.0001\n",
      "  approximately  P=0.0001\n",
      "        around  P=0.0001\n",
      "            or  P=0.0001\n",
      "\n",
      "Step 17:\n",
      "             5  P=0.9834\n",
      "             4  P=0.0159\n",
      "             3  P=0.0006\n",
      "             6  P=0.0000\n",
      "             0  P=0.0000\n",
      "\n",
      "Step 18:\n",
      "             )  P=0.9786\n",
      "            ):  P=0.0203\n",
      "            ),  P=0.0006\n",
      "          ):ĊĊ  P=0.0003\n",
      "            ).  P=0.0001\n",
      "\n",
      "Step 19:\n",
      "           Ġto  P=0.3735\n",
      "           Ġat  P=0.3735\n",
      "           Ġas  P=0.1374\n",
      "           Ġis  P=0.1070\n",
      "       Ġaround  P=0.0060\n",
      "\n",
      "Step 20:\n",
      "           Ġbe  P=0.9997\n",
      "        Ġrange  P=0.0002\n",
      "         Ġhave  P=0.0000\n",
      "  Ġapproximately  P=0.0000\n",
      "        Ġoccur  P=0.0000\n",
      "\n",
      "Step 21:\n",
      "       Ġaround  P=0.4796\n",
      "  Ġapproximately  P=0.4232\n",
      "             Ġ  P=0.0573\n",
      "             :  P=0.0128\n",
      "      Ġbetween  P=0.0113\n",
      "\n",
      "Step 22:\n",
      "             Ġ  P=0.9979\n",
      "             :  P=0.0008\n",
      "            Ġ.  P=0.0006\n",
      "            Ġ(  P=0.0003\n",
      "           Ġ**  P=0.0001\n",
      "\n",
      "Step 23:\n",
      "            12  P=0.0896\n",
      "            25  P=0.0697\n",
      "            22  P=0.0697\n",
      "            34  P=0.0543\n",
      "             0  P=0.0479\n",
      "\n",
      "Step 24:\n",
      "             .  P=0.3700\n",
      "            %.  P=0.3265\n",
      "             %  P=0.1748\n",
      "             -  P=0.0936\n",
      "            %,  P=0.0184\n",
      "\n",
      "Step 25:\n",
      "             4  P=0.2632\n",
      "             3  P=0.2323\n",
      "             5  P=0.1809\n",
      "             7  P=0.0587\n",
      "             8  P=0.0457\n",
      "\n",
      "Step 26:\n",
      "            %.  P=0.8635\n",
      "             %  P=0.1031\n",
      "            %,  P=0.0295\n",
      "          %.ĊĊ  P=0.0013\n",
      "            %-  P=0.0008\n",
      "\n",
      "Step 27:\n",
      "    <|eot_id|>  P=0.9793\n",
      "      ĠHowever  P=0.0085\n",
      "           ĠĊĊ  P=0.0075\n",
      "         ĠThis  P=0.0024\n",
      "          ĠThe  P=0.0004\n"
     ]
    }
   ],
   "source": [
    "top_k = 5\n",
    "logits_per_step = out.scores  # list of tensors, logits over the whole vocab\n",
    "# interested_setp  = 4\n",
    "\n",
    "for step, logits in enumerate(logits_per_step):\n",
    "    probs = softmax(logits[0], dim=-1)  # Convert logits to probabilities\n",
    "    top_probs, top_ids = probs.topk(top_k)\n",
    "\n",
    "    # if step+1 == interested_setp: print(f\"\\nStep {step+1}:\")\n",
    "    print(f\"\\nStep {step+1}:\")\n",
    "    for i in range(top_k):\n",
    "        token = tokenizer.convert_ids_to_tokens(int(top_ids[i]))\n",
    "        # if step+1 == interested_setp: print(f\"  {token:>12s}  P={top_probs[i]:.4f}\")\n",
    "        print(f\"  {token:>12s}  P={top_probs[i]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "variance of estimate? what does it mean here?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Complement Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    # Added \"probabilistic\" to the system message\n",
    "        {\"role\": \"system\", \"content\": \"You are a probability expert who only answers in terms of probabilty estimates, with no additional text.\"},\n",
    "        {\"role\": \"user\", \"content\": \"What is the probability of not having an economic recession in the United States this year (2025)??\"}\n",
    "    ]\n",
    "\n",
    "#  Format the conversation with the model‑specific chat template\n",
    "prompt_ids = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,          \n",
    "    return_tensors=\"pt\"\n",
    ").to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "out = model.generate(\n",
    "    prompt_ids,\n",
    "    max_new_tokens=40,\n",
    "    return_dict_in_generate=True,\n",
    "    output_scores=True,                  # gives one logits tensor per generated step\n",
    ")\n",
    "\n",
    "\n",
    "#  Convert those logits to token‑wise probabilities\n",
    "#     compute_transition_scores aligns each score tensor with the token it chose\n",
    "log_probs = model.compute_transition_scores(\n",
    "    out.sequences, out.scores, normalize_logits=True         # True → log p_i, not raw logits\n",
    ")     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           0  0.2824\n",
      "           .  1.0000\n",
      "           8  0.0361\n",
      "           -  0.0753\n",
      "           0  1.0000\n",
      "           .  1.0000\n",
      "           9  0.3486\n",
      "  <|eot_id|>  0.5139\n"
     ]
    }
   ],
   "source": [
    "gen_ids   = out.sequences[0, prompt_ids.shape[-1]:]          # only the new tokens\n",
    "probs     = log_probs[0].exp()                               # p_i = e^{log p_i}\n",
    "tokens    = tokenizer.convert_ids_to_tokens(gen_ids)\n",
    "\n",
    "for tok, p in zip(tokens, probs.tolist()):\n",
    "    print(f\"{tok:>12s}  {p:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero SHOT classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae17881cd36744ab90d4ab930b012157",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of LlamaForSequenceClassification were not initialized from the model checkpoint at meta-llama/Llama-3.2-3B-Instruct and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Failed to determine 'entailment' label id from the label2id mapping in the model config. Setting to -1. Define a descriptive label2id mapping in the model config to ensure correct outputs.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-3B-Instruct\")\n",
    "\n",
    "pipe = pipeline(\n",
    "    \"zero-shot-classification\",\n",
    "    tokenizer= tokenizer,\n",
    "    model=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "    model_kwargs={\"torch_dtype\": torch.bfloat16},\n",
    "    device=\"mps\",\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizer was not supporting padding necessary for zero-shot, attempting to use  `pad_token=eos_token`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sequence': 'Will the United States go into economic recession this year (2025)?', 'labels': ['No', 'Yes'], 'scores': [0.872347354888916, 0.1276526153087616]}\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Will the United States go into economic recession this year (2025)?\"\n",
    "candidate_labels = [\"Yes\", \"No\"]\n",
    "\n",
    "result = pipe(prompt, candidate_labels)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sequence': 'Will the New York Mets win over Los Angeles Dodgers tomorrow?', 'labels': ['New York Mets win', 'Los Angeles Dodgers win'], 'scores': [0.5295066833496094, 0.4704933166503906]}\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Will the New York Mets win over Los Angeles Dodgers tomorrow?\"\n",
    "candidate_labels = [\"New York Mets win\", \"Los Angeles Dodgers win\"]\n",
    "\n",
    "result = pipe(prompt, candidate_labels)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sequence': 'Will the New York Mets win over Los Angeles Dodgers tomorrow?', 'labels': ['No', 'Yes'], 'scores': [0.6198165416717529, 0.38018345832824707]}\n"
     ]
    }
   ],
   "source": [
    "candidate_labels = [\"Yes\", \"No\"]\n",
    "\n",
    "result = pipe(prompt, candidate_labels)\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "del pipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate reasoning but conclude with an answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "744a67d053c54cd3978de338ce115eb5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_id = \"meta-llama/Llama-3.2-3B-Instruct\"          # any Llama‑2/3 chat checkpoint works\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, padding_side=\"left\")\n",
    "model     = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16).to(\"mps\")\n",
    "\n",
    "messages = [\n",
    "    # Added \"probabilistic\" to the system message\n",
    "        {\"role\": \"system\", \"content\": \"You are a probability expert who provides reasoning and certainly conclude with a final estimate of event probabilities.\"},\n",
    "        {\"role\": \"user\", \"content\": \"What is the probability of an economic recession in the United States this year (2025)?\"}\n",
    "    ]\n",
    "\n",
    "#  Format the conversation with the model‑specific chat template\n",
    "prompt_ids = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,          \n",
    "    return_tensors=\"pt\"\n",
    ").to(model.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "# Generate while asking for the raw scores (logits) at every step\n",
    "out = model.generate(\n",
    "    prompt_ids,\n",
    "    max_new_tokens=40,\n",
    "    return_dict_in_generate=True,\n",
    "    output_scores=True,                  # gives one logits tensor per generated step\n",
    ")\n",
    "\n",
    "#  Convert those logits to token‑wise probabilities\n",
    "#     compute_transition_scores aligns each score tensor with the token it chose\n",
    "log_probs = model.compute_transition_scores(\n",
    "    out.sequences, out.scores, normalize_logits=True         # True → log p_i, not raw logits\n",
    ")     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          As  0.5300\n",
      "          Ġa  1.0000\n",
      "Ġprobability  1.0000\n",
      "     Ġexpert  1.0000\n",
      "           ,  1.0000\n",
      "          ĠI  1.0000\n",
      "       Ġmust  0.5519\n",
      "  Ġemphasize  1.0000\n",
      "       Ġthat  1.0000\n",
      " Ġpredicting  1.0000\n",
      "   Ġeconomic  1.0000\n",
      "     Ġrecess  1.0000\n",
      "        ions  1.0000\n",
      "         Ġis  0.8411\n",
      " Ġinherently  1.0000\n",
      "  Ġuncertain  1.0000\n",
      "        Ġand  1.0000\n",
      "    Ġsubject  1.0000\n",
      "         Ġto  1.0000\n",
      "    Ġvarious  1.0000\n",
      "    Ġfactors  1.0000\n",
      "           ,  0.8670\n",
      "  Ġincluding  1.0000\n",
      "     Ġunfore  0.7311\n",
      "        seen  1.0000\n",
      "     Ġevents  1.0000\n",
      "        Ġand  0.5519\n",
      "    Ġchanges  0.7390\n",
      "         Ġin  1.0000\n",
      "     Ġglobal  0.6971\n",
      "   Ġeconomic  1.0000\n",
      " Ġconditions  1.0000\n",
      "         .ĊĊ  0.6514\n",
      "        That  1.0000\n",
      "      Ġbeing  1.0000\n",
      "       Ġsaid  1.0000\n",
      "           ,  1.0000\n",
      "      Ġbased  0.3170\n",
      "         Ġon  1.0000\n",
      " Ġhistorical  0.2608\n"
     ]
    }
   ],
   "source": [
    "gen_ids   = out.sequences[0, prompt_ids.shape[-1]:]          # only the new tokens\n",
    "probs     = log_probs[0].exp()                               # p_i = e^{log p_i}\n",
    "tokens    = tokenizer.convert_ids_to_tokens(gen_ids)\n",
    "\n",
    "for tok, p in zip(tokens, probs.tolist()):\n",
    "    print(f\"{tok:>12s}  {p:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Some issues above. It doesnt have to conclude in 40 tokens and can take a lot of time. Context is not strictly enforced\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forcing generation to Yes/No by logits processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nCutting Knowledge Date: December 2023\\nToday Date: 20 Aug 2025\\n\\nYou are a probability expert who provides reasoning and certainly conclude with a final estimate of event probabilities.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nWhat is the probability of an economic recession in the United States this year (2025)?<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(prompt_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YesNoLogitsProcessor(LogitsProcessor):\n",
    "    \"\"\"\n",
    "    A LogitsProcessor that forces the model to predict only \"yes\" or \"no\".\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, tokenizer, yes_token_ids, no_token_ids):\n",
    "        \"\"\"\n",
    "        Initializes the YesNoLogitsProcessor.\n",
    "\n",
    "        Args:\n",
    "            tokenizer:  The tokenizer used by the model.\n",
    "            yes_token_ids: The token IDs for \"yes\" (can be single or multiple tokens).\n",
    "            no_token_ids: The token IDs for \"no\" (can be single or multiple tokens).\n",
    "        \"\"\"\n",
    "        self.tokenizer = tokenizer\n",
    "        self.yes_token_ids = yes_token_ids\n",
    "        self.no_token_ids = no_token_ids\n",
    "        self.allowed_token_ids = list(set(yes_token_ids + no_token_ids)) # Unique allowed\n",
    "\n",
    "    def __call__(self, input_ids, scores):\n",
    "        \"\"\"\n",
    "        Filters the logits to allow only \"yes\" or \"no\" tokens.\n",
    "\n",
    "        Args:\n",
    "            input_ids: torch.LongTensor. The batch of input token ids.\n",
    "            scores: torch.FloatTensor. The model's output logits.\n",
    "\n",
    "        Returns:\n",
    "            torch.FloatTensor: The filtered logits.\n",
    "        \"\"\"\n",
    "        if len(input_ids) == 0:\n",
    "            return scores\n",
    "\n",
    "        # Create a mask to filter out unwanted tokens\n",
    "        mask = torch.ones(scores.shape[-1], dtype=torch.bool, device=scores.device)\n",
    "        mask[self.allowed_token_ids] = False  # Set allowed tokens to False\n",
    "\n",
    "        # Create a large negative number for masking\n",
    "        very_negative_number = torch.finfo(scores.dtype).min\n",
    "        scores[:, mask] = very_negative_number\n",
    "\n",
    "        return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Will the United States go into economic recession this year (2025)?\"\n",
    "question = \"Will the sun rise in the east tomorrow?\"\n",
    "\n",
    "messages = [\n",
    "    # Added \"probabilistic\" to the system message\n",
    "        {\"role\": \"system\", \"content\": \"You are an expert in everything, who answers only in Yes or No\"},\n",
    "        {\"role\": \"user\", \"content\": question}\n",
    "    ]\n",
    "\n",
    "#  Format the conversation with the model‑specific chat template\n",
    "prompt_ids = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,          \n",
    "    return_tensors=\"pt\"\n",
    ").to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes token IDs: [9642, 9891, 14331]\n",
      "No token IDs: [2201, 9173, 2822]\n"
     ]
    }
   ],
   "source": [
    "yes_tokens = [\"yes\", \"Yes\", \"YES\"]\n",
    "no_tokens = [\"no\", \"No\", \"NO\"]\n",
    "yes_token_ids = []\n",
    "no_token_ids = []\n",
    "\n",
    "for y in yes_tokens:\n",
    "    yes_token_ids.extend(tokenizer.encode(y, add_special_tokens=False))\n",
    "for n in no_tokens:\n",
    "    no_token_ids.extend(tokenizer.encode(n, add_special_tokens=False))\n",
    "#Get unique ids\n",
    "yes_token_ids = list(set(yes_token_ids))\n",
    "no_token_ids = list(set(no_token_ids))\n",
    "print(f\"Yes token IDs: {yes_token_ids}\")\n",
    "print(f\"No token IDs: {no_token_ids}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create the LogitsProcessor\n",
    "yes_no_processor = YesNoLogitsProcessor(tokenizer, yes_token_ids, no_token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "out = model.generate(\n",
    "    prompt_ids,\n",
    "    max_new_tokens=1,\n",
    "    return_dict_in_generate=True,\n",
    "    logits_processor=[yes_no_processor],\n",
    "    output_scores=True,                  # gives one logits tensor per generated step\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Yes  1.0000\n"
     ]
    }
   ],
   "source": [
    "log_probs = model.compute_transition_scores(\n",
    "    out.sequences, out.scores, normalize_logits=True         # True → log p_i, not raw logits\n",
    ")     \n",
    "\n",
    "gen_ids   = out.sequences[0, prompt_ids.shape[-1]:]          # only the new tokens\n",
    "probs     = log_probs[0].exp()                               # p_i = e^{log p_i}\n",
    "tokens    = tokenizer.convert_ids_to_tokens(gen_ids)\n",
    "\n",
    "for tok, p in zip(tokens, probs.tolist()):\n",
    "    print(f\"{tok:>12s}  {p:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
