{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/llama/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "import torch\n",
    "import torch\n",
    "# import torchaudio\n",
    "import torchgen\n",
    "# import torchvision\n",
    "# import bitsandbytes\n",
    "import accelerate\n",
    "from torch.nn import functional as F\n",
    "from torch.nn.functional import softmax\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import time\n",
    "import math\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook: Testing Llama Cpp Functionality\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6,8B\n",
    "# model_path=\"/Users/jandh/.cache/huggingface/hub/models--bartowski--Meta-Llama-3.1-8B-Instruct-GGUF/snapshots/bf5b95e96dac0462e2a09145ec66cae9a3f12067/Meta-Llama-3.1-8B-Instruct-Q6_K.gguf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8B perfect\n",
    "model_path = \"/Users/jandh/.cache/huggingface/hub/models--bartowski--Meta-Llama-3.1-8B-Instruct-GGUF/snapshots/bf5b95e96dac0462e2a09145ec66cae9a3f12067/Meta-Llama-3.1-8B-Instruct-Q8_0.gguf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_init_from_model: n_ctx_per_seq (4096) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n"
     ]
    }
   ],
   "source": [
    "llm = Llama(\n",
    "    model_path=model_path,\n",
    "    n_gpu_layers=-1,   # push every layer to the Apple GPU\n",
    "    n_ctx=4096,\n",
    "    logits_all=True,\n",
    "    verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = datetime(2021, 1, 1)\n",
    "end_date = datetime(2023, 12, 1)\n",
    "current_date = start_date\n",
    "date_strs = []\n",
    "while current_date <= end_date:\n",
    "    date_strs.append(current_date.strftime(\"%Y-%m-%d\"))\n",
    "    current_date += relativedelta(months=1)\n",
    "\n",
    "\n",
    "\n",
    "fact_df = pd.DataFrame(columns=['date','question','yes','no','inference_time','event_date'])\n",
    "fact_df = fact_df.set_index(['question','date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = ['Is Queen Elizabeth II alive today?','Is Nancy Pelosi the Speaker of the U.S House of Representatives today?','Is Mike Johnson the Speaker of the U.S House of Representatives today?'\n",
    "             \"Is Kevin McCarthy the Speaker of the U.S House of Representatives today?\",\"Did an earthquake occur in Turkey this Month?\",\n",
    "             \"Did Argentina win the FIFA World Cup this year?\"]\n",
    "events_dates = [\"2022-09-22\",\"2023-01-03\",\"2023-10-25\",\"2023-01-03\",\"2023-02-01\",\"2022-12-18\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_fact_prompt(question,date):\n",
    "    message = [\n",
    "    {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": \n",
    "        f\"Assume today is {date}. Take is as a date cutoff and discard all knowledge after this date strictly\"\n",
    "    \n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": f\"{question}? Answer in a single Word: Yes or No only.\"}\n",
    "    ]\n",
    "    return message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_yes_no(resp):\n",
    "    \"\"\"\n",
    "    Extracts log-probs and computes probabilities for 'Yes' and 'No'\n",
    "    from a llama_cpp response structure.\n",
    "    \"\"\"\n",
    "    # Navigate to the top_logprobs list\n",
    "    top_k = resp[\"choices\"][0][\"logprobs\"][\"content\"][0][\"top_logprobs\"]\n",
    "    \n",
    "    # Build a map: token -> logprob\n",
    "    logp_map = {item[\"token\"].strip(): float(item[\"logprob\"]) for item in top_k}\n",
    "    # Try both capitalizations\n",
    "    lp_yes = logp_map.get(\"Yes\", logp_map.get(\"yes\"))\n",
    "    lp_no  = logp_map.get(\"No\",  logp_map.get(\"no\"))\n",
    "    if lp_yes is None or lp_no is None:\n",
    "        raise ValueError(\"Expected 'Yes' and 'No' tokens in top_logprobs.\")\n",
    "    \n",
    "    # Convert log-probs to normalised probabilities\n",
    "    p_yes = math.exp(lp_yes)\n",
    "    p_no  = math.exp(lp_no)\n",
    "    # total = p_yes + p_no\n",
    "    \n",
    "    return p_yes , p_no\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for event_date,question in zip(events_dates,questions):\n",
    "    # fact_df.loc[question,\"event_date\"] = event_date\n",
    "    for date_str in date_strs:\n",
    "        prompt = generate_fact_prompt(question,date_str)\n",
    "        st = time.time()\n",
    "        resp = llm.create_chat_completion(\n",
    "        messages=prompt,\n",
    "        max_tokens=1,\n",
    "        temperature=0.0,\n",
    "        logprobs=True,\n",
    "        top_logprobs=2\n",
    "        )\n",
    "        \n",
    "\n",
    "        # response = tokenizer.decode(outputs[:, inputs[\"input_ids\"].shape[1]:][0], skip_special_tokens=True)\n",
    "        fact_df.loc[(question, date_str),'inference_time'] = time.time() - st\n",
    "\n",
    "        p_yes,p_no = parse_yes_no(resp)\n",
    "  \n",
    "\n",
    "        # print(f\"\\n\\n{prompt[0]['content']}\\n{prompt[1]['content']}\")\n",
    "        # print( {w: float(probs[tokenizer.encode(w, add_special_tokens=False)[0]])\n",
    "        #     for w in [\"Yes\", \"No\"]})\n",
    "        fact_df.loc[(question, date_str),\"event_date\"] = event_date\n",
    "        fact_df.loc[(question, date_str),\"yes\"] = p_yes\n",
    "        fact_df.loc[(question, date_str),\"no\"] = p_no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fact_df.index = fact_df.index.set_levels(pd.to_datetime(fact_df.index.levels[1]), level='date')\n",
    "\n",
    "\n",
    "fact_df['event_date'] = pd.to_datetime(fact_df['event_date'])\n",
    "\n",
    "\n",
    "\n",
    "unique_questions = fact_df.index.get_level_values('question').unique()\n",
    "for question in unique_questions:\n",
    "    question_df = fact_df.loc[question]\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(question_df.index, question_df['yes'], label='Probability of Yes')\n",
    "    plt.plot(question_df.index, question_df['no'], label='Probability of No')\n",
    "\n",
    "    # Get the event date for the current question\n",
    "    event_date = question_df['event_date'].iloc[0]\n",
    "    plt.axvline(x=event_date, color='r', linestyle='--', label=f'Event Date: {event_date.strftime(\"%Y-%m-%d\")}')\n",
    "\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Probability')\n",
    "    plt.title(f'Model Prediction for: {question}')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def yes_no_probs(resp):\n",
    "\n",
    "#     top_k = resp[\"choices\"][0][\"logprobs\"][\"content\"][0][\"top_logprobs\"]\n",
    "    \n",
    "#     # build {token: logprob}\n",
    "#     logp = {item[\"token\"].strip(): float(item[\"logprob\"]) \n",
    "#             for item in top_k\n",
    "#             if item[\"token\"].strip().lower() in {\"yes\", \"no\"}}\n",
    "    \n",
    "#     yes_lp = logp.get(\"Yes\") or logp.get(\"yes\")\n",
    "#     no_lp  = logp.get(\"No\")  or logp.get(\"no\")\n",
    "    \n",
    "#     if yes_lp is None or no_lp is None:\n",
    "#         raise ValueError(\"Either 'Yes' or 'No' not found in top_logprobs\")\n",
    "\n",
    "#     # convert to plain probabilities\n",
    "#     p_yes = math.exp(yes_lp)\n",
    "#     p_no  = math.exp(no_lp)\n",
    "#     total = p_yes + p_no\n",
    "\n",
    "#     return {\n",
    "#         \"logp\": {\"Yes\": yes_lp, \"No\": no_lp},\n",
    "#         \"prob\": {\"Yes\": p_yes / total, \"No\": p_no / total}\n",
    "#     }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_yes_no(resp):\n",
    "    \"\"\"\n",
    "    Extracts log-probs and computes probabilities for 'Yes' and 'No'\n",
    "    from a llama_cpp response structure.\n",
    "    \"\"\"\n",
    "    # Navigate to the top_logprobs list\n",
    "    top_k = resp[\"choices\"][0][\"logprobs\"][\"content\"][0][\"top_logprobs\"]\n",
    "    \n",
    "    # # Build a map: token -> logprob\n",
    "    # logp_map = {item[\"token\"].strip(): float(item[\"logprob\"]) for item in top_k}\n",
    "    # # Try both capitalizations\n",
    "    # lp_yes = logp_map.get(\"Yes\", logp_map.get(\"yes\"))\n",
    "    # lp_no  = logp_map.get(\"No\",  logp_map.get(\"no\"))\n",
    "    # if lp_yes is None or lp_no is None:\n",
    "    #     raise ValueError(\"Expected 'Yes' and 'No' tokens in top_logprobs.\")\n",
    "    \n",
    "    # Convert log-probs to normalised probabilities\n",
    "    # p_yes = math.exp(lp_yes)\n",
    "    # p_no  = math.exp(lp_no)\n",
    "\n",
    "\n",
    "    p_yes = 0\n",
    "    p_no = 0\n",
    "    for item in top_k:\n",
    "        if item['token'].strip() in ['No','no']:\n",
    "            p_no += math.exp(float(item[\"logprob\"]))\n",
    "        if item['token'].strip() in ['Yes','yes'] :\n",
    "            p_yes += math.exp(float(item[\"logprob\"]))\n",
    "\n",
    "    \n",
    "    # p_yes = math.exp(lp_yes) if lp_yes else 0\n",
    "    # p_no  = math.exp(lp_no) if lp_no else 0 \n",
    "\n",
    "\n",
    "    # total = p_yes + p_no\n",
    "    \n",
    "    return p_yes , p_no"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_fact_prompt_2(date,question):\n",
    "    message = [\n",
    "    {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": (\n",
    "        f\"You have to give a yes or no answer to each question based on your best knowledge and reasoning including and up to {date}. \"\n",
    "        \"Do not provide explanations, details, or any other text. \"\n",
    "    )\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": f\"{question}\"}\n",
    "    ]\n",
    "    return message\n",
    "\n",
    "def generate_fact_prompt_3(date,question):\n",
    "\n",
    "    message1 = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": (\n",
    "            f\"You have to give a yes or no answer to each question based on your best knowledge and reasoning including and up to {date}. \"\n",
    "            \"Give specific factors you consider and end with yes/no and a period. I want no language of uncertainty.\"\n",
    "        )\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": f\"{question}\"}\n",
    "    ]\n",
    "    return message1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>question</th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>$ARB above $1.25 on April 7?</td>\n",
       "      <td>March 31, 2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>$ARB above $1.25 on April 7?</td>\n",
       "      <td>April 30, 2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>$ARB above $1.25 on April 7?</td>\n",
       "      <td>April 30, 2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Another African coup by October 31?</td>\n",
       "      <td>August 31, 2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Another African coup by October 31?</td>\n",
       "      <td>September 30, 2023</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                             question                Date\n",
       "0           0         $ARB above $1.25 on April 7?      March 31, 2023\n",
       "1           1         $ARB above $1.25 on April 7?      April 30, 2023\n",
       "2           2         $ARB above $1.25 on April 7?      April 30, 2023\n",
       "3           3  Another African coup by October 31?     August 31, 2023\n",
       "4           4  Another African coup by October 31?  September 30, 2023"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"precut_clean_llama.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =    2544.10 ms /    90 tokens (   28.27 ms per token,    35.38 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =    2563.65 ms /    91 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 0 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     539.64 ms /    42 tokens (   12.85 ms per token,    77.83 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     546.62 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 89 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     512.77 ms /     1 runs   (  512.77 ms per token,     1.95 tokens per second)\n",
      "llama_perf_context_print:       total time =     514.72 ms /     2 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 2 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 38 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     416.36 ms /    38 tokens (   10.96 ms per token,    91.27 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     417.81 ms /    39 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 3 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 38 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     403.89 ms /    38 tokens (   10.63 ms per token,    94.08 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     406.97 ms /    39 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 4 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 38 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     516.79 ms /    38 tokens (   13.60 ms per token,    73.53 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     522.15 ms /    39 tokens\n",
      "Llama.generate: 85 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     108.98 ms /     1 runs   (  108.98 ms per token,     9.18 tokens per second)\n",
      "llama_perf_context_print:       total time =     111.30 ms /     2 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 5 done\n",
      "Question 6 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     657.08 ms /    42 tokens (   15.64 ms per token,    63.92 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     667.81 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 7 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     524.25 ms /    42 tokens (   12.48 ms per token,    80.11 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     539.47 ms /    43 tokens\n",
      "Llama.generate: 89 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     115.41 ms /     1 runs   (  115.41 ms per token,     8.67 tokens per second)\n",
      "llama_perf_context_print:       total time =     117.15 ms /     2 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 8 done\n",
      "Question 9 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     419.06 ms /    40 tokens (   10.48 ms per token,    95.45 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     436.06 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 10 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     599.88 ms /    40 tokens (   15.00 ms per token,    66.68 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     605.25 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 11 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     601.64 ms /    40 tokens (   15.04 ms per token,    66.48 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     604.14 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 12 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     514.58 ms /    40 tokens (   12.86 ms per token,    77.73 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     515.90 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 13 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     401.44 ms /    40 tokens (   10.04 ms per token,    99.64 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     407.41 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 14 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     425.41 ms /    40 tokens (   10.64 ms per token,    94.03 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     426.66 ms /    41 tokens\n",
      "Llama.generate: 87 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     124.27 ms /     1 runs   (  124.27 ms per token,     8.05 tokens per second)\n",
      "llama_perf_context_print:       total time =     126.26 ms /     2 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 15 done\n",
      "Question 16 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     408.40 ms /    40 tokens (   10.21 ms per token,    97.94 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     410.85 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 17 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     496.80 ms /    40 tokens (   12.42 ms per token,    80.51 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     498.71 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 18 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     400.25 ms /    40 tokens (   10.01 ms per token,    99.94 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     403.29 ms /    41 tokens\n",
      "Llama.generate: 87 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     100.87 ms /     1 runs   (  100.87 ms per token,     9.91 tokens per second)\n",
      "llama_perf_context_print:       total time =     102.01 ms /     2 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 19 done\n",
      "Question 20 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     531.85 ms /    40 tokens (   13.30 ms per token,    75.21 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     533.31 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 21 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     491.15 ms /    40 tokens (   12.28 ms per token,    81.44 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     492.55 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 22 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     404.24 ms /    40 tokens (   10.11 ms per token,    98.95 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     407.27 ms /    41 tokens\n",
      "Llama.generate: 87 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     128.02 ms /     1 runs   (  128.02 ms per token,     7.81 tokens per second)\n",
      "llama_perf_context_print:       total time =     128.83 ms /     2 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 23 done\n",
      "Question 24 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     424.23 ms /    41 tokens (   10.35 ms per token,    96.65 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     425.55 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 25 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     515.24 ms /    41 tokens (   12.57 ms per token,    79.57 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     538.94 ms /    42 tokens\n",
      "Llama.generate: 88 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     114.96 ms /     1 runs   (  114.96 ms per token,     8.70 tokens per second)\n",
      "llama_perf_context_print:       total time =     115.93 ms /     2 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 26 done\n",
      "Question 27 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     453.93 ms /    42 tokens (   10.81 ms per token,    92.52 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     469.10 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 28 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     772.62 ms /    42 tokens (   18.40 ms per token,    54.36 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     777.16 ms /    43 tokens\n",
      "Llama.generate: 89 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     104.90 ms /     1 runs   (  104.90 ms per token,     9.53 tokens per second)\n",
      "llama_perf_context_print:       total time =     106.96 ms /     2 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 29 done\n",
      "Question 30 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     413.84 ms /    40 tokens (   10.35 ms per token,    96.66 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     424.05 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 31 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     395.37 ms /    40 tokens (    9.88 ms per token,   101.17 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     396.42 ms /    41 tokens\n",
      "Llama.generate: 87 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     100.58 ms /     1 runs   (  100.58 ms per token,     9.94 tokens per second)\n",
      "llama_perf_context_print:       total time =     102.35 ms /     2 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 32 done\n",
      "Question 33 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     405.66 ms /    46 tokens (    8.82 ms per token,   113.40 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     408.52 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 34 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     405.53 ms /    46 tokens (    8.82 ms per token,   113.43 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     406.58 ms /    47 tokens\n",
      "Llama.generate: 93 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =      99.10 ms /     1 runs   (   99.10 ms per token,    10.09 tokens per second)\n",
      "llama_perf_context_print:       total time =      99.73 ms /     2 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 35 done\n",
      "Question 36 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     401.64 ms /    40 tokens (   10.04 ms per token,    99.59 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     404.64 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 37 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.16 ms /    40 tokens (    9.93 ms per token,   100.72 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.48 ms /    41 tokens\n",
      "Llama.generate: 87 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     109.50 ms /     1 runs   (  109.50 ms per token,     9.13 tokens per second)\n",
      "llama_perf_context_print:       total time =     110.09 ms /     2 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 38 done\n",
      "Question 39 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     400.73 ms /    40 tokens (   10.02 ms per token,    99.82 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     402.66 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 40 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     402.91 ms /    40 tokens (   10.07 ms per token,    99.28 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     405.01 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 41 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     396.05 ms /    40 tokens (    9.90 ms per token,   101.00 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.69 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 42 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     522.85 ms /    40 tokens (   13.07 ms per token,    76.50 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     524.87 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 43 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     404.76 ms /    40 tokens (   10.12 ms per token,    98.82 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     407.07 ms /    41 tokens\n",
      "Llama.generate: 87 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     102.04 ms /     1 runs   (  102.04 ms per token,     9.80 tokens per second)\n",
      "llama_perf_context_print:       total time =     102.91 ms /     2 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 44 done\n",
      "Question 45 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     408.45 ms /    40 tokens (   10.21 ms per token,    97.93 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     411.77 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 46 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     406.51 ms /    40 tokens (   10.16 ms per token,    98.40 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     407.58 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 47 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     404.87 ms /    40 tokens (   10.12 ms per token,    98.80 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     407.17 ms /    41 tokens\n",
      "Llama.generate: 87 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     101.37 ms /     1 runs   (  101.37 ms per token,     9.86 tokens per second)\n",
      "llama_perf_context_print:       total time =     102.34 ms /     2 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 48 done\n",
      "Question 49 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     396.72 ms /    40 tokens (    9.92 ms per token,   100.83 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.80 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 50 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     407.37 ms /    40 tokens (   10.18 ms per token,    98.19 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     410.51 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 51 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     519.79 ms /    40 tokens (   12.99 ms per token,    76.95 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     522.57 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 52 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     400.64 ms /    39 tokens (   10.27 ms per token,    97.34 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     402.58 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 53 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     407.05 ms /    39 tokens (   10.44 ms per token,    95.81 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     409.09 ms /    40 tokens\n",
      "Llama.generate: 86 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     107.04 ms /     1 runs   (  107.04 ms per token,     9.34 tokens per second)\n",
      "llama_perf_context_print:       total time =     109.13 ms /     2 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 54 done\n",
      "Question 55 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     399.79 ms /    43 tokens (    9.30 ms per token,   107.56 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     402.80 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 56 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     401.17 ms /    43 tokens (    9.33 ms per token,   107.19 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     402.18 ms /    44 tokens\n",
      "Llama.generate: 90 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     101.05 ms /     1 runs   (  101.05 ms per token,     9.90 tokens per second)\n",
      "llama_perf_context_print:       total time =     102.68 ms /     2 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 57 done\n",
      "Question 58 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     401.29 ms /    43 tokens (    9.33 ms per token,   107.15 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     404.40 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 59 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     403.46 ms /    43 tokens (    9.38 ms per token,   106.58 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     406.15 ms /    44 tokens\n",
      "Llama.generate: 90 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     100.40 ms /     1 runs   (  100.40 ms per token,     9.96 tokens per second)\n",
      "llama_perf_context_print:       total time =     100.94 ms /     2 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 60 done\n",
      "Question 61 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     406.98 ms /    46 tokens (    8.85 ms per token,   113.03 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     410.49 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 62 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     401.59 ms /    46 tokens (    8.73 ms per token,   114.55 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     402.78 ms /    47 tokens\n",
      "Llama.generate: 93 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =      99.61 ms /     1 runs   (   99.61 ms per token,    10.04 tokens per second)\n",
      "llama_perf_context_print:       total time =     101.03 ms /     2 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 63 done\n",
      "Question 64 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 45 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     400.98 ms /    45 tokens (    8.91 ms per token,   112.22 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     405.13 ms /    46 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 65 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 45 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     408.10 ms /    45 tokens (    9.07 ms per token,   110.27 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     409.75 ms /    46 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 66 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 45 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     407.91 ms /    45 tokens (    9.06 ms per token,   110.32 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     408.91 ms /    46 tokens\n",
      "Llama.generate: 92 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =      94.08 ms /     1 runs   (   94.08 ms per token,    10.63 tokens per second)\n",
      "llama_perf_context_print:       total time =      95.12 ms /     2 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 67 done\n",
      "Question 68 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 45 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     411.76 ms /    45 tokens (    9.15 ms per token,   109.29 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     415.06 ms /    46 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 69 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 45 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     401.31 ms /    45 tokens (    8.92 ms per token,   112.13 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     402.51 ms /    46 tokens\n",
      "Llama.generate: 92 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =      94.16 ms /     1 runs   (   94.16 ms per token,    10.62 tokens per second)\n",
      "llama_perf_context_print:       total time =      94.68 ms /     2 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 70 done\n",
      "Question 71 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 48 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     404.62 ms /    48 tokens (    8.43 ms per token,   118.63 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     408.58 ms /    49 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 72 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 48 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     403.42 ms /    48 tokens (    8.40 ms per token,   118.98 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     404.51 ms /    49 tokens\n",
      "Llama.generate: 95 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =      93.70 ms /     1 runs   (   93.70 ms per token,    10.67 tokens per second)\n",
      "llama_perf_context_print:       total time =      94.36 ms /     2 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 73 done\n",
      "Question 74 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     401.41 ms /    43 tokens (    9.34 ms per token,   107.12 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     404.40 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 75 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     404.19 ms /    43 tokens (    9.40 ms per token,   106.39 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     405.18 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 76 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     444.57 ms /    43 tokens (   10.34 ms per token,    96.72 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     447.15 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 77 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 90 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     314.84 ms /     1 runs   (  314.84 ms per token,     3.18 tokens per second)\n",
      "llama_perf_context_print:       total time =     315.30 ms /     2 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 78 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 45 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     405.95 ms /    45 tokens (    9.02 ms per token,   110.85 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     406.96 ms /    46 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 79 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 45 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     385.22 ms /    45 tokens (    8.56 ms per token,   116.82 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     387.69 ms /    46 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 80 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 45 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     404.97 ms /    45 tokens (    9.00 ms per token,   111.12 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     405.98 ms /    46 tokens\n",
      "Llama.generate: 92 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =      94.28 ms /     1 runs   (   94.28 ms per token,    10.61 tokens per second)\n",
      "llama_perf_context_print:       total time =      95.31 ms /     2 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 81 done\n",
      "Question 82 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.42 ms /    42 tokens (    9.46 ms per token,   105.68 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     400.74 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 83 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     402.40 ms /    42 tokens (    9.58 ms per token,   104.37 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     404.79 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 84 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     406.59 ms /    43 tokens (    9.46 ms per token,   105.76 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     407.86 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 85 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     400.78 ms /    43 tokens (    9.32 ms per token,   107.29 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     401.72 ms /    44 tokens\n",
      "Llama.generate: 90 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =      99.42 ms /     1 runs   (   99.42 ms per token,    10.06 tokens per second)\n",
      "llama_perf_context_print:       total time =     100.32 ms /     2 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 86 done\n",
      "Question 87 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     394.93 ms /    41 tokens (    9.63 ms per token,   103.82 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     397.94 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 88 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     400.28 ms /    41 tokens (    9.76 ms per token,   102.43 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     401.22 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 89 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     402.91 ms /    41 tokens (    9.83 ms per token,   101.76 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     403.89 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 90 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     383.24 ms /    41 tokens (    9.35 ms per token,   106.98 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     384.18 ms /    42 tokens\n",
      "Llama.generate: 88 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =      98.07 ms /     1 runs   (   98.07 ms per token,    10.20 tokens per second)\n",
      "llama_perf_context_print:       total time =      99.12 ms /     2 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 91 done\n",
      "Question 92 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     391.57 ms /    40 tokens (    9.79 ms per token,   102.15 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     394.59 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 93 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     399.38 ms /    40 tokens (    9.98 ms per token,   100.16 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     400.35 ms /    41 tokens\n",
      "Llama.generate: 87 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =      99.38 ms /     1 runs   (   99.38 ms per token,    10.06 tokens per second)\n",
      "llama_perf_context_print:       total time =     100.48 ms /     2 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 94 done\n",
      "Question 95 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.13 ms /    44 tokens (    9.05 ms per token,   110.52 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.81 ms /    45 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 96 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     401.31 ms /    44 tokens (    9.12 ms per token,   109.64 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     402.34 ms /    45 tokens\n",
      "Llama.generate: 91 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =      98.71 ms /     1 runs   (   98.71 ms per token,    10.13 tokens per second)\n",
      "llama_perf_context_print:       total time =      99.21 ms /     2 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 97 done\n",
      "Question 98 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 50 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     546.54 ms /    50 tokens (   10.93 ms per token,    91.48 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     548.20 ms /    51 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 99 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 50 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     676.20 ms /    50 tokens (   13.52 ms per token,    73.94 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     677.46 ms /    51 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 100 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 50 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     520.42 ms /    50 tokens (   10.41 ms per token,    96.08 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     523.48 ms /    51 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 101 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 50 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     408.92 ms /    50 tokens (    8.18 ms per token,   122.27 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     410.26 ms /    51 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 102 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 50 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     403.75 ms /    50 tokens (    8.07 ms per token,   123.84 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     406.72 ms /    51 tokens\n",
      "Llama.generate: 97 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =      97.13 ms /     1 runs   (   97.13 ms per token,    10.30 tokens per second)\n",
      "llama_perf_context_print:       total time =      98.07 ms /     2 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 103 done\n",
      "Question 104 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     399.07 ms /    40 tokens (    9.98 ms per token,   100.23 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     401.88 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 105 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     406.36 ms /    40 tokens (   10.16 ms per token,    98.44 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     408.99 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 106 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     395.03 ms /    40 tokens (    9.88 ms per token,   101.26 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     397.28 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 107 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 87 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     375.38 ms /     1 runs   (  375.38 ms per token,     2.66 tokens per second)\n",
      "llama_perf_context_print:       total time =     375.83 ms /     2 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 108 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     390.39 ms /    43 tokens (    9.08 ms per token,   110.15 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     393.00 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 109 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     422.12 ms /    43 tokens (    9.82 ms per token,   101.87 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     423.07 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 110 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     396.97 ms /    43 tokens (    9.23 ms per token,   108.32 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     401.00 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 111 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     503.32 ms /    43 tokens (   11.71 ms per token,    85.43 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     505.77 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 112 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     401.38 ms /    44 tokens (    9.12 ms per token,   109.62 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     402.37 ms /    45 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 113 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     403.03 ms /    44 tokens (    9.16 ms per token,   109.17 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     405.72 ms /    45 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 114 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     387.18 ms /    44 tokens (    8.80 ms per token,   113.64 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     388.23 ms /    45 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 115 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     403.88 ms /    44 tokens (    9.18 ms per token,   108.94 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     406.37 ms /    45 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 116 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     394.49 ms /    44 tokens (    8.97 ms per token,   111.54 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     397.24 ms /    45 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 117 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     401.05 ms /    44 tokens (    9.11 ms per token,   109.71 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     403.47 ms /    45 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 118 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 52 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     407.84 ms /    52 tokens (    7.84 ms per token,   127.50 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     409.30 ms /    53 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 119 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 52 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     426.13 ms /    52 tokens (    8.19 ms per token,   122.03 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     427.42 ms /    53 tokens\n",
      "Llama.generate: 99 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =      99.97 ms /     1 runs   (   99.97 ms per token,    10.00 tokens per second)\n",
      "llama_perf_context_print:       total time =     101.02 ms /     2 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 120 done\n",
      "Question 121 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 74 prefix-match hit, remaining 18 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     254.17 ms /    18 tokens (   14.12 ms per token,    70.82 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     260.47 ms /    19 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 122 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     537.66 ms /    44 tokens (   12.22 ms per token,    81.84 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     538.69 ms /    45 tokens\n",
      "Llama.generate: 91 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =      99.08 ms /     1 runs   (   99.08 ms per token,    10.09 tokens per second)\n",
      "llama_perf_context_print:       total time =     100.12 ms /     2 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 123 done\n",
      "Question 124 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     407.38 ms /    44 tokens (    9.26 ms per token,   108.01 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     410.54 ms /    45 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 125 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     390.24 ms /    44 tokens (    8.87 ms per token,   112.75 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     391.37 ms /    45 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 126 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     402.78 ms /    44 tokens (    9.15 ms per token,   109.24 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     403.89 ms /    45 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 127 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.57 ms /    44 tokens (    9.06 ms per token,   110.39 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.56 ms /    45 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 128 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     403.50 ms /    44 tokens (    9.17 ms per token,   109.05 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     405.98 ms /    45 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 129 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     405.44 ms /    44 tokens (    9.21 ms per token,   108.52 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     407.84 ms /    45 tokens\n",
      "Llama.generate: 91 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =      94.25 ms /     1 runs   (   94.25 ms per token,    10.61 tokens per second)\n",
      "llama_perf_context_print:       total time =      95.28 ms /     2 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 130 done\n",
      "Question 131 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     407.97 ms /    44 tokens (    9.27 ms per token,   107.85 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     410.85 ms /    45 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 132 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     644.21 ms /    44 tokens (   14.64 ms per token,    68.30 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     645.35 ms /    45 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 133 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     405.06 ms /    44 tokens (    9.21 ms per token,   108.63 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     407.71 ms /    45 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 134 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 91 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     363.64 ms /     1 runs   (  363.64 ms per token,     2.75 tokens per second)\n",
      "llama_perf_context_print:       total time =     364.06 ms /     2 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 135 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     400.22 ms /    39 tokens (   10.26 ms per token,    97.45 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     402.89 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 136 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     507.18 ms /    39 tokens (   13.00 ms per token,    76.90 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     509.66 ms /    40 tokens\n",
      "Llama.generate: 86 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     100.18 ms /     1 runs   (  100.18 ms per token,     9.98 tokens per second)\n",
      "llama_perf_context_print:       total time =     101.60 ms /     2 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 137 done\n",
      "Question 138 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     388.92 ms /    41 tokens (    9.49 ms per token,   105.42 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     392.40 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 139 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     405.20 ms /    41 tokens (    9.88 ms per token,   101.19 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     407.39 ms /    42 tokens\n",
      "Llama.generate: 88 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     100.99 ms /     1 runs   (  100.99 ms per token,     9.90 tokens per second)\n",
      "llama_perf_context_print:       total time =     102.06 ms /     2 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 140 done\n",
      "Question 141 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     431.56 ms /    46 tokens (    9.38 ms per token,   106.59 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     435.07 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 142 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     402.94 ms /    46 tokens (    8.76 ms per token,   114.16 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     404.32 ms /    47 tokens\n",
      "Llama.generate: 93 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =      92.80 ms /     1 runs   (   92.80 ms per token,    10.78 tokens per second)\n",
      "llama_perf_context_print:       total time =      93.32 ms /     2 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 143 done\n",
      "Question 144 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     404.12 ms /    46 tokens (    8.79 ms per token,   113.83 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     407.37 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 145 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     404.61 ms /    46 tokens (    8.80 ms per token,   113.69 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     407.22 ms /    47 tokens\n",
      "Llama.generate: 93 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =      98.24 ms /     1 runs   (   98.24 ms per token,    10.18 tokens per second)\n",
      "llama_perf_context_print:       total time =      98.64 ms /     2 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 146 done\n",
      "Question 147 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     406.13 ms /    41 tokens (    9.91 ms per token,   100.95 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     409.15 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 148 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     407.85 ms /    41 tokens (    9.95 ms per token,   100.53 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     410.27 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 149 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     522.68 ms /    41 tokens (   12.75 ms per token,    78.44 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     523.63 ms /    42 tokens\n",
      "Llama.generate: 88 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =      99.42 ms /     1 runs   (   99.42 ms per token,    10.06 tokens per second)\n",
      "llama_perf_context_print:       total time =     100.00 ms /     2 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 150 done\n",
      "Question 151 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 50 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     579.26 ms /    50 tokens (   11.59 ms per token,    86.32 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     582.42 ms /    51 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 152 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 50 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     492.91 ms /    50 tokens (    9.86 ms per token,   101.44 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     494.07 ms /    51 tokens\n",
      "Llama.generate: 97 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     100.77 ms /     1 runs   (  100.77 ms per token,     9.92 tokens per second)\n",
      "llama_perf_context_print:       total time =     101.19 ms /     2 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 153 done\n",
      "Question 154 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 74 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     374.98 ms /    17 tokens (   22.06 ms per token,    45.34 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     376.32 ms /    18 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 155 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     593.42 ms /    43 tokens (   13.80 ms per token,    72.46 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     594.50 ms /    44 tokens\n",
      "Llama.generate: 90 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =      99.64 ms /     1 runs   (   99.64 ms per token,    10.04 tokens per second)\n",
      "llama_perf_context_print:       total time =     100.62 ms /     2 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 156 done\n",
      "Question 157 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 45 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     393.89 ms /    45 tokens (    8.75 ms per token,   114.24 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     396.67 ms /    46 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 158 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 45 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     406.24 ms /    45 tokens (    9.03 ms per token,   110.77 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     408.82 ms /    46 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 159 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 45 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     405.59 ms /    45 tokens (    9.01 ms per token,   110.95 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     408.06 ms /    46 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 160 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 92 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     291.58 ms /     1 runs   (  291.58 ms per token,     3.43 tokens per second)\n",
      "llama_perf_context_print:       total time =     291.90 ms /     2 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 161 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     399.00 ms /    39 tokens (   10.23 ms per token,    97.74 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     401.53 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 162 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     394.14 ms /    39 tokens (   10.11 ms per token,    98.95 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     395.09 ms /    40 tokens\n",
      "Llama.generate: 86 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =      98.66 ms /     1 runs   (   98.66 ms per token,    10.14 tokens per second)\n",
      "llama_perf_context_print:       total time =      99.51 ms /     2 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 163 done\n",
      "Question 164 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     400.22 ms /    39 tokens (   10.26 ms per token,    97.45 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     403.29 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 165 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     391.38 ms /    39 tokens (   10.04 ms per token,    99.65 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     392.34 ms /    40 tokens\n",
      "Llama.generate: 86 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =      99.95 ms /     1 runs   (   99.95 ms per token,    10.01 tokens per second)\n",
      "llama_perf_context_print:       total time =     101.00 ms /     2 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 166 done\n",
      "Question 167 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     405.96 ms /    41 tokens (    9.90 ms per token,   101.00 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     407.60 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 168 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     401.31 ms /    41 tokens (    9.79 ms per token,   102.17 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     403.77 ms /    42 tokens\n",
      "Llama.generate: 88 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =      98.41 ms /     1 runs   (   98.41 ms per token,    10.16 tokens per second)\n",
      "llama_perf_context_print:       total time =      99.48 ms /     2 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 169 done\n",
      "Question 170 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     400.59 ms /    46 tokens (    8.71 ms per token,   114.83 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     403.91 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 171 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     403.49 ms /    46 tokens (    8.77 ms per token,   114.00 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     404.54 ms /    47 tokens\n",
      "Llama.generate: 93 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =      98.88 ms /     1 runs   (   98.88 ms per token,    10.11 tokens per second)\n",
      "llama_perf_context_print:       total time =      99.31 ms /     2 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 172 done\n",
      "Question 173 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     408.36 ms /    41 tokens (    9.96 ms per token,   100.40 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     411.05 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 174 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     386.40 ms /    41 tokens (    9.42 ms per token,   106.11 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     387.31 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 175 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     401.54 ms /    41 tokens (    9.79 ms per token,   102.11 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     404.08 ms /    42 tokens\n",
      "Llama.generate: 88 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     100.01 ms /     1 runs   (  100.01 ms per token,    10.00 tokens per second)\n",
      "llama_perf_context_print:       total time =     101.03 ms /     2 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 176 done\n",
      "Question 177 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 74 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     234.61 ms /    16 tokens (   14.66 ms per token,    68.20 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     236.35 ms /    17 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 178 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     395.36 ms /    42 tokens (    9.41 ms per token,   106.23 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     396.38 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 179 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     387.81 ms /    42 tokens (    9.23 ms per token,   108.30 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     388.70 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 180 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     401.27 ms /    42 tokens (    9.55 ms per token,   104.67 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     403.83 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 181 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     406.19 ms /    42 tokens (    9.67 ms per token,   103.40 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     408.54 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 182 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     525.61 ms /    42 tokens (   12.51 ms per token,    79.91 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     528.16 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 183 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     399.84 ms /    42 tokens (    9.52 ms per token,   105.04 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     400.88 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 184 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     401.97 ms /    42 tokens (    9.57 ms per token,   104.49 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     404.31 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 185 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     435.63 ms /    40 tokens (   10.89 ms per token,    91.82 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     436.67 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 186 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     400.47 ms /    40 tokens (   10.01 ms per token,    99.88 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     401.38 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 187 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     388.87 ms /    40 tokens (    9.72 ms per token,   102.86 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     389.82 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 188 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     400.36 ms /    40 tokens (   10.01 ms per token,    99.91 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     401.33 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 189 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     402.60 ms /    40 tokens (   10.07 ms per token,    99.35 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     404.88 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 190 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     390.71 ms /    40 tokens (    9.77 ms per token,   102.38 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     391.67 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 191 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     403.18 ms /    40 tokens (   10.08 ms per token,    99.21 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     405.57 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 192 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     406.92 ms /    41 tokens (    9.92 ms per token,   100.76 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     407.95 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 193 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     405.72 ms /    41 tokens (    9.90 ms per token,   101.06 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     406.69 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 194 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     404.34 ms /    41 tokens (    9.86 ms per token,   101.40 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     406.52 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 195 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     394.82 ms /    42 tokens (    9.40 ms per token,   106.38 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     397.36 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 196 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     402.44 ms /    42 tokens (    9.58 ms per token,   104.36 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     404.92 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 197 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 89 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     327.00 ms /     1 runs   (  327.00 ms per token,     3.06 tokens per second)\n",
      "llama_perf_context_print:       total time =     327.49 ms /     2 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 198 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     416.54 ms /    46 tokens (    9.06 ms per token,   110.43 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     419.07 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 199 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     400.65 ms /    46 tokens (    8.71 ms per token,   114.81 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     401.86 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 200 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     403.06 ms /    46 tokens (    8.76 ms per token,   114.13 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     405.63 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 201 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 93 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     318.71 ms /     1 runs   (  318.71 ms per token,     3.14 tokens per second)\n",
      "llama_perf_context_print:       total time =     319.14 ms /     2 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 202 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.79 ms /    42 tokens (    9.49 ms per token,   105.32 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     401.91 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 203 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     400.40 ms /    42 tokens (    9.53 ms per token,   104.90 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     401.53 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 204 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     400.82 ms /    42 tokens (    9.54 ms per token,   104.78 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     401.80 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 205 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     402.55 ms /    42 tokens (    9.58 ms per token,   104.34 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     405.05 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 206 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 75 prefix-match hit, remaining 20 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     218.06 ms /    20 tokens (   10.90 ms per token,    91.72 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     219.73 ms /    21 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 207 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 47 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     552.98 ms /    47 tokens (   11.77 ms per token,    84.99 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     554.06 ms /    48 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 208 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 47 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     403.15 ms /    47 tokens (    8.58 ms per token,   116.58 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     405.72 ms /    48 tokens\n",
      "Llama.generate: 94 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =      98.68 ms /     1 runs   (   98.68 ms per token,    10.13 tokens per second)\n",
      "llama_perf_context_print:       total time =      99.19 ms /     2 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 209 done\n",
      "Question 210 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 47 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     405.71 ms /    47 tokens (    8.63 ms per token,   115.85 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     409.23 ms /    48 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 211 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 47 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     434.33 ms /    47 tokens (    9.24 ms per token,   108.21 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     436.99 ms /    48 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 212 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 47 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     587.68 ms /    47 tokens (   12.50 ms per token,    79.98 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     588.80 ms /    48 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 213 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 47 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     402.84 ms /    47 tokens (    8.57 ms per token,   116.67 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     403.85 ms /    48 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 214 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 47 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     386.85 ms /    47 tokens (    8.23 ms per token,   121.49 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     389.25 ms /    48 tokens\n",
      "Llama.generate: 94 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     100.96 ms /     1 runs   (  100.96 ms per token,     9.90 tokens per second)\n",
      "llama_perf_context_print:       total time =     101.93 ms /     2 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 215 done\n",
      "Question 216 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 47 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     395.57 ms /    47 tokens (    8.42 ms per token,   118.81 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.39 ms /    48 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 217 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 47 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     443.83 ms /    47 tokens (    9.44 ms per token,   105.90 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     444.90 ms /    48 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 218 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 47 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.63 ms /    47 tokens (    8.46 ms per token,   118.20 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     400.38 ms /    48 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 219 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 47 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     504.21 ms /    47 tokens (   10.73 ms per token,    93.22 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     505.37 ms /    48 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 220 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 47 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     404.00 ms /    47 tokens (    8.60 ms per token,   116.34 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     406.83 ms /    48 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 221 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 47 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     548.82 ms /    47 tokens (   11.68 ms per token,    85.64 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     549.93 ms /    48 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 222 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 47 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     399.08 ms /    47 tokens (    8.49 ms per token,   117.77 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     401.77 ms /    48 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 223 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 94 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     324.74 ms /     1 runs   (  324.74 ms per token,     3.08 tokens per second)\n",
      "llama_perf_context_print:       total time =     325.18 ms /     2 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 224 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     406.28 ms /    41 tokens (    9.91 ms per token,   100.92 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     408.93 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 225 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     406.20 ms /    41 tokens (    9.91 ms per token,   100.94 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     408.47 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 226 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     406.88 ms /    41 tokens (    9.92 ms per token,   100.77 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     407.83 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 227 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     396.08 ms /    41 tokens (    9.66 ms per token,   103.51 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     397.02 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 228 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.63 ms /    40 tokens (    9.94 ms per token,   100.60 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.74 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 229 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     401.87 ms /    40 tokens (   10.05 ms per token,    99.54 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     404.04 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 230 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     506.55 ms /    40 tokens (   12.66 ms per token,    78.97 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     507.55 ms /    41 tokens\n",
      "Llama.generate: 87 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =      99.90 ms /     1 runs   (   99.90 ms per token,    10.01 tokens per second)\n",
      "llama_perf_context_print:       total time =     100.75 ms /     2 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 231 done\n",
      "Question 232 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     555.93 ms /    39 tokens (   14.25 ms per token,    70.15 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     557.40 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 233 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     402.18 ms /    39 tokens (   10.31 ms per token,    96.97 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     404.74 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 234 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     405.93 ms /    39 tokens (   10.41 ms per token,    96.08 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     406.97 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 235 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     386.97 ms /    39 tokens (    9.92 ms per token,   100.78 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     389.15 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 236 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     399.91 ms /    39 tokens (   10.25 ms per token,    97.52 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     402.15 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 237 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     499.61 ms /    39 tokens (   12.81 ms per token,    78.06 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     500.59 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 238 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 74 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     228.97 ms /    15 tokens (   15.26 ms per token,    65.51 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     230.08 ms /    16 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 239 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     403.33 ms /    41 tokens (    9.84 ms per token,   101.65 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     404.43 ms /    42 tokens\n",
      "Llama.generate: 88 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     100.67 ms /     1 runs   (  100.67 ms per token,     9.93 tokens per second)\n",
      "llama_perf_context_print:       total time =     101.70 ms /     2 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 240 done\n",
      "Question 241 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     409.69 ms /    41 tokens (    9.99 ms per token,   100.07 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     412.73 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 242 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     406.00 ms /    41 tokens (    9.90 ms per token,   100.98 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     407.35 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 243 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     446.29 ms /    41 tokens (   10.89 ms per token,    91.87 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     447.18 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 244 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 88 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     366.35 ms /     1 runs   (  366.35 ms per token,     2.73 tokens per second)\n",
      "llama_perf_context_print:       total time =     366.85 ms /     2 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 245 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 45 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     403.90 ms /    45 tokens (    8.98 ms per token,   111.41 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     406.26 ms /    46 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 246 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 45 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     528.43 ms /    45 tokens (   11.74 ms per token,    85.16 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     530.99 ms /    46 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 247 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 45 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     399.53 ms /    45 tokens (    8.88 ms per token,   112.63 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     401.98 ms /    46 tokens\n",
      "Llama.generate: 92 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =      94.37 ms /     1 runs   (   94.37 ms per token,    10.60 tokens per second)\n",
      "llama_perf_context_print:       total time =      95.34 ms /     2 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 248 done\n",
      "Question 249 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 45 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     400.52 ms /    45 tokens (    8.90 ms per token,   112.35 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     403.55 ms /    46 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 250 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 45 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     406.47 ms /    45 tokens (    9.03 ms per token,   110.71 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     409.06 ms /    46 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 251 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 45 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     445.44 ms /    45 tokens (    9.90 ms per token,   101.02 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     448.18 ms /    46 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 252 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 45 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     514.23 ms /    45 tokens (   11.43 ms per token,    87.51 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     515.35 ms /    46 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 253 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     388.90 ms /    44 tokens (    8.84 ms per token,   113.14 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     391.22 ms /    45 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 254 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     537.84 ms /    44 tokens (   12.22 ms per token,    81.81 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     538.84 ms /    45 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 255 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     404.50 ms /    44 tokens (    9.19 ms per token,   108.78 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     407.16 ms /    45 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 256 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 91 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     290.51 ms /     1 runs   (  290.51 ms per token,     3.44 tokens per second)\n",
      "llama_perf_context_print:       total time =     290.95 ms /     2 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 257 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     400.79 ms /    42 tokens (    9.54 ms per token,   104.79 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     401.75 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 258 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     400.44 ms /    42 tokens (    9.53 ms per token,   104.88 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     402.62 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 259 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     387.72 ms /    42 tokens (    9.23 ms per token,   108.32 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     388.64 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 260 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     399.71 ms /    39 tokens (   10.25 ms per token,    97.57 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     400.58 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 261 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     389.36 ms /    39 tokens (    9.98 ms per token,   100.16 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     391.48 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 262 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     405.31 ms /    39 tokens (   10.39 ms per token,    96.22 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     406.44 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 263 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 47 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     403.62 ms /    47 tokens (    8.59 ms per token,   116.44 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     404.74 ms /    48 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 264 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 47 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     392.14 ms /    47 tokens (    8.34 ms per token,   119.85 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     394.87 ms /    48 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 265 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 47 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     404.83 ms /    47 tokens (    8.61 ms per token,   116.10 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     407.60 ms /    48 tokens\n",
      "Llama.generate: 94 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =      99.78 ms /     1 runs   (   99.78 ms per token,    10.02 tokens per second)\n",
      "llama_perf_context_print:       total time =     100.79 ms /     2 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 266 done\n",
      "Question 267 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 48 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     393.60 ms /    48 tokens (    8.20 ms per token,   121.95 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     397.25 ms /    49 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 268 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 48 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     399.45 ms /    48 tokens (    8.32 ms per token,   120.16 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     400.58 ms /    49 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 269 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 48 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     403.80 ms /    48 tokens (    8.41 ms per token,   118.87 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     404.85 ms /    49 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 270 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 48 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     391.18 ms /    48 tokens (    8.15 ms per token,   122.71 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     392.22 ms /    49 tokens\n",
      "Llama.generate: 95 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =      96.95 ms /     1 runs   (   96.95 ms per token,    10.32 tokens per second)\n",
      "llama_perf_context_print:       total time =      97.70 ms /     2 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 271 done\n",
      "Question 272 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 83 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     376.88 ms /    13 tokens (   28.99 ms per token,    34.49 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     378.02 ms /    14 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 273 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 48 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     576.93 ms /    48 tokens (   12.02 ms per token,    83.20 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     578.04 ms /    49 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 274 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 48 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     403.80 ms /    48 tokens (    8.41 ms per token,   118.87 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     406.71 ms /    49 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 275 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 48 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     504.33 ms /    48 tokens (   10.51 ms per token,    95.18 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     505.57 ms /    49 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 276 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 48 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     399.16 ms /    48 tokens (    8.32 ms per token,   120.25 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     401.73 ms /    49 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 277 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 48 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     391.62 ms /    48 tokens (    8.16 ms per token,   122.57 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     394.42 ms /    49 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 278 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 48 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     410.41 ms /    48 tokens (    8.55 ms per token,   116.96 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     413.03 ms /    49 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 279 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 48 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     403.70 ms /    48 tokens (    8.41 ms per token,   118.90 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     406.30 ms /    49 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 280 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 48 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.25 ms /    48 tokens (    8.30 ms per token,   120.53 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     400.82 ms /    49 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 281 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 48 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     400.61 ms /    48 tokens (    8.35 ms per token,   119.82 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     403.27 ms /    49 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 282 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 48 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     399.62 ms /    48 tokens (    8.33 ms per token,   120.12 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     400.73 ms /    49 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 283 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 83 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     210.12 ms /    13 tokens (   16.16 ms per token,    61.87 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     211.23 ms /    14 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 284 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 48 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     535.06 ms /    48 tokens (   11.15 ms per token,    89.71 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     536.20 ms /    49 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 285 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 48 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.08 ms /    48 tokens (    8.27 ms per token,   120.88 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.62 ms /    49 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 286 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 48 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     407.31 ms /    48 tokens (    8.49 ms per token,   117.85 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     408.44 ms /    49 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 287 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 48 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     399.70 ms /    48 tokens (    8.33 ms per token,   120.09 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     403.02 ms /    49 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 288 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 47 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     517.82 ms /    47 tokens (   11.02 ms per token,    90.76 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     520.23 ms /    48 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 289 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 47 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     402.86 ms /    47 tokens (    8.57 ms per token,   116.67 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     403.96 ms /    48 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 290 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 47 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.59 ms /    47 tokens (    8.48 ms per token,   117.91 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     401.38 ms /    48 tokens\n",
      "Llama.generate: 94 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =      93.73 ms /     1 runs   (   93.73 ms per token,    10.67 tokens per second)\n",
      "llama_perf_context_print:       total time =      94.79 ms /     2 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 291 done\n",
      "Question 292 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     394.16 ms /    46 tokens (    8.57 ms per token,   116.70 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     396.76 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 293 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     396.41 ms /    46 tokens (    8.62 ms per token,   116.04 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     397.95 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 294 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     400.45 ms /    43 tokens (    9.31 ms per token,   107.38 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     401.56 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 295 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     406.20 ms /    43 tokens (    9.45 ms per token,   105.86 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     407.14 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 296 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     400.93 ms /    43 tokens (    9.32 ms per token,   107.25 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     401.87 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 297 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     400.80 ms /    44 tokens (    9.11 ms per token,   109.78 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     403.25 ms /    45 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 298 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     403.45 ms /    44 tokens (    9.17 ms per token,   109.06 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     405.95 ms /    45 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 299 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     389.35 ms /    44 tokens (    8.85 ms per token,   113.01 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     390.41 ms /    45 tokens\n",
      "Llama.generate: 91 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     100.74 ms /     1 runs   (  100.74 ms per token,     9.93 tokens per second)\n",
      "llama_perf_context_print:       total time =     101.07 ms /     2 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 300 done\n",
      "Question 301 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.97 ms /    44 tokens (    9.07 ms per token,   110.29 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     402.14 ms /    45 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 302 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     409.08 ms /    44 tokens (    9.30 ms per token,   107.56 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     411.65 ms /    45 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 303 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     402.90 ms /    44 tokens (    9.16 ms per token,   109.21 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     405.22 ms /    45 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 304 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     390.84 ms /    44 tokens (    8.88 ms per token,   112.58 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     391.95 ms /    45 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 305 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     401.38 ms /    41 tokens (    9.79 ms per token,   102.15 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     403.50 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 306 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     401.34 ms /    41 tokens (    9.79 ms per token,   102.16 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     402.44 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 307 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     387.53 ms /    41 tokens (    9.45 ms per token,   105.80 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     390.18 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 308 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     400.57 ms /    41 tokens (    9.77 ms per token,   102.35 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     402.94 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 309 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     389.86 ms /    41 tokens (    9.51 ms per token,   105.17 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     390.94 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 310 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     404.48 ms /    44 tokens (    9.19 ms per token,   108.78 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     406.90 ms /    45 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 311 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     403.21 ms /    44 tokens (    9.16 ms per token,   109.12 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     406.12 ms /    45 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 312 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     390.30 ms /    44 tokens (    8.87 ms per token,   112.73 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     392.50 ms /    45 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 313 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     400.95 ms /    44 tokens (    9.11 ms per token,   109.74 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     403.39 ms /    45 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 314 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     399.96 ms /    42 tokens (    9.52 ms per token,   105.01 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     402.21 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 315 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     387.59 ms /    42 tokens (    9.23 ms per token,   108.36 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     389.88 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 316 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     403.89 ms /    42 tokens (    9.62 ms per token,   103.99 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     404.88 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 317 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 37 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     399.61 ms /    37 tokens (   10.80 ms per token,    92.59 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     400.75 ms /    38 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 318 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 37 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     530.86 ms /    37 tokens (   14.35 ms per token,    69.70 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     533.04 ms /    38 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 319 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 37 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     400.09 ms /    37 tokens (   10.81 ms per token,    92.48 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     402.23 ms /    38 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 320 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 37 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.58 ms /    37 tokens (   10.77 ms per token,    92.83 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     400.63 ms /    38 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 321 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 74 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     234.42 ms /    13 tokens (   18.03 ms per token,    55.46 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     235.43 ms /    14 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 322 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     517.06 ms /    39 tokens (   13.26 ms per token,    75.43 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     519.21 ms /    40 tokens\n",
      "Llama.generate: 86 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =      95.68 ms /     1 runs   (   95.68 ms per token,    10.45 tokens per second)\n",
      "llama_perf_context_print:       total time =      96.72 ms /     2 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 323 done\n",
      "Question 324 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 45 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     543.38 ms /    45 tokens (   12.08 ms per token,    82.81 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     546.62 ms /    46 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 325 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 45 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     502.10 ms /    45 tokens (   11.16 ms per token,    89.62 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     503.30 ms /    46 tokens\n",
      "Llama.generate: 92 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =      98.58 ms /     1 runs   (   98.58 ms per token,    10.14 tokens per second)\n",
      "llama_perf_context_print:       total time =      99.30 ms /     2 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 326 done\n",
      "Question 327 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     553.34 ms /    39 tokens (   14.19 ms per token,    70.48 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     554.69 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 328 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     418.22 ms /    39 tokens (   10.72 ms per token,    93.25 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     419.20 ms /    40 tokens\n",
      "Llama.generate: 86 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     100.84 ms /     1 runs   (  100.84 ms per token,     9.92 tokens per second)\n",
      "llama_perf_context_print:       total time =     101.85 ms /     2 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 329 done\n",
      "Question 330 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 47 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     408.46 ms /    47 tokens (    8.69 ms per token,   115.07 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     411.72 ms /    48 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 331 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 47 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     527.60 ms /    47 tokens (   11.23 ms per token,    89.08 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     529.24 ms /    48 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 332 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 47 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     409.41 ms /    47 tokens (    8.71 ms per token,   114.80 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     411.84 ms /    48 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 333 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 47 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     402.52 ms /    47 tokens (    8.56 ms per token,   116.77 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     403.58 ms /    48 tokens\n",
      "Llama.generate: 94 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =      99.02 ms /     1 runs   (   99.02 ms per token,    10.10 tokens per second)\n",
      "llama_perf_context_print:       total time =      99.82 ms /     2 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 334 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ggml_metal_free: deallocating\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 335 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     557.31 ms /    41 tokens (   13.59 ms per token,    73.57 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     558.73 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 336 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     387.19 ms /    41 tokens (    9.44 ms per token,   105.89 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     389.48 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 337 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     395.63 ms /    41 tokens (    9.65 ms per token,   103.63 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     396.62 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 338 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 38 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.42 ms /    38 tokens (   10.46 ms per token,    95.62 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     400.09 ms /    39 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 339 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 38 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     402.62 ms /    38 tokens (   10.60 ms per token,    94.38 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     405.07 ms /    39 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 340 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 38 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     401.20 ms /    38 tokens (   10.56 ms per token,    94.72 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     402.33 ms /    39 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 341 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 38 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     392.47 ms /    38 tokens (   10.33 ms per token,    96.82 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     393.42 ms /    39 tokens\n",
      "Llama.generate: 85 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =      98.87 ms /     1 runs   (   98.87 ms per token,    10.11 tokens per second)\n",
      "llama_perf_context_print:       total time =      99.97 ms /     2 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 342 done\n",
      "Question 343 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     405.33 ms /    39 tokens (   10.39 ms per token,    96.22 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     408.43 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 344 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     390.22 ms /    39 tokens (   10.01 ms per token,    99.94 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     392.67 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 345 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     401.12 ms /    39 tokens (   10.29 ms per token,    97.23 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     403.52 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 346 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     401.57 ms /    43 tokens (    9.34 ms per token,   107.08 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     402.59 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 347 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     443.13 ms /    43 tokens (   10.31 ms per token,    97.04 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     445.82 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 348 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     484.99 ms /    43 tokens (   11.28 ms per token,    88.66 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     486.13 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 349 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     424.00 ms /    43 tokens (    9.86 ms per token,   101.41 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     426.59 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 350 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     395.99 ms /    41 tokens (    9.66 ms per token,   103.54 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.49 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 351 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     399.70 ms /    41 tokens (    9.75 ms per token,   102.58 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     400.59 ms /    42 tokens\n",
      "Llama.generate: 88 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =      93.14 ms /     1 runs   (   93.14 ms per token,    10.74 tokens per second)\n",
      "llama_perf_context_print:       total time =      93.72 ms /     2 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 352 done\n",
      "Question 353 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     544.56 ms /    41 tokens (   13.28 ms per token,    75.29 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     547.19 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 354 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     497.33 ms /    41 tokens (   12.13 ms per token,    82.44 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     500.06 ms /    42 tokens\n",
      "Llama.generate: 88 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =      99.95 ms /     1 runs   (   99.95 ms per token,    10.00 tokens per second)\n",
      "llama_perf_context_print:       total time =     101.06 ms /     2 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 355 done\n",
      "Question 356 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     537.61 ms /    41 tokens (   13.11 ms per token,    76.26 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     540.22 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 357 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     488.27 ms /    41 tokens (   11.91 ms per token,    83.97 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     490.59 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 358 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     535.48 ms /    41 tokens (   13.06 ms per token,    76.57 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     537.73 ms /    42 tokens\n",
      "Llama.generate: 88 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =      93.59 ms /     1 runs   (   93.59 ms per token,    10.69 tokens per second)\n",
      "llama_perf_context_print:       total time =      94.55 ms /     2 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 359 done\n",
      "Question 360 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     418.68 ms /    44 tokens (    9.52 ms per token,   105.09 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     421.98 ms /    45 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 361 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     510.45 ms /    44 tokens (   11.60 ms per token,    86.20 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     513.07 ms /    45 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 362 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     399.25 ms /    44 tokens (    9.07 ms per token,   110.21 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     401.74 ms /    45 tokens\n",
      "Llama.generate: 91 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =      99.08 ms /     1 runs   (   99.08 ms per token,    10.09 tokens per second)\n",
      "llama_perf_context_print:       total time =     100.03 ms /     2 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 363 done\n",
      "Question 364 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 49 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     405.11 ms /    49 tokens (    8.27 ms per token,   120.95 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     409.28 ms /    50 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 365 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 49 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     394.17 ms /    49 tokens (    8.04 ms per token,   124.31 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     396.79 ms /    50 tokens\n",
      "Llama.generate: 96 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     100.44 ms /     1 runs   (  100.44 ms per token,     9.96 tokens per second)\n",
      "llama_perf_context_print:       total time =     101.40 ms /     2 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 366 done\n",
      "Question 367 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 74 prefix-match hit, remaining 20 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     231.65 ms /    20 tokens (   11.58 ms per token,    86.34 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     238.77 ms /    21 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 368 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     591.31 ms /    46 tokens (   12.85 ms per token,    77.79 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     592.38 ms /    47 tokens\n",
      "Llama.generate: 93 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =      97.96 ms /     1 runs   (   97.96 ms per token,    10.21 tokens per second)\n",
      "llama_perf_context_print:       total time =      98.53 ms /     2 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 369 done\n",
      "Question 370 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 76 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     233.17 ms /    13 tokens (   17.94 ms per token,    55.75 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     244.63 ms /    14 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 371 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     574.37 ms /    41 tokens (   14.01 ms per token,    71.38 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     575.41 ms /    42 tokens\n",
      "Llama.generate: 88 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =      99.01 ms /     1 runs   (   99.01 ms per token,    10.10 tokens per second)\n",
      "llama_perf_context_print:       total time =      99.58 ms /     2 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 372 done\n",
      "Question 373 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     388.61 ms /    41 tokens (    9.48 ms per token,   105.50 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     391.59 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 374 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     399.60 ms /    41 tokens (    9.75 ms per token,   102.60 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     400.49 ms /    42 tokens\n",
      "Llama.generate: 88 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =      99.54 ms /     1 runs   (   99.54 ms per token,    10.05 tokens per second)\n",
      "llama_perf_context_print:       total time =     100.23 ms /     2 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 375 done\n",
      "Question 376 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 74 prefix-match hit, remaining 15 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     233.46 ms /    15 tokens (   15.56 ms per token,    64.25 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     234.66 ms /    16 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 377 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     517.77 ms /    41 tokens (   12.63 ms per token,    79.19 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     520.00 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 378 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     402.57 ms /    41 tokens (    9.82 ms per token,   101.85 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     403.53 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 379 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     399.75 ms /    41 tokens (    9.75 ms per token,   102.57 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     402.38 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 380 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.74 ms /    41 tokens (    9.70 ms per token,   103.08 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.88 ms /    42 tokens\n",
      "Llama.generate: 88 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =      93.68 ms /     1 runs   (   93.68 ms per token,    10.67 tokens per second)\n",
      "llama_perf_context_print:       total time =      94.04 ms /     2 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 381 done\n",
      "Question 382 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     412.18 ms /    40 tokens (   10.30 ms per token,    97.05 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     414.95 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 383 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     403.92 ms /    40 tokens (   10.10 ms per token,    99.03 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     406.11 ms /    41 tokens\n",
      "Llama.generate: 87 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =      96.20 ms /     1 runs   (   96.20 ms per token,    10.40 tokens per second)\n",
      "llama_perf_context_print:       total time =      97.16 ms /     2 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 384 done\n",
      "Question 385 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     396.28 ms /    40 tokens (    9.91 ms per token,   100.94 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.19 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 386 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     403.40 ms /    40 tokens (   10.08 ms per token,    99.16 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     405.74 ms /    41 tokens\n",
      "Llama.generate: 87 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =      93.86 ms /     1 runs   (   93.86 ms per token,    10.65 tokens per second)\n",
      "llama_perf_context_print:       total time =      94.83 ms /     2 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 387 done\n",
      "Question 388 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     400.62 ms /    42 tokens (    9.54 ms per token,   104.84 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     403.20 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 389 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     407.83 ms /    42 tokens (    9.71 ms per token,   102.98 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     410.10 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 390 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     422.16 ms /    42 tokens (   10.05 ms per token,    99.49 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     424.59 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 391 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     503.44 ms /    43 tokens (   11.71 ms per token,    85.41 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     505.79 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 392 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     390.07 ms /    43 tokens (    9.07 ms per token,   110.24 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     392.47 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 393 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     402.33 ms /    43 tokens (    9.36 ms per token,   106.88 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     404.56 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 394 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     402.92 ms /    43 tokens (    9.37 ms per token,   106.72 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     405.19 ms /    44 tokens\n",
      "Llama.generate: 90 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =      97.98 ms /     1 runs   (   97.98 ms per token,    10.21 tokens per second)\n",
      "llama_perf_context_print:       total time =      98.38 ms /     2 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 395 done\n",
      "Question 396 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     406.71 ms /    43 tokens (    9.46 ms per token,   105.73 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     409.59 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 397 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     403.46 ms /    43 tokens (    9.38 ms per token,   106.58 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     405.80 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 398 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     400.35 ms /    43 tokens (    9.31 ms per token,   107.41 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     402.58 ms /    44 tokens\n",
      "Llama.generate: 90 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =      99.14 ms /     1 runs   (   99.14 ms per token,    10.09 tokens per second)\n",
      "llama_perf_context_print:       total time =     100.24 ms /     2 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 399 done\n",
      "Question 400 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     402.47 ms /    43 tokens (    9.36 ms per token,   106.84 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     405.52 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 401 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     387.16 ms /    43 tokens (    9.00 ms per token,   111.06 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     388.19 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 402 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.31 ms /    43 tokens (    9.24 ms per token,   108.23 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.63 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 403 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     402.89 ms /    43 tokens (    9.37 ms per token,   106.73 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     405.18 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 404 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     402.33 ms /    43 tokens (    9.36 ms per token,   106.88 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     403.36 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 405 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     401.66 ms /    43 tokens (    9.34 ms per token,   107.06 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     403.90 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 406 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 90 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     319.58 ms /     1 runs   (  319.58 ms per token,     3.13 tokens per second)\n",
      "llama_perf_context_print:       total time =     320.11 ms /     2 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 407 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 48 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     399.23 ms /    48 tokens (    8.32 ms per token,   120.23 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     402.04 ms /    49 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 408 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 48 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     407.36 ms /    48 tokens (    8.49 ms per token,   117.83 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     408.34 ms /    49 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 409 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 48 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     388.94 ms /    48 tokens (    8.10 ms per token,   123.41 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     391.13 ms /    49 tokens\n",
      "Llama.generate: 95 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =      98.03 ms /     1 runs   (   98.03 ms per token,    10.20 tokens per second)\n",
      "llama_perf_context_print:       total time =      98.65 ms /     2 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 410 done\n",
      "Question 411 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     393.88 ms /    39 tokens (   10.10 ms per token,    99.01 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     396.43 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 412 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     443.53 ms /    39 tokens (   11.37 ms per token,    87.93 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     444.51 ms /    40 tokens\n",
      "Llama.generate: 86 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =      99.36 ms /     1 runs   (   99.36 ms per token,    10.06 tokens per second)\n",
      "llama_perf_context_print:       total time =     100.43 ms /     2 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 413 done\n",
      "Question 414 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     387.86 ms /    39 tokens (    9.95 ms per token,   100.55 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     390.62 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 415 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     406.56 ms /    39 tokens (   10.42 ms per token,    95.93 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     409.19 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 416 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     401.92 ms /    39 tokens (   10.31 ms per token,    97.03 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     402.92 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 417 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     410.66 ms /    39 tokens (   10.53 ms per token,    94.97 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     412.77 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 418 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     399.05 ms /    39 tokens (   10.23 ms per token,    97.73 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     401.21 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 419 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     389.64 ms /    39 tokens (    9.99 ms per token,   100.09 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     391.71 ms /    40 tokens\n",
      "Llama.generate: 86 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =      94.01 ms /     1 runs   (   94.01 ms per token,    10.64 tokens per second)\n",
      "llama_perf_context_print:       total time =      95.05 ms /     2 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 420 done\n",
      "Question 421 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     403.33 ms /    39 tokens (   10.34 ms per token,    96.70 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     406.00 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 422 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     403.45 ms /    39 tokens (   10.34 ms per token,    96.67 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     405.83 ms /    40 tokens\n",
      "Llama.generate: 86 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     101.53 ms /     1 runs   (  101.53 ms per token,     9.85 tokens per second)\n",
      "llama_perf_context_print:       total time =     102.54 ms /     2 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 423 done\n",
      "Question 424 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     404.87 ms /    39 tokens (   10.38 ms per token,    96.33 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     407.49 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 425 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     395.83 ms /    39 tokens (   10.15 ms per token,    98.53 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.06 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 426 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     400.72 ms /    39 tokens (   10.27 ms per token,    97.33 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     401.67 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 427 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     400.45 ms /    39 tokens (   10.27 ms per token,    97.39 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     402.60 ms /    40 tokens\n",
      "Llama.generate: 86 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =      92.59 ms /     1 runs   (   92.59 ms per token,    10.80 tokens per second)\n",
      "llama_perf_context_print:       total time =      93.65 ms /     2 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 428 done\n",
      "Question 429 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     399.83 ms /    39 tokens (   10.25 ms per token,    97.54 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     402.46 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 430 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     401.66 ms /    39 tokens (   10.30 ms per token,    97.10 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     402.56 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 431 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     384.98 ms /    39 tokens (    9.87 ms per token,   101.31 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     387.38 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 432 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     408.43 ms /    43 tokens (    9.50 ms per token,   105.28 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     410.74 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 433 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     402.63 ms /    43 tokens (    9.36 ms per token,   106.80 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     404.86 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 434 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     519.26 ms /    43 tokens (   12.08 ms per token,    82.81 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     520.24 ms /    44 tokens\n",
      "Llama.generate: 90 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =      98.50 ms /     1 runs   (   98.50 ms per token,    10.15 tokens per second)\n",
      "llama_perf_context_print:       total time =      99.63 ms /     2 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 435 done\n",
      "Question 436 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     562.67 ms /    44 tokens (   12.79 ms per token,    78.20 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     564.29 ms /    45 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 437 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     396.80 ms /    44 tokens (    9.02 ms per token,   110.89 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.13 ms /    45 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 438 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     402.38 ms /    44 tokens (    9.14 ms per token,   109.35 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     404.77 ms /    45 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 439 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     650.52 ms /    39 tokens (   16.68 ms per token,    59.95 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     651.46 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 440 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     401.12 ms /    39 tokens (   10.29 ms per token,    97.23 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     403.31 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 441 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     516.91 ms /    39 tokens (   13.25 ms per token,    75.45 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     517.89 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 442 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     553.82 ms /    42 tokens (   13.19 ms per token,    75.84 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     556.14 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 443 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     516.07 ms /    42 tokens (   12.29 ms per token,    81.38 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     517.17 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 444 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     399.95 ms /    42 tokens (    9.52 ms per token,   105.01 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     402.28 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 445 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 38 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     400.45 ms /    38 tokens (   10.54 ms per token,    94.89 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     402.56 ms /    39 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 446 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 38 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     387.89 ms /    38 tokens (   10.21 ms per token,    97.97 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     390.19 ms /    39 tokens\n",
      "Llama.generate: 85 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =      99.13 ms /     1 runs   (   99.13 ms per token,    10.09 tokens per second)\n",
      "llama_perf_context_print:       total time =     100.24 ms /     2 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 447 done\n",
      "Question 448 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     400.17 ms /    39 tokens (   10.26 ms per token,    97.46 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     402.94 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 449 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     392.09 ms /    39 tokens (   10.05 ms per token,    99.47 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     394.18 ms /    40 tokens\n",
      "Llama.generate: 86 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     101.33 ms /     1 runs   (  101.33 ms per token,     9.87 tokens per second)\n",
      "llama_perf_context_print:       total time =     102.41 ms /     2 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 450 done\n",
      "Question 451 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     405.61 ms /    39 tokens (   10.40 ms per token,    96.15 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     408.23 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 452 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     387.29 ms /    39 tokens (    9.93 ms per token,   100.70 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     389.42 ms /    40 tokens\n",
      "Llama.generate: 86 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =      98.82 ms /     1 runs   (   98.82 ms per token,    10.12 tokens per second)\n",
      "llama_perf_context_print:       total time =      99.87 ms /     2 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 453 done\n",
      "Question 454 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     538.01 ms /    44 tokens (   12.23 ms per token,    81.78 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     539.39 ms /    45 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 455 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.60 ms /    44 tokens (    9.04 ms per token,   110.66 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.88 ms /    45 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 456 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 91 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     372.87 ms /     1 runs   (  372.87 ms per token,     2.68 tokens per second)\n",
      "llama_perf_context_print:       total time =     373.40 ms /     2 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 457 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 49 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     394.73 ms /    49 tokens (    8.06 ms per token,   124.14 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     397.13 ms /    50 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 458 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 49 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     518.70 ms /    49 tokens (   10.59 ms per token,    94.47 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     521.64 ms /    50 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 459 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 49 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     406.01 ms /    49 tokens (    8.29 ms per token,   120.69 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     408.63 ms /    50 tokens\n",
      "Llama.generate: 96 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =      93.93 ms /     1 runs   (   93.93 ms per token,    10.65 tokens per second)\n",
      "llama_perf_context_print:       total time =      94.95 ms /     2 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 460 done\n",
      "Question 461 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 45 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     401.96 ms /    45 tokens (    8.93 ms per token,   111.95 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     404.67 ms /    46 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 462 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 45 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     403.19 ms /    45 tokens (    8.96 ms per token,   111.61 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     405.57 ms /    46 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 463 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 45 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     402.83 ms /    45 tokens (    8.95 ms per token,   111.71 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     403.85 ms /    46 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 464 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 45 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     402.28 ms /    45 tokens (    8.94 ms per token,   111.86 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     404.65 ms /    46 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 465 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 45 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.59 ms /    45 tokens (    8.84 ms per token,   113.18 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.87 ms /    46 tokens\n",
      "Llama.generate: 92 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     101.18 ms /     1 runs   (  101.18 ms per token,     9.88 tokens per second)\n",
      "llama_perf_context_print:       total time =     101.78 ms /     2 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 466 done\n",
      "Question 467 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     395.46 ms /    39 tokens (   10.14 ms per token,    98.62 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     397.98 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 468 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.01 ms /    39 tokens (   10.18 ms per token,    98.23 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.28 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 469 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     400.51 ms /    39 tokens (   10.27 ms per token,    97.38 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     402.65 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 470 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     406.45 ms /    42 tokens (    9.68 ms per token,   103.33 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     407.42 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 471 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     418.31 ms /    42 tokens (    9.96 ms per token,   100.40 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     420.73 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 472 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     399.26 ms /    42 tokens (    9.51 ms per token,   105.19 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     400.20 ms /    43 tokens\n",
      "Llama.generate: 89 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =      98.21 ms /     1 runs   (   98.21 ms per token,    10.18 tokens per second)\n",
      "llama_perf_context_print:       total time =      98.78 ms /     2 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 473 done\n",
      "Question 474 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 48 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     407.84 ms /    48 tokens (    8.50 ms per token,   117.69 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     410.94 ms /    49 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 475 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 48 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     400.59 ms /    48 tokens (    8.35 ms per token,   119.82 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     401.66 ms /    49 tokens\n",
      "Llama.generate: 95 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =      98.34 ms /     1 runs   (   98.34 ms per token,    10.17 tokens per second)\n",
      "llama_perf_context_print:       total time =      99.35 ms /     2 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 476 done\n",
      "Question 477 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     458.88 ms /    40 tokens (   11.47 ms per token,    87.17 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     466.53 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 478 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     404.99 ms /    40 tokens (   10.12 ms per token,    98.77 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     406.76 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 479 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     446.34 ms /    40 tokens (   11.16 ms per token,    89.62 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     447.90 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 480 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     510.78 ms /    46 tokens (   11.10 ms per token,    90.06 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     512.98 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 481 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     452.35 ms /    46 tokens (    9.83 ms per token,   101.69 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     453.62 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 482 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     447.59 ms /    46 tokens (    9.73 ms per token,   102.77 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     449.17 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 483 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     394.30 ms /    46 tokens (    8.57 ms per token,   116.66 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     395.76 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 484 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     408.08 ms /    46 tokens (    8.87 ms per token,   112.72 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     409.54 ms /    47 tokens\n",
      "Llama.generate: 93 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     102.14 ms /     1 runs   (  102.14 ms per token,     9.79 tokens per second)\n",
      "llama_perf_context_print:       total time =     104.28 ms /     2 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 485 done\n",
      "Question 486 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 49 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     400.42 ms /    49 tokens (    8.17 ms per token,   122.37 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     402.26 ms /    50 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 487 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 49 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     450.87 ms /    49 tokens (    9.20 ms per token,   108.68 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     452.29 ms /    50 tokens\n",
      "Llama.generate: 96 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     109.50 ms /     1 runs   (  109.50 ms per token,     9.13 tokens per second)\n",
      "llama_perf_context_print:       total time =     111.01 ms /     2 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 488 done\n",
      "Question 489 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 74 prefix-match hit, remaining 20 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     229.09 ms /    20 tokens (   11.45 ms per token,    87.30 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     230.82 ms /    21 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 490 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     569.46 ms /    46 tokens (   12.38 ms per token,    80.78 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     570.49 ms /    47 tokens\n",
      "Llama.generate: 93 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     103.52 ms /     1 runs   (  103.52 ms per token,     9.66 tokens per second)\n",
      "llama_perf_context_print:       total time =     104.74 ms /     2 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 491 done\n",
      "Question 492 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     391.92 ms /    40 tokens (    9.80 ms per token,   102.06 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     393.28 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 493 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     445.99 ms /    40 tokens (   11.15 ms per token,    89.69 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     447.66 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 494 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     443.45 ms /    40 tokens (   11.09 ms per token,    90.20 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     444.84 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 495 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     656.51 ms /    41 tokens (   16.01 ms per token,    62.45 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     657.92 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 496 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     439.94 ms /    41 tokens (   10.73 ms per token,    93.20 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     441.28 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 497 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     683.68 ms /    41 tokens (   16.68 ms per token,    59.97 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     685.01 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 498 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     637.14 ms /    42 tokens (   15.17 ms per token,    65.92 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     638.47 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 499 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     647.16 ms /    42 tokens (   15.41 ms per token,    64.90 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     648.70 ms /    43 tokens\n",
      "Llama.generate: 89 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     107.87 ms /     1 runs   (  107.87 ms per token,     9.27 tokens per second)\n",
      "llama_perf_context_print:       total time =     109.06 ms /     2 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 500 done\n",
      "Question 501 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     392.64 ms /    41 tokens (    9.58 ms per token,   104.42 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     393.94 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 502 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     445.25 ms /    41 tokens (   10.86 ms per token,    92.08 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     446.99 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 503 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     646.97 ms /    41 tokens (   15.78 ms per token,    63.37 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     648.38 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 504 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     410.40 ms /    41 tokens (   10.01 ms per token,    99.90 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     411.96 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 505 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     454.20 ms /    41 tokens (   11.08 ms per token,    90.27 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     455.70 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 506 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     630.88 ms /    41 tokens (   15.39 ms per token,    64.99 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     632.44 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 507 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 88 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     352.97 ms /     1 runs   (  352.97 ms per token,     2.83 tokens per second)\n",
      "llama_perf_context_print:       total time =     354.86 ms /     2 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 508 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 52 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     399.70 ms /    52 tokens (    7.69 ms per token,   130.10 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     401.50 ms /    53 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 509 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 52 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     408.72 ms /    52 tokens (    7.86 ms per token,   127.23 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     409.79 ms /    53 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 510 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     410.09 ms /    46 tokens (    8.91 ms per token,   112.17 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     411.54 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 511 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     453.01 ms /    46 tokens (    9.85 ms per token,   101.54 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     454.08 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 512 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 93 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     328.81 ms /     1 runs   (  328.81 ms per token,     3.04 tokens per second)\n",
      "llama_perf_context_print:       total time =     329.77 ms /     2 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 513 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 86 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     322.99 ms /     8 tokens (   40.37 ms per token,    24.77 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     323.68 ms /     9 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 514 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     558.78 ms /    46 tokens (   12.15 ms per token,    82.32 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     560.27 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 515 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     409.85 ms /    46 tokens (    8.91 ms per token,   112.24 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     411.39 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 516 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     400.38 ms /    46 tokens (    8.70 ms per token,   114.89 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     401.87 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 517 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.65 ms /    46 tokens (    8.67 ms per token,   115.39 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     400.10 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 518 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     446.63 ms /    46 tokens (    9.71 ms per token,   102.99 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     448.08 ms /    47 tokens\n",
      "Llama.generate: 93 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     103.16 ms /     1 runs   (  103.16 ms per token,     9.69 tokens per second)\n",
      "llama_perf_context_print:       total time =     104.28 ms /     2 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 519 done\n",
      "Question 520 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 86 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     339.24 ms /     8 tokens (   42.41 ms per token,    23.58 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     340.11 ms /     9 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 521 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     450.04 ms /    46 tokens (    9.78 ms per token,   102.21 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     451.65 ms /    47 tokens\n",
      "Llama.generate: 93 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     107.85 ms /     1 runs   (  107.85 ms per token,     9.27 tokens per second)\n",
      "llama_perf_context_print:       total time =     109.40 ms /     2 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 522 done\n",
      "Question 523 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 86 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     332.03 ms /     8 tokens (   41.50 ms per token,    24.09 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     332.91 ms /     9 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 524 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     399.29 ms /    46 tokens (    8.68 ms per token,   115.20 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     400.65 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 525 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     568.11 ms /    46 tokens (   12.35 ms per token,    80.97 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     569.76 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 526 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     644.82 ms /    46 tokens (   14.02 ms per token,    71.34 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     646.56 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 527 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     445.22 ms /    46 tokens (    9.68 ms per token,   103.32 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     446.21 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 528 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     675.19 ms /    46 tokens (   14.68 ms per token,    68.13 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     676.66 ms /    47 tokens\n",
      "Llama.generate: 93 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     109.52 ms /     1 runs   (  109.52 ms per token,     9.13 tokens per second)\n",
      "llama_perf_context_print:       total time =     111.00 ms /     2 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 529 done\n",
      "Question 530 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 86 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     325.18 ms /     8 tokens (   40.65 ms per token,    24.60 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     326.12 ms /     9 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 531 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     438.26 ms /    46 tokens (    9.53 ms per token,   104.96 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     439.33 ms /    47 tokens\n",
      "Llama.generate: 93 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     108.04 ms /     1 runs   (  108.04 ms per token,     9.26 tokens per second)\n",
      "llama_perf_context_print:       total time =     109.46 ms /     2 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 532 done\n",
      "Question 533 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 86 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     334.83 ms /     8 tokens (   41.85 ms per token,    23.89 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     335.67 ms /     9 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 534 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     589.55 ms /    46 tokens (   12.82 ms per token,    78.03 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     590.98 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 535 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     447.19 ms /    46 tokens (    9.72 ms per token,   102.87 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     450.70 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 536 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     601.51 ms /    46 tokens (   13.08 ms per token,    76.47 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     603.00 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 537 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.34 ms /    46 tokens (    8.66 ms per token,   115.48 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.79 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 538 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     408.69 ms /    46 tokens (    8.88 ms per token,   112.56 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     410.32 ms /    47 tokens\n",
      "Llama.generate: 93 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     105.56 ms /     1 runs   (  105.56 ms per token,     9.47 tokens per second)\n",
      "llama_perf_context_print:       total time =     107.03 ms /     2 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 539 done\n",
      "Question 540 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 86 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     338.83 ms /     8 tokens (   42.35 ms per token,    23.61 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     339.71 ms /     9 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 541 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     457.74 ms /    46 tokens (    9.95 ms per token,   100.49 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     459.21 ms /    47 tokens\n",
      "Llama.generate: 93 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     106.05 ms /     1 runs   (  106.05 ms per token,     9.43 tokens per second)\n",
      "llama_perf_context_print:       total time =     107.63 ms /     2 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 542 done\n",
      "Question 543 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 86 prefix-match hit, remaining 8 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     331.64 ms /     8 tokens (   41.45 ms per token,    24.12 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     332.47 ms /     9 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 544 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     564.77 ms /    46 tokens (   12.28 ms per token,    81.45 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     566.19 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 545 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     412.96 ms /    46 tokens (    8.98 ms per token,   111.39 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     414.83 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 546 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     655.97 ms /    46 tokens (   14.26 ms per token,    70.13 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     657.47 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 547 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     401.68 ms /    46 tokens (    8.73 ms per token,   114.52 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     403.17 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 548 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     439.29 ms /    46 tokens (    9.55 ms per token,   104.71 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     440.76 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 549 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 93 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     307.92 ms /     1 runs   (  307.92 ms per token,     3.25 tokens per second)\n",
      "llama_perf_context_print:       total time =     308.82 ms /     2 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 550 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     412.48 ms /    46 tokens (    8.97 ms per token,   111.52 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     413.91 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 551 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     452.34 ms /    46 tokens (    9.83 ms per token,   101.69 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     453.78 ms /    47 tokens\n",
      "Llama.generate: 93 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     107.28 ms /     1 runs   (  107.28 ms per token,     9.32 tokens per second)\n",
      "llama_perf_context_print:       total time =     109.13 ms /     2 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 552 done\n",
      "Question 553 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     401.99 ms /    46 tokens (    8.74 ms per token,   114.43 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     403.66 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 554 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     450.78 ms /    46 tokens (    9.80 ms per token,   102.05 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     452.19 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 555 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     504.26 ms /    46 tokens (   10.96 ms per token,    91.22 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     505.95 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 556 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 81 prefix-match hit, remaining 13 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.86 ms /    13 tokens (   30.68 ms per token,    32.59 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     400.01 ms /    14 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 557 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     590.90 ms /    46 tokens (   12.85 ms per token,    77.85 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     592.50 ms /    47 tokens\n",
      "Llama.generate: 93 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     103.59 ms /     1 runs   (  103.59 ms per token,     9.65 tokens per second)\n",
      "llama_perf_context_print:       total time =     105.07 ms /     2 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 558 done\n",
      "Question 559 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     395.62 ms /    46 tokens (    8.60 ms per token,   116.27 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     397.22 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 560 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     408.39 ms /    46 tokens (    8.88 ms per token,   112.64 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     412.50 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 561 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     452.23 ms /    46 tokens (    9.83 ms per token,   101.72 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     453.31 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 562 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     634.60 ms /    43 tokens (   14.76 ms per token,    67.76 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     635.99 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 563 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     447.09 ms /    43 tokens (   10.40 ms per token,    96.18 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     448.75 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 564 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     667.75 ms /    43 tokens (   15.53 ms per token,    64.39 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     669.04 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 565 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     451.51 ms /    41 tokens (   11.01 ms per token,    90.81 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     452.81 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 566 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     596.14 ms /    41 tokens (   14.54 ms per token,    68.78 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     597.53 ms /    42 tokens\n",
      "Llama.generate: 88 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     109.89 ms /     1 runs   (  109.89 ms per token,     9.10 tokens per second)\n",
      "llama_perf_context_print:       total time =     110.51 ms /     2 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 567 done\n",
      "Question 568 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 75 prefix-match hit, remaining 16 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     231.82 ms /    16 tokens (   14.49 ms per token,    69.02 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     233.00 ms /    17 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 569 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     628.88 ms /    43 tokens (   14.63 ms per token,    68.37 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     630.14 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 570 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     410.91 ms /    43 tokens (    9.56 ms per token,   104.65 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     412.41 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 571 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     450.43 ms /    43 tokens (   10.48 ms per token,    95.46 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     451.93 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 572 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     647.85 ms /    43 tokens (   15.07 ms per token,    66.37 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     649.21 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 573 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     644.48 ms /    40 tokens (   16.11 ms per token,    62.07 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     646.01 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 574 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     684.62 ms /    40 tokens (   17.12 ms per token,    58.43 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     686.19 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 575 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 87 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     325.03 ms /     1 runs   (  325.03 ms per token,     3.08 tokens per second)\n",
      "llama_perf_context_print:       total time =     325.79 ms /     2 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 576 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.90 ms /    44 tokens (    9.07 ms per token,   110.30 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     400.40 ms /    45 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 577 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     449.91 ms /    44 tokens (   10.23 ms per token,    97.80 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     451.31 ms /    45 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 578 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     437.41 ms /    44 tokens (    9.94 ms per token,   100.59 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     438.83 ms /    45 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 579 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 91 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     344.03 ms /     1 runs   (  344.03 ms per token,     2.91 tokens per second)\n",
      "llama_perf_context_print:       total time =     344.64 ms /     2 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 580 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     394.78 ms /    41 tokens (    9.63 ms per token,   103.86 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     396.29 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 581 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     406.61 ms /    41 tokens (    9.92 ms per token,   100.83 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     407.95 ms /    42 tokens\n",
      "Llama.generate: 88 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     108.90 ms /     1 runs   (  108.90 ms per token,     9.18 tokens per second)\n",
      "llama_perf_context_print:       total time =     110.59 ms /     2 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 582 done\n",
      "Question 583 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     407.74 ms /    42 tokens (    9.71 ms per token,   103.01 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     409.26 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 584 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     405.50 ms /    42 tokens (    9.65 ms per token,   103.58 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     408.13 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 585 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     407.82 ms /    42 tokens (    9.71 ms per token,   102.99 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     409.22 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 586 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     451.95 ms /    42 tokens (   10.76 ms per token,    92.93 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     453.56 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 587 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     451.51 ms /    42 tokens (   10.75 ms per token,    93.02 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     452.87 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 588 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 89 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     311.32 ms /     1 runs   (  311.32 ms per token,     3.21 tokens per second)\n",
      "llama_perf_context_print:       total time =     311.65 ms /     2 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 589 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 49 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     408.11 ms /    49 tokens (    8.33 ms per token,   120.07 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     411.11 ms /    50 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 590 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 49 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     405.34 ms /    49 tokens (    8.27 ms per token,   120.89 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     408.73 ms /    50 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 591 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 49 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     407.75 ms /    49 tokens (    8.32 ms per token,   120.17 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     409.35 ms /    50 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 592 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 49 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     439.88 ms /    49 tokens (    8.98 ms per token,   111.39 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     440.98 ms /    50 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 593 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 49 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     530.30 ms /    49 tokens (   10.82 ms per token,    92.40 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     531.85 ms /    50 tokens\n",
      "Llama.generate: 96 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     107.55 ms /     1 runs   (  107.55 ms per token,     9.30 tokens per second)\n",
      "llama_perf_context_print:       total time =     108.72 ms /     2 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 594 done\n",
      "Question 595 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 49 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     556.16 ms /    49 tokens (   11.35 ms per token,    88.10 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     557.76 ms /    50 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 596 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 49 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     635.45 ms /    49 tokens (   12.97 ms per token,    77.11 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     636.95 ms /    50 tokens\n",
      "Llama.generate: 96 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =     107.84 ms /     1 runs   (  107.84 ms per token,     9.27 tokens per second)\n",
      "llama_perf_context_print:       total time =     108.67 ms /     2 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 597 done\n",
      "Question 598 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     642.50 ms /    46 tokens (   13.97 ms per token,    71.59 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     643.99 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 599 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     439.84 ms /    46 tokens (    9.56 ms per token,   104.58 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     441.45 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 600 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 48 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     637.48 ms /    48 tokens (   13.28 ms per token,    75.30 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     639.12 ms /    49 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 601 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 48 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     448.56 ms /    48 tokens (    9.35 ms per token,   107.01 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     450.05 ms /    49 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 602 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     643.27 ms /    46 tokens (   13.98 ms per token,    71.51 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     644.93 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 603 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     446.63 ms /    46 tokens (    9.71 ms per token,   102.99 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     448.03 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 604 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     466.14 ms /    40 tokens (   11.65 ms per token,    85.81 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     467.05 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 605 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     556.51 ms /    40 tokens (   13.91 ms per token,    71.88 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     557.97 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 606 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     446.08 ms /    40 tokens (   11.15 ms per token,    89.67 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     447.34 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 607 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     682.26 ms /    40 tokens (   17.06 ms per token,    58.63 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     683.79 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 608 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     449.66 ms /    42 tokens (   10.71 ms per token,    93.40 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     451.01 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 609 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     628.46 ms /    42 tokens (   14.96 ms per token,    66.83 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     629.83 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 610 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     440.85 ms /    42 tokens (   10.50 ms per token,    95.27 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     442.56 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 611 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     630.63 ms /    42 tokens (   15.01 ms per token,    66.60 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     632.51 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 612 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     437.32 ms /    42 tokens (   10.41 ms per token,    96.04 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     438.82 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 613 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     440.89 ms /    42 tokens (   10.50 ms per token,    95.26 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     442.31 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 614 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     647.81 ms /    42 tokens (   15.42 ms per token,    64.83 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     648.89 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 615 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     514.11 ms /    42 tokens (   12.24 ms per token,    81.70 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     515.04 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 616 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 38 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     453.41 ms /    38 tokens (   11.93 ms per token,    83.81 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     454.79 ms /    39 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 617 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 38 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     445.29 ms /    38 tokens (   11.72 ms per token,    85.34 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     446.79 ms /    39 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 618 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 38 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     440.35 ms /    38 tokens (   11.59 ms per token,    86.30 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     441.49 ms /    39 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 619 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 38 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     438.72 ms /    38 tokens (   11.55 ms per token,    86.62 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     440.20 ms /    39 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 620 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 38 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     678.40 ms /    38 tokens (   17.85 ms per token,    56.01 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     679.80 ms /    39 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 621 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 38 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     627.09 ms /    38 tokens (   16.50 ms per token,    60.60 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     628.17 ms /    39 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 622 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 38 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     436.48 ms /    38 tokens (   11.49 ms per token,    87.06 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     437.82 ms /    39 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 623 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     682.74 ms /    42 tokens (   16.26 ms per token,    61.52 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     684.24 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 624 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     630.02 ms /    42 tokens (   15.00 ms per token,    66.66 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     631.43 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 625 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     400.11 ms /    42 tokens (    9.53 ms per token,   104.97 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     401.59 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 626 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     400.36 ms /    42 tokens (    9.53 ms per token,   104.90 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     403.14 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 627 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     414.28 ms /    42 tokens (    9.86 ms per token,   101.38 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     415.72 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 628 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     659.55 ms /    42 tokens (   15.70 ms per token,    63.68 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     660.95 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 629 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     455.01 ms /    42 tokens (   10.83 ms per token,    92.31 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     456.55 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 630 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     629.27 ms /    42 tokens (   14.98 ms per token,    66.74 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     630.75 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 631 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     449.36 ms /    40 tokens (   11.23 ms per token,    89.02 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     450.86 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 632 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     645.29 ms /    40 tokens (   16.13 ms per token,    61.99 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     646.57 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 633 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     439.40 ms /    40 tokens (   10.99 ms per token,    91.03 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     440.83 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 634 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     677.72 ms /    40 tokens (   16.94 ms per token,    59.02 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     679.06 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 635 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     637.56 ms /    40 tokens (   15.94 ms per token,    62.74 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     638.81 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 636 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     409.21 ms /    40 tokens (   10.23 ms per token,    97.75 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     410.61 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 637 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     447.57 ms /    40 tokens (   11.19 ms per token,    89.37 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     448.97 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 638 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     629.73 ms /    40 tokens (   15.74 ms per token,    63.52 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     631.24 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 639 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     400.53 ms /    40 tokens (   10.01 ms per token,    99.87 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     401.91 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 640 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     442.88 ms /    40 tokens (   11.07 ms per token,    90.32 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     443.99 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 641 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     639.51 ms /    40 tokens (   15.99 ms per token,    62.55 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     640.72 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 642 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     450.37 ms /    40 tokens (   11.26 ms per token,    88.82 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     452.01 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 643 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     653.93 ms /    40 tokens (   16.35 ms per token,    61.17 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     656.68 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 644 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     647.09 ms /    40 tokens (   16.18 ms per token,    61.82 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     648.38 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 645 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     452.70 ms /    40 tokens (   11.32 ms per token,    88.36 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     455.27 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 646 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     616.78 ms /    40 tokens (   15.42 ms per token,    64.85 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     618.14 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 647 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     443.95 ms /    40 tokens (   11.10 ms per token,    90.10 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     445.29 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 648 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     690.05 ms /    40 tokens (   17.25 ms per token,    57.97 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     691.41 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 649 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     653.48 ms /    41 tokens (   15.94 ms per token,    62.74 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     654.54 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 650 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     633.67 ms /    41 tokens (   15.46 ms per token,    64.70 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     635.27 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 651 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     452.68 ms /    41 tokens (   11.04 ms per token,    90.57 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     454.17 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 652 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     639.54 ms /    41 tokens (   15.60 ms per token,    64.11 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     641.09 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 653 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     442.11 ms /    41 tokens (   10.78 ms per token,    92.74 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     443.63 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 654 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     534.49 ms /    41 tokens (   13.04 ms per token,    76.71 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     535.76 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 655 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     452.87 ms /    41 tokens (   11.05 ms per token,    90.53 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     454.13 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 656 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     448.56 ms /    41 tokens (   10.94 ms per token,    91.40 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     449.93 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 657 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     410.03 ms /    42 tokens (    9.76 ms per token,   102.43 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     411.33 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 658 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     449.04 ms /    42 tokens (   10.69 ms per token,    93.53 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     450.42 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 659 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     690.06 ms /    42 tokens (   16.43 ms per token,    60.86 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     691.62 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 660 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     631.08 ms /    42 tokens (   15.03 ms per token,    66.55 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     632.47 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 661 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     399.47 ms /    42 tokens (    9.51 ms per token,   105.14 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     400.87 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 662 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     407.63 ms /    42 tokens (    9.71 ms per token,   103.03 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     409.46 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 663 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     645.60 ms /    42 tokens (   15.37 ms per token,    65.06 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     647.44 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 664 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     454.38 ms /    42 tokens (   10.82 ms per token,    92.43 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     455.91 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 665 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     643.00 ms /    40 tokens (   16.08 ms per token,    62.21 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     644.23 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 666 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     654.14 ms /    40 tokens (   16.35 ms per token,    61.15 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     655.49 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 667 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     410.05 ms /    40 tokens (   10.25 ms per token,    97.55 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     411.53 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 668 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     449.63 ms /    40 tokens (   11.24 ms per token,    88.96 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     451.03 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 669 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     638.28 ms /    40 tokens (   15.96 ms per token,    62.67 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     639.60 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 670 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     408.64 ms /    40 tokens (   10.22 ms per token,    97.89 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     410.63 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 671 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     643.90 ms /    40 tokens (   16.10 ms per token,    62.12 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     645.10 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 672 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     439.47 ms /    40 tokens (   10.99 ms per token,    91.02 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     440.88 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 673 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     659.77 ms /    46 tokens (   14.34 ms per token,    69.72 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     661.41 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 674 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     642.44 ms /    46 tokens (   13.97 ms per token,    71.60 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     643.72 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 675 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     400.28 ms /    46 tokens (    8.70 ms per token,   114.92 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     401.73 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 676 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     410.13 ms /    46 tokens (    8.92 ms per token,   112.16 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     411.57 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 677 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     412.41 ms /    46 tokens (    8.97 ms per token,   111.54 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     414.07 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 678 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     684.74 ms /    46 tokens (   14.89 ms per token,    67.18 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     686.32 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 679 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     623.74 ms /    46 tokens (   13.56 ms per token,    73.75 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     625.36 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 680 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     412.25 ms /    46 tokens (    8.96 ms per token,   111.58 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     414.06 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 681 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     451.23 ms /    40 tokens (   11.28 ms per token,    88.65 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     452.69 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 682 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     620.99 ms /    40 tokens (   15.52 ms per token,    64.41 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     622.40 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 683 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     452.25 ms /    40 tokens (   11.31 ms per token,    88.45 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     453.71 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 684 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     625.25 ms /    40 tokens (   15.63 ms per token,    63.97 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     626.66 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 685 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     452.29 ms /    40 tokens (   11.31 ms per token,    88.44 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     453.59 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 686 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     484.86 ms /    40 tokens (   12.12 ms per token,    82.50 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     485.90 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 687 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     564.70 ms /    40 tokens (   14.12 ms per token,    70.83 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     566.08 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 688 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     446.88 ms /    40 tokens (   11.17 ms per token,    89.51 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     448.27 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 689 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     682.21 ms /    40 tokens (   17.06 ms per token,    58.63 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     683.61 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 690 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     410.34 ms /    40 tokens (   10.26 ms per token,    97.48 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     411.63 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 691 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     448.28 ms /    40 tokens (   11.21 ms per token,    89.23 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     449.66 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 692 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     605.89 ms /    40 tokens (   15.15 ms per token,    66.02 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     607.35 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 693 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     410.34 ms /    40 tokens (   10.26 ms per token,    97.48 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     411.69 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 694 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     451.03 ms /    40 tokens (   11.28 ms per token,    88.69 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     452.41 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 695 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     650.38 ms /    40 tokens (   16.26 ms per token,    61.50 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     651.91 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 696 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     409.42 ms /    40 tokens (   10.24 ms per token,    97.70 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     410.75 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 697 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     434.84 ms /    40 tokens (   10.87 ms per token,    91.99 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     436.23 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 698 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     641.98 ms /    40 tokens (   16.05 ms per token,    62.31 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     643.35 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 699 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     454.25 ms /    40 tokens (   11.36 ms per token,    88.06 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     455.86 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 700 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     613.30 ms /    40 tokens (   15.33 ms per token,    65.22 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     614.55 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 701 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     440.46 ms /    40 tokens (   11.01 ms per token,    90.81 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     442.21 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 702 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     676.57 ms /    40 tokens (   16.91 ms per token,    59.12 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     677.94 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 703 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     449.30 ms /    40 tokens (   11.23 ms per token,    89.03 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     450.43 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 704 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     635.45 ms /    40 tokens (   15.89 ms per token,    62.95 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     636.61 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 705 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     454.81 ms /    40 tokens (   11.37 ms per token,    87.95 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     456.45 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 706 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     613.49 ms /    40 tokens (   15.34 ms per token,    65.20 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     615.18 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 707 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     451.73 ms /    40 tokens (   11.29 ms per token,    88.55 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     453.19 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 708 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     659.37 ms /    40 tokens (   16.48 ms per token,    60.66 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     660.78 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 709 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     401.20 ms /    40 tokens (   10.03 ms per token,    99.70 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     402.55 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 710 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     440.64 ms /    40 tokens (   11.02 ms per token,    90.78 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     442.13 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 711 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     658.01 ms /    40 tokens (   16.45 ms per token,    60.79 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     659.42 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 712 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     453.56 ms /    40 tokens (   11.34 ms per token,    88.19 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     454.91 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 713 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     650.26 ms /    40 tokens (   16.26 ms per token,    61.51 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     651.57 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 714 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     437.70 ms /    40 tokens (   10.94 ms per token,    91.39 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     439.19 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 715 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     638.28 ms /    40 tokens (   15.96 ms per token,    62.67 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     639.44 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 716 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     443.06 ms /    40 tokens (   11.08 ms per token,    90.28 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     444.69 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 717 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     629.63 ms /    40 tokens (   15.74 ms per token,    63.53 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     631.07 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 718 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     439.16 ms /    39 tokens (   11.26 ms per token,    88.81 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     440.59 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 719 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     658.36 ms /    39 tokens (   16.88 ms per token,    59.24 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     659.96 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 720 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     412.87 ms /    39 tokens (   10.59 ms per token,    94.46 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     414.30 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 721 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     443.44 ms /    39 tokens (   11.37 ms per token,    87.95 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     444.80 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 722 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     656.58 ms /    39 tokens (   16.84 ms per token,    59.40 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     658.10 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 723 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     451.04 ms /    39 tokens (   11.57 ms per token,    86.47 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     452.47 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 724 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     652.95 ms /    39 tokens (   16.74 ms per token,    59.73 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     654.36 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 725 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     410.37 ms /    39 tokens (   10.52 ms per token,    95.04 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     411.67 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 726 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     400.96 ms /    43 tokens (    9.32 ms per token,   107.24 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     401.86 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 727 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     412.85 ms /    43 tokens (    9.60 ms per token,   104.15 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     414.47 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 728 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     453.80 ms /    43 tokens (   10.55 ms per token,    94.76 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     455.47 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 729 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     624.68 ms /    43 tokens (   14.53 ms per token,    68.84 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     626.02 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 730 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.97 ms /    43 tokens (    9.28 ms per token,   107.78 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     400.38 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 731 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     450.63 ms /    43 tokens (   10.48 ms per token,    95.42 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     452.03 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 732 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     627.48 ms /    43 tokens (   14.59 ms per token,    68.53 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     628.76 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 733 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     448.48 ms /    43 tokens (   10.43 ms per token,    95.88 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     449.93 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 734 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     645.60 ms /    43 tokens (   15.01 ms per token,    66.60 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     646.93 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 735 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     438.38 ms /    43 tokens (   10.19 ms per token,    98.09 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     439.97 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 736 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     518.46 ms /    43 tokens (   12.06 ms per token,    82.94 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     519.60 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 737 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.97 ms /    43 tokens (    9.28 ms per token,   107.78 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     400.46 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 738 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     452.72 ms /    43 tokens (   10.53 ms per token,    94.98 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     454.18 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 739 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     408.85 ms /    43 tokens (    9.51 ms per token,   105.17 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     410.28 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 740 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     453.15 ms /    43 tokens (   10.54 ms per token,    94.89 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     454.59 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 741 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     437.05 ms /    43 tokens (   10.16 ms per token,    98.39 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     438.45 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 742 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     446.01 ms /    46 tokens (    9.70 ms per token,   103.14 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     447.59 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 743 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     672.48 ms /    46 tokens (   14.62 ms per token,    68.40 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     674.02 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 744 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     626.52 ms /    46 tokens (   13.62 ms per token,    73.42 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     627.97 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 745 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     453.73 ms /    46 tokens (    9.86 ms per token,   101.38 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     455.21 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 746 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     622.73 ms /    46 tokens (   13.54 ms per token,    73.87 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     624.16 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 747 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     400.29 ms /    46 tokens (    8.70 ms per token,   114.92 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     401.80 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 748 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     448.19 ms /    46 tokens (    9.74 ms per token,   102.64 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     449.73 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 749 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     641.72 ms /    46 tokens (   13.95 ms per token,    71.68 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     643.05 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 750 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 45 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     399.89 ms /    45 tokens (    8.89 ms per token,   112.53 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     401.34 ms /    46 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 751 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 45 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     413.96 ms /    45 tokens (    9.20 ms per token,   108.71 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     415.53 ms /    46 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 752 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 45 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     452.69 ms /    45 tokens (   10.06 ms per token,    99.41 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     455.76 ms /    46 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 753 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 45 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     562.83 ms /    45 tokens (   12.51 ms per token,    79.95 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     563.86 ms /    46 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 754 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 45 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     396.70 ms /    45 tokens (    8.82 ms per token,   113.44 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.20 ms /    46 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 755 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 45 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     408.96 ms /    45 tokens (    9.09 ms per token,   110.04 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     410.61 ms /    46 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 756 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 45 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     412.59 ms /    45 tokens (    9.17 ms per token,   109.07 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     414.05 ms /    46 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 757 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 45 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     452.82 ms /    45 tokens (   10.06 ms per token,    99.38 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     454.39 ms /    46 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 758 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 45 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     657.67 ms /    45 tokens (   14.61 ms per token,    68.42 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     659.13 ms /    46 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 759 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 45 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     413.84 ms /    45 tokens (    9.20 ms per token,   108.74 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     415.38 ms /    46 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 760 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 45 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     449.08 ms /    45 tokens (    9.98 ms per token,   100.20 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     450.17 ms /    46 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 761 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 45 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     623.12 ms /    45 tokens (   13.85 ms per token,    72.22 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     624.65 ms /    46 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 762 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 45 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     449.30 ms /    45 tokens (    9.98 ms per token,   100.16 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     450.84 ms /    46 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 763 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 45 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     659.23 ms /    45 tokens (   14.65 ms per token,    68.26 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     660.80 ms /    46 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 764 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 45 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     635.17 ms /    45 tokens (   14.11 ms per token,    70.85 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     636.53 ms /    46 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 765 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 48 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     409.97 ms /    48 tokens (    8.54 ms per token,   117.08 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     411.36 ms /    49 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 766 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 48 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     660.25 ms /    48 tokens (   13.76 ms per token,    72.70 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     662.14 ms /    49 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 767 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 48 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     452.42 ms /    48 tokens (    9.43 ms per token,   106.10 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     454.05 ms /    49 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 768 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 48 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     615.97 ms /    48 tokens (   12.83 ms per token,    77.93 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     617.54 ms /    49 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 769 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 48 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     447.39 ms /    48 tokens (    9.32 ms per token,   107.29 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     449.11 ms /    49 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 770 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 48 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     660.05 ms /    48 tokens (   13.75 ms per token,    72.72 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     661.60 ms /    49 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 771 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 48 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     632.75 ms /    48 tokens (   13.18 ms per token,    75.86 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     634.20 ms /    49 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 772 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 48 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     446.21 ms /    48 tokens (    9.30 ms per token,   107.57 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     447.32 ms /    49 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 773 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     595.22 ms /    43 tokens (   13.84 ms per token,    72.24 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     596.60 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 774 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     408.00 ms /    43 tokens (    9.49 ms per token,   105.39 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     409.54 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 775 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     452.93 ms /    43 tokens (   10.53 ms per token,    94.94 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     454.84 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 776 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     638.12 ms /    43 tokens (   14.84 ms per token,    67.39 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     639.54 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 777 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     450.95 ms /    43 tokens (   10.49 ms per token,    95.35 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     452.39 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 778 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     645.20 ms /    43 tokens (   15.00 ms per token,    66.65 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     646.59 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 779 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     453.82 ms /    43 tokens (   10.55 ms per token,    94.75 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     455.19 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 780 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 45 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     642.56 ms /    45 tokens (   14.28 ms per token,    70.03 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     644.10 ms /    46 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 781 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 45 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     452.16 ms /    45 tokens (   10.05 ms per token,    99.52 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     453.63 ms /    46 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 782 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 45 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     642.33 ms /    45 tokens (   14.27 ms per token,    70.06 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     643.82 ms /    46 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 783 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 45 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     452.76 ms /    45 tokens (   10.06 ms per token,    99.39 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     454.28 ms /    46 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 784 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 45 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     642.60 ms /    45 tokens (   14.28 ms per token,    70.03 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     644.13 ms /    46 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 785 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 45 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     399.10 ms /    45 tokens (    8.87 ms per token,   112.75 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     400.55 ms /    46 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 786 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 45 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.76 ms /    45 tokens (    8.86 ms per token,   112.85 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     400.34 ms /    46 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 787 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     437.45 ms /    42 tokens (   10.42 ms per token,    96.01 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     438.89 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 788 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     682.05 ms /    42 tokens (   16.24 ms per token,    61.58 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     683.42 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 789 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     649.85 ms /    42 tokens (   15.47 ms per token,    64.63 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     651.52 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 790 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     406.99 ms /    42 tokens (    9.69 ms per token,   103.20 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     408.43 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 791 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     452.93 ms /    42 tokens (   10.78 ms per token,    92.73 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     454.73 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 792 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     624.39 ms /    42 tokens (   14.87 ms per token,    67.27 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     625.75 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 793 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     412.86 ms /    42 tokens (    9.83 ms per token,   101.73 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     414.30 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 794 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     452.89 ms /    42 tokens (   10.78 ms per token,    92.74 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     454.27 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 795 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     626.18 ms /    42 tokens (   14.91 ms per token,    67.07 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     627.57 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 796 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     452.83 ms /    43 tokens (   10.53 ms per token,    94.96 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     454.28 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 797 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     625.57 ms /    43 tokens (   14.55 ms per token,    68.74 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     627.07 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 798 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.72 ms /    43 tokens (    9.27 ms per token,   107.84 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     400.11 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 799 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     413.15 ms /    43 tokens (    9.61 ms per token,   104.08 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     414.68 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 800 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     449.91 ms /    43 tokens (   10.46 ms per token,    95.57 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     451.34 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 801 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     669.62 ms /    43 tokens (   15.57 ms per token,    64.22 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     670.99 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 802 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     646.20 ms /    43 tokens (   15.03 ms per token,    66.54 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     647.51 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 803 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     649.29 ms /    43 tokens (   15.10 ms per token,    66.23 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     650.87 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 804 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     408.08 ms /    41 tokens (    9.95 ms per token,   100.47 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     409.50 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 805 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     672.63 ms /    41 tokens (   16.41 ms per token,    60.95 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     674.69 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 806 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     450.65 ms /    41 tokens (   10.99 ms per token,    90.98 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     452.08 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 807 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     625.65 ms /    41 tokens (   15.26 ms per token,    65.53 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     627.27 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 808 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     453.21 ms /    41 tokens (   11.05 ms per token,    90.47 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     454.62 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 809 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     614.62 ms /    41 tokens (   14.99 ms per token,    66.71 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     616.03 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 810 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     407.66 ms /    40 tokens (   10.19 ms per token,    98.12 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     409.09 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 811 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     412.21 ms /    40 tokens (   10.31 ms per token,    97.04 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     413.64 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 812 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     450.33 ms /    40 tokens (   11.26 ms per token,    88.82 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     451.72 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 813 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     630.33 ms /    40 tokens (   15.76 ms per token,    63.46 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     631.77 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 814 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.09 ms /    40 tokens (    9.93 ms per token,   100.73 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.50 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 815 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.07 ms /    40 tokens (    9.95 ms per token,   100.48 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.38 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 816 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.79 ms /    40 tokens (    9.97 ms per token,   100.30 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     400.27 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 817 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.47 ms /    40 tokens (    9.94 ms per token,   100.64 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.86 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 818 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     453.62 ms /    44 tokens (   10.31 ms per token,    97.00 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     455.49 ms /    45 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 819 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     677.19 ms /    44 tokens (   15.39 ms per token,    64.97 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     678.92 ms /    45 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 820 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     636.97 ms /    44 tokens (   14.48 ms per token,    69.08 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     638.40 ms /    45 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 821 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     657.08 ms /    44 tokens (   14.93 ms per token,    66.96 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     658.54 ms /    45 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 822 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     450.97 ms /    44 tokens (   10.25 ms per token,    97.57 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     452.42 ms /    45 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 823 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     642.31 ms /    44 tokens (   14.60 ms per token,    68.50 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     643.76 ms /    45 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 824 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     452.95 ms /    44 tokens (   10.29 ms per token,    97.14 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     454.39 ms /    45 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 825 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     610.61 ms /    44 tokens (   13.88 ms per token,    72.06 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     611.91 ms /    45 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 826 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 50 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     454.65 ms /    50 tokens (    9.09 ms per token,   109.97 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     456.44 ms /    51 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 827 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 50 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     669.11 ms /    50 tokens (   13.38 ms per token,    74.73 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     670.63 ms /    51 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 828 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 50 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     453.54 ms /    50 tokens (    9.07 ms per token,   110.24 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     456.69 ms /    51 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 829 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 50 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     590.83 ms /    50 tokens (   11.82 ms per token,    84.63 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     592.53 ms /    51 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 830 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 50 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     412.91 ms /    50 tokens (    8.26 ms per token,   121.09 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     414.48 ms /    51 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 831 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     674.80 ms /    40 tokens (   16.87 ms per token,    59.28 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     676.22 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 832 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     637.37 ms /    40 tokens (   15.93 ms per token,    62.76 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     638.74 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 833 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     408.98 ms /    40 tokens (   10.22 ms per token,    97.80 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     410.31 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 834 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     402.29 ms /    40 tokens (   10.06 ms per token,    99.43 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     403.72 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 835 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     696.03 ms /    40 tokens (   17.40 ms per token,    57.47 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     697.62 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 836 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     635.97 ms /    40 tokens (   15.90 ms per token,    62.90 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     637.34 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 837 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     446.73 ms /    40 tokens (   11.17 ms per token,    89.54 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     448.15 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 838 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     628.84 ms /    43 tokens (   14.62 ms per token,    68.38 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     630.23 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 839 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     400.38 ms /    43 tokens (    9.31 ms per token,   107.40 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     403.67 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 840 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     663.16 ms /    43 tokens (   15.42 ms per token,    64.84 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     664.66 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 841 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     453.57 ms /    43 tokens (   10.55 ms per token,    94.80 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     454.90 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 842 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     482.25 ms /    43 tokens (   11.22 ms per token,    89.17 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     483.24 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 843 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     598.32 ms /    43 tokens (   13.91 ms per token,    71.87 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     599.74 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 844 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     400.87 ms /    44 tokens (    9.11 ms per token,   109.76 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     402.36 ms /    45 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 845 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     413.49 ms /    44 tokens (    9.40 ms per token,   106.41 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     415.32 ms /    45 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 846 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     650.64 ms /    44 tokens (   14.79 ms per token,    67.63 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     652.14 ms /    45 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 847 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     440.13 ms /    44 tokens (   10.00 ms per token,    99.97 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     441.62 ms /    45 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 848 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     651.13 ms /    44 tokens (   14.80 ms per token,    67.57 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     652.76 ms /    45 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 849 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 52 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     415.36 ms /    52 tokens (    7.99 ms per token,   125.19 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     417.17 ms /    53 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 850 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 52 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     677.27 ms /    52 tokens (   13.02 ms per token,    76.78 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     679.20 ms /    53 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 851 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 52 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     411.46 ms /    52 tokens (    7.91 ms per token,   126.38 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     413.05 ms /    53 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 852 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 52 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     649.66 ms /    52 tokens (   12.49 ms per token,    80.04 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     651.28 ms /    53 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 853 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 52 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     460.36 ms /    52 tokens (    8.85 ms per token,   112.96 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     461.95 ms /    53 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 854 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 52 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     657.73 ms /    52 tokens (   12.65 ms per token,    79.06 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     659.43 ms /    53 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 855 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 52 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     410.91 ms /    52 tokens (    7.90 ms per token,   126.55 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     412.64 ms /    53 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 856 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 52 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     676.83 ms /    52 tokens (   13.02 ms per token,    76.83 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     678.64 ms /    53 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 857 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     673.97 ms /    44 tokens (   15.32 ms per token,    65.29 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     675.50 ms /    45 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 858 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     648.90 ms /    44 tokens (   14.75 ms per token,    67.81 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     650.26 ms /    45 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 859 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     450.75 ms /    44 tokens (   10.24 ms per token,    97.62 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     452.18 ms /    45 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 860 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     635.57 ms /    44 tokens (   14.44 ms per token,    69.23 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     637.21 ms /    45 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 861 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     396.64 ms /    44 tokens (    9.01 ms per token,   110.93 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.07 ms /    45 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 862 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     399.79 ms /    44 tokens (    9.09 ms per token,   110.06 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     401.34 ms /    45 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 863 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     392.83 ms /    44 tokens (    8.93 ms per token,   112.01 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     394.21 ms /    45 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 864 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     396.77 ms /    44 tokens (    9.02 ms per token,   110.90 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.27 ms /    45 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 865 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.67 ms /    44 tokens (    9.06 ms per token,   110.37 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.94 ms /    45 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 866 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     389.19 ms /    44 tokens (    8.85 ms per token,   113.05 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     392.32 ms /    45 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 867 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     399.13 ms /    44 tokens (    9.07 ms per token,   110.24 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     401.90 ms /    45 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 868 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     391.28 ms /    44 tokens (    8.89 ms per token,   112.45 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     392.81 ms /    45 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 869 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.30 ms /    44 tokens (    9.03 ms per token,   110.75 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.66 ms /    45 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 870 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.15 ms /    44 tokens (    9.05 ms per token,   110.51 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.50 ms /    45 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 871 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     399.21 ms /    44 tokens (    9.07 ms per token,   110.22 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     400.56 ms /    45 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 872 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.63 ms /    44 tokens (    9.06 ms per token,   110.38 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     400.28 ms /    45 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 873 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     392.10 ms /    44 tokens (    8.91 ms per token,   112.22 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     393.42 ms /    45 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 874 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     547.62 ms /    44 tokens (   12.45 ms per token,    80.35 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     548.91 ms /    45 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 875 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     399.69 ms /    44 tokens (    9.08 ms per token,   110.08 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     401.02 ms /    45 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 876 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     386.91 ms /    39 tokens (    9.92 ms per token,   100.80 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     389.97 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 877 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     392.67 ms /    39 tokens (   10.07 ms per token,    99.32 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     393.97 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 878 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.72 ms /    39 tokens (   10.22 ms per token,    97.81 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     401.80 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 879 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.85 ms /    39 tokens (   10.20 ms per token,    98.03 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.14 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 880 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     387.10 ms /    39 tokens (    9.93 ms per token,   100.75 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     388.39 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 881 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     386.22 ms /    39 tokens (    9.90 ms per token,   100.98 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     387.47 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 882 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     701.75 ms /    39 tokens (   17.99 ms per token,    55.58 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     703.07 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 883 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     396.38 ms /    39 tokens (   10.16 ms per token,    98.39 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     397.81 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 884 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     389.22 ms /    41 tokens (    9.49 ms per token,   105.34 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     390.56 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 885 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     392.22 ms /    41 tokens (    9.57 ms per token,   104.53 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     395.09 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 886 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     390.43 ms /    41 tokens (    9.52 ms per token,   105.01 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     394.14 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 887 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     395.88 ms /    41 tokens (    9.66 ms per token,   103.57 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     397.59 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 888 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     389.07 ms /    41 tokens (    9.49 ms per token,   105.38 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     390.35 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 889 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.42 ms /    41 tokens (    9.72 ms per token,   102.91 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.73 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 890 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.98 ms /    41 tokens (    9.71 ms per token,   103.02 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.23 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 891 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.70 ms /    41 tokens (    9.72 ms per token,   102.83 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     400.22 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 892 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.26 ms /    46 tokens (    8.66 ms per token,   115.50 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.99 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 893 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     399.24 ms /    46 tokens (    8.68 ms per token,   115.22 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     400.70 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 894 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.37 ms /    46 tokens (    8.64 ms per token,   115.76 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.79 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 895 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     399.16 ms /    46 tokens (    8.68 ms per token,   115.24 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     400.74 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 896 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     700.82 ms /    46 tokens (   15.24 ms per token,    65.64 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     702.30 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 897 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.33 ms /    46 tokens (    8.66 ms per token,   115.48 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.75 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 898 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     399.52 ms /    46 tokens (    8.69 ms per token,   115.14 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     400.95 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 899 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.00 ms /    46 tokens (    8.65 ms per token,   115.58 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.33 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 900 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     399.27 ms /    46 tokens (    8.68 ms per token,   115.21 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     400.65 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 901 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     399.19 ms /    46 tokens (    8.68 ms per token,   115.23 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     400.48 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 902 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.91 ms /    46 tokens (    8.67 ms per token,   115.32 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     400.26 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 903 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     399.27 ms /    46 tokens (    8.68 ms per token,   115.21 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     400.66 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 904 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     399.33 ms /    46 tokens (    8.68 ms per token,   115.19 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     400.71 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 905 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     567.21 ms /    46 tokens (   12.33 ms per token,    81.10 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     568.63 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 906 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     396.72 ms /    46 tokens (    8.62 ms per token,   115.95 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.53 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 907 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     391.55 ms /    46 tokens (    8.51 ms per token,   117.48 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     394.88 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 908 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     394.84 ms /    46 tokens (    8.58 ms per token,   116.50 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.20 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 909 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     392.71 ms /    41 tokens (    9.58 ms per token,   104.40 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     394.04 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 910 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     387.31 ms /    41 tokens (    9.45 ms per token,   105.86 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     390.55 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 911 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     393.22 ms /    41 tokens (    9.59 ms per token,   104.27 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     394.52 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 912 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     386.05 ms /    41 tokens (    9.42 ms per token,   106.20 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     387.15 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 913 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     395.86 ms /    41 tokens (    9.66 ms per token,   103.57 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     397.37 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 914 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     388.15 ms /    41 tokens (    9.47 ms per token,   105.63 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     389.22 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 915 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     396.92 ms /    41 tokens (    9.68 ms per token,   103.29 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.27 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 916 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 50 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     539.52 ms /    50 tokens (   10.79 ms per token,    92.68 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     540.97 ms /    51 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 917 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 50 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     395.25 ms /    50 tokens (    7.90 ms per token,   126.50 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     396.71 ms /    51 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 918 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 50 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.10 ms /    50 tokens (    7.94 ms per token,   125.91 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.59 ms /    51 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 919 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 50 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     395.98 ms /    50 tokens (    7.92 ms per token,   126.27 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     397.42 ms /    51 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 920 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 50 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.64 ms /    50 tokens (    7.97 ms per token,   125.43 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     401.67 ms /    51 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 921 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 50 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     399.60 ms /    50 tokens (    7.99 ms per token,   125.13 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     401.08 ms /    51 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 922 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 50 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.85 ms /    50 tokens (    7.96 ms per token,   125.68 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.34 ms /    51 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 923 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 50 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     396.15 ms /    50 tokens (    7.92 ms per token,   126.21 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     397.63 ms /    51 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 924 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     395.08 ms /    43 tokens (    9.19 ms per token,   108.84 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.31 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 925 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     394.66 ms /    43 tokens (    9.18 ms per token,   108.96 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     395.96 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 926 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.59 ms /    43 tokens (    9.27 ms per token,   107.88 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.99 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 927 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     387.68 ms /    43 tokens (    9.02 ms per token,   110.92 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     389.04 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 928 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     548.66 ms /    43 tokens (   12.76 ms per token,    78.37 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     550.00 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 929 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     399.17 ms /    43 tokens (    9.28 ms per token,   107.72 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     400.44 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 930 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.48 ms /    43 tokens (    9.27 ms per token,   107.91 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.79 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 931 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     400.15 ms /    43 tokens (    9.31 ms per token,   107.46 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     401.50 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 932 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 45 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.36 ms /    45 tokens (    8.85 ms per token,   112.96 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.66 ms /    46 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 933 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 45 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.95 ms /    45 tokens (    8.84 ms per token,   113.08 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.49 ms /    46 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 934 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 45 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     390.56 ms /    45 tokens (    8.68 ms per token,   115.22 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     393.63 ms /    46 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 935 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 45 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     392.46 ms /    45 tokens (    8.72 ms per token,   114.66 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     393.82 ms /    46 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 936 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 45 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.26 ms /    45 tokens (    8.85 ms per token,   112.99 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.69 ms /    46 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 937 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 45 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     399.48 ms /    45 tokens (    8.88 ms per token,   112.65 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     400.88 ms /    46 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 938 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 45 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.18 ms /    45 tokens (    8.83 ms per token,   113.30 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.65 ms /    46 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 939 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     540.23 ms /    39 tokens (   13.85 ms per token,    72.19 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     541.34 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 940 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     396.55 ms /    39 tokens (   10.17 ms per token,    98.35 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     397.84 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 941 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     387.34 ms /    39 tokens (    9.93 ms per token,   100.69 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     388.72 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 942 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     395.25 ms /    39 tokens (   10.13 ms per token,    98.67 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.28 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 943 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     396.86 ms /    39 tokens (   10.18 ms per token,    98.27 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.19 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 944 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     395.65 ms /    39 tokens (   10.14 ms per token,    98.57 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.75 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 945 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     395.66 ms /    39 tokens (   10.15 ms per token,    98.57 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     397.28 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 946 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     393.34 ms /    39 tokens (   10.09 ms per token,    99.15 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     394.58 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 947 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     385.78 ms /    39 tokens (    9.89 ms per token,   101.09 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     387.06 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 948 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     710.94 ms /    39 tokens (   18.23 ms per token,    54.86 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     712.18 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 949 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     386.81 ms /    39 tokens (    9.92 ms per token,   100.83 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     388.14 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 950 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.38 ms /    39 tokens (   10.19 ms per token,    98.14 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.60 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 951 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     387.26 ms /    39 tokens (    9.93 ms per token,   100.71 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     388.57 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 952 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     386.07 ms /    39 tokens (    9.90 ms per token,   101.02 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     387.28 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 953 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     547.45 ms /    39 tokens (   14.04 ms per token,    71.24 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     548.73 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 954 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     387.98 ms /    39 tokens (    9.95 ms per token,   100.52 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     389.57 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 955 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     394.89 ms /    41 tokens (    9.63 ms per token,   103.83 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     396.43 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 956 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     387.56 ms /    41 tokens (    9.45 ms per token,   105.79 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     389.04 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 957 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     391.12 ms /    41 tokens (    9.54 ms per token,   104.83 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     392.39 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 958 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     399.23 ms /    41 tokens (    9.74 ms per token,   102.70 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     400.91 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 959 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     395.53 ms /    41 tokens (    9.65 ms per token,   103.66 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     396.89 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 960 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.35 ms /    41 tokens (    9.69 ms per token,   103.18 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.89 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 961 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     389.62 ms /    41 tokens (    9.50 ms per token,   105.23 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     390.91 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 962 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.03 ms /    41 tokens (    9.68 ms per token,   103.27 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.52 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 963 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     391.91 ms /    46 tokens (    8.52 ms per token,   117.37 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     395.18 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 964 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     573.94 ms /    46 tokens (   12.48 ms per token,    80.15 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     575.37 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 965 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.77 ms /    46 tokens (    8.67 ms per token,   115.36 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     400.33 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 966 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     390.72 ms /    46 tokens (    8.49 ms per token,   117.73 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     394.09 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 967 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     395.49 ms /    46 tokens (    8.60 ms per token,   116.31 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     396.87 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 968 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.11 ms /    46 tokens (    8.65 ms per token,   115.55 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.48 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 969 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.87 ms /    46 tokens (    8.65 ms per token,   115.62 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.24 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 970 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     399.04 ms /    46 tokens (    8.67 ms per token,   115.28 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     400.48 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 971 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.74 ms /    41 tokens (    9.70 ms per token,   103.08 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.04 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 972 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     388.03 ms /    41 tokens (    9.46 ms per token,   105.66 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     389.35 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 973 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     650.04 ms /    41 tokens (   15.85 ms per token,    63.07 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     651.32 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 974 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     387.10 ms /    41 tokens (    9.44 ms per token,   105.92 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     388.39 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 975 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     396.86 ms /    41 tokens (    9.68 ms per token,   103.31 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.38 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 976 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     390.46 ms /    41 tokens (    9.52 ms per token,   105.00 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     391.78 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 977 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.74 ms /    41 tokens (    9.73 ms per token,   102.82 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     400.24 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 978 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     390.42 ms /    42 tokens (    9.30 ms per token,   107.58 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     393.62 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 979 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     390.97 ms /    42 tokens (    9.31 ms per token,   107.42 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     392.35 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 980 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     391.36 ms /    42 tokens (    9.32 ms per token,   107.32 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     392.66 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 981 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     396.31 ms /    40 tokens (    9.91 ms per token,   100.93 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     397.84 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 982 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     589.50 ms /    40 tokens (   14.74 ms per token,    67.85 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     590.80 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 983 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     389.43 ms /    40 tokens (    9.74 ms per token,   102.71 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     392.16 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 984 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     399.22 ms /    40 tokens (    9.98 ms per token,   100.19 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     401.83 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 985 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     391.29 ms /    40 tokens (    9.78 ms per token,   102.23 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     393.99 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 986 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     396.12 ms /    40 tokens (    9.90 ms per token,   100.98 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     397.34 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 987 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     386.98 ms /    40 tokens (    9.67 ms per token,   103.37 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     388.22 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 988 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     399.61 ms /    40 tokens (    9.99 ms per token,   100.10 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     402.77 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 989 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.46 ms /    40 tokens (    9.94 ms per token,   100.64 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.70 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 990 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     387.03 ms /    40 tokens (    9.68 ms per token,   103.35 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     388.28 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 991 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     396.77 ms /    40 tokens (    9.92 ms per token,   100.81 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.00 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 992 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     386.80 ms /    40 tokens (    9.67 ms per token,   103.41 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     388.06 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 993 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     522.40 ms /    40 tokens (   13.06 ms per token,    76.57 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     523.55 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 994 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.98 ms /    41 tokens (    9.71 ms per token,   103.02 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.27 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 995 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.80 ms /    41 tokens (    9.73 ms per token,   102.81 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     400.07 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 996 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.64 ms /    41 tokens (    9.72 ms per token,   102.85 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.92 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 997 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     386.76 ms /    41 tokens (    9.43 ms per token,   106.01 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     388.07 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 998 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     396.39 ms /    41 tokens (    9.67 ms per token,   103.43 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     397.69 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 999 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.64 ms /    41 tokens (    9.72 ms per token,   102.85 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.91 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1000 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     570.09 ms /    41 tokens (   13.90 ms per token,    71.92 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     571.34 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1001 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     396.63 ms /    42 tokens (    9.44 ms per token,   105.89 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     397.93 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1002 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     388.64 ms /    42 tokens (    9.25 ms per token,   108.07 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     390.22 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1003 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     395.40 ms /    42 tokens (    9.41 ms per token,   106.22 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     396.74 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1004 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     399.35 ms /    42 tokens (    9.51 ms per token,   105.17 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     401.04 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1005 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     395.93 ms /    42 tokens (    9.43 ms per token,   106.08 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     397.26 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1006 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     399.88 ms /    42 tokens (    9.52 ms per token,   105.03 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     402.46 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1007 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     396.19 ms /    42 tokens (    9.43 ms per token,   106.01 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     397.48 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1008 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     399.11 ms /    42 tokens (    9.50 ms per token,   105.24 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     400.69 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1009 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     391.73 ms /    46 tokens (    8.52 ms per token,   117.43 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     395.20 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1010 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     395.53 ms /    46 tokens (    8.60 ms per token,   116.30 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     397.15 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1011 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     400.17 ms /    46 tokens (    8.70 ms per token,   114.95 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     401.62 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1012 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     399.20 ms /    46 tokens (    8.68 ms per token,   115.23 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     400.85 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1013 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     388.56 ms /    46 tokens (    8.45 ms per token,   118.39 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     389.98 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1014 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     395.79 ms /    46 tokens (    8.60 ms per token,   116.22 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     397.34 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1015 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     539.14 ms /    46 tokens (   11.72 ms per token,    85.32 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     540.56 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1016 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     392.68 ms /    42 tokens (    9.35 ms per token,   106.96 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     395.40 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1017 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     395.35 ms /    42 tokens (    9.41 ms per token,   106.23 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     396.73 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1018 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.98 ms /    42 tokens (    9.48 ms per token,   105.53 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.34 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1019 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     387.00 ms /    42 tokens (    9.21 ms per token,   108.53 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     388.34 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1020 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     390.80 ms /    42 tokens (    9.30 ms per token,   107.47 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     393.68 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1021 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     395.24 ms /    42 tokens (    9.41 ms per token,   106.27 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     396.55 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1022 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 47 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     399.98 ms /    47 tokens (    8.51 ms per token,   117.51 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     401.43 ms /    48 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1023 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 47 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     399.61 ms /    47 tokens (    8.50 ms per token,   117.61 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     401.03 ms /    48 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1024 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 47 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     399.35 ms /    47 tokens (    8.50 ms per token,   117.69 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     400.88 ms /    48 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1025 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 47 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     566.76 ms /    47 tokens (   12.06 ms per token,    82.93 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     568.21 ms /    48 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1026 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 47 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     395.15 ms /    47 tokens (    8.41 ms per token,   118.94 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     396.61 ms /    48 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1027 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 47 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     396.17 ms /    47 tokens (    8.43 ms per token,   118.64 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     397.66 ms /    48 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1028 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 47 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.26 ms /    47 tokens (    8.45 ms per token,   118.31 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.70 ms /    48 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1029 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 47 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.88 ms /    47 tokens (    8.47 ms per token,   118.13 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.32 ms /    48 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1030 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 47 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.32 ms /    47 tokens (    8.45 ms per token,   118.29 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.79 ms /    48 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1031 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 47 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     396.91 ms /    47 tokens (    8.44 ms per token,   118.41 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.35 ms /    48 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1032 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 47 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     399.53 ms /    47 tokens (    8.50 ms per token,   117.64 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     402.38 ms /    48 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1033 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 47 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     390.30 ms /    47 tokens (    8.30 ms per token,   120.42 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     391.66 ms /    48 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1034 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 47 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     396.17 ms /    47 tokens (    8.43 ms per token,   118.64 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     397.65 ms /    48 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1035 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 47 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     585.56 ms /    47 tokens (   12.46 ms per token,    80.26 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     587.10 ms /    48 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1036 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 47 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     396.59 ms /    47 tokens (    8.44 ms per token,   118.51 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.06 ms /    48 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1037 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     394.68 ms /    41 tokens (    9.63 ms per token,   103.88 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     396.19 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1038 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     393.88 ms /    41 tokens (    9.61 ms per token,   104.09 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     395.19 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1039 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.81 ms /    41 tokens (    9.70 ms per token,   103.06 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.36 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1040 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     390.42 ms /    41 tokens (    9.52 ms per token,   105.01 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     391.77 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1041 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     396.94 ms /    41 tokens (    9.68 ms per token,   103.29 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.32 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1042 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     394.79 ms /    41 tokens (    9.63 ms per token,   103.85 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     396.09 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1043 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     386.71 ms /    40 tokens (    9.67 ms per token,   103.44 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     388.34 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1044 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     627.18 ms /    40 tokens (   15.68 ms per token,    63.78 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     628.56 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1045 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     387.84 ms /    40 tokens (    9.70 ms per token,   103.14 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     389.07 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1046 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.73 ms /    40 tokens (    9.97 ms per token,   100.32 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     401.29 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1047 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     391.38 ms /    40 tokens (    9.78 ms per token,   102.20 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     392.63 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1048 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     387.47 ms /    40 tokens (    9.69 ms per token,   103.23 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     388.69 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1049 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     395.92 ms /    40 tokens (    9.90 ms per token,   101.03 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     397.22 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1050 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.19 ms /    39 tokens (   10.21 ms per token,    97.94 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.44 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1051 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     703.49 ms /    39 tokens (   18.04 ms per token,    55.44 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     704.75 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1052 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.95 ms /    39 tokens (   10.20 ms per token,    98.00 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.31 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1053 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     395.63 ms /    39 tokens (   10.14 ms per token,    98.58 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     396.84 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1054 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     387.44 ms /    39 tokens (    9.93 ms per token,   100.66 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     389.08 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1055 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     387.87 ms /    39 tokens (    9.95 ms per token,   100.55 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     390.41 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1056 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     388.37 ms /    39 tokens (    9.96 ms per token,   100.42 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     389.74 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1057 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     390.72 ms /    39 tokens (   10.02 ms per token,    99.82 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     391.92 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1058 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.55 ms /    39 tokens (   10.19 ms per token,    98.10 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.85 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1059 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     395.44 ms /    39 tokens (   10.14 ms per token,    98.62 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     396.57 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1060 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     569.38 ms /    39 tokens (   14.60 ms per token,    68.50 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     570.71 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1061 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     386.96 ms /    39 tokens (    9.92 ms per token,   100.79 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     388.29 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1062 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.45 ms /    39 tokens (   10.19 ms per token,    98.13 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.69 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1063 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     387.70 ms /    39 tokens (    9.94 ms per token,   100.59 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     390.78 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1064 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     395.20 ms /    41 tokens (    9.64 ms per token,   103.74 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     396.47 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1065 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.66 ms /    41 tokens (    9.72 ms per token,   102.85 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.96 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1066 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     386.85 ms /    41 tokens (    9.44 ms per token,   105.98 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     388.10 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1067 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     628.80 ms /    41 tokens (   15.34 ms per token,    65.20 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     630.20 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1068 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     399.72 ms /    41 tokens (    9.75 ms per token,   102.57 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     402.66 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1069 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     389.36 ms /    41 tokens (    9.50 ms per token,   105.30 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     391.11 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1070 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     395.87 ms /    41 tokens (    9.66 ms per token,   103.57 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     397.23 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1071 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     399.39 ms /    41 tokens (    9.74 ms per token,   102.66 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     401.04 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1072 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.53 ms /    41 tokens (    9.72 ms per token,   102.88 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     401.13 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1073 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     399.18 ms /    41 tokens (    9.74 ms per token,   102.71 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     401.78 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1074 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     392.35 ms /    41 tokens (    9.57 ms per token,   104.50 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     393.69 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1075 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.38 ms /    41 tokens (    9.69 ms per token,   103.18 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.12 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1076 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.02 ms /    41 tokens (    9.71 ms per token,   103.01 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.36 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1077 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     399.68 ms /    41 tokens (    9.75 ms per token,   102.58 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     402.25 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1078 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     394.31 ms /    41 tokens (    9.62 ms per token,   103.98 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     395.61 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1079 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 45 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     399.29 ms /    45 tokens (    8.87 ms per token,   112.70 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     400.95 ms /    46 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1080 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 45 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     399.38 ms /    45 tokens (    8.88 ms per token,   112.67 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     400.80 ms /    46 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1081 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 45 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.76 ms /    45 tokens (    8.86 ms per token,   112.85 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     400.15 ms /    46 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1082 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 45 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.02 ms /    45 tokens (    8.84 ms per token,   113.06 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.46 ms /    46 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1083 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 45 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     399.09 ms /    45 tokens (    8.87 ms per token,   112.76 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     400.75 ms /    46 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1084 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 45 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.16 ms /    45 tokens (    8.83 ms per token,   113.31 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.60 ms /    46 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1085 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 45 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.69 ms /    45 tokens (    8.86 ms per token,   112.87 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     400.22 ms /    46 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1086 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 45 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.09 ms /    45 tokens (    8.85 ms per token,   113.04 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.46 ms /    46 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1087 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 45 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.25 ms /    45 tokens (    8.85 ms per token,   113.00 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.85 ms /    46 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1088 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 45 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     393.95 ms /    45 tokens (    8.75 ms per token,   114.23 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     395.40 ms /    46 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1089 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 45 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.69 ms /    45 tokens (    8.84 ms per token,   113.15 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.15 ms /    46 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1090 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 45 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     562.59 ms /    45 tokens (   12.50 ms per token,    79.99 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     564.02 ms /    46 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1091 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 45 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     396.91 ms /    45 tokens (    8.82 ms per token,   113.38 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.29 ms /    46 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1092 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.84 ms /    44 tokens (    9.06 ms per token,   110.32 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     400.23 ms /    45 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1093 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.40 ms /    44 tokens (    9.05 ms per token,   110.44 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.73 ms /    45 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1094 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.60 ms /    44 tokens (    9.06 ms per token,   110.39 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     401.72 ms /    45 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1095 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     395.75 ms /    44 tokens (    8.99 ms per token,   111.18 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.05 ms /    45 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1096 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.34 ms /    44 tokens (    9.03 ms per token,   110.74 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     400.60 ms /    45 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1097 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     394.74 ms /    44 tokens (    8.97 ms per token,   111.47 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     396.07 ms /    45 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1098 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.98 ms /    44 tokens (    9.07 ms per token,   110.28 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     400.38 ms /    45 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1099 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.46 ms /    42 tokens (    9.49 ms per token,   105.40 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     401.65 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1100 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.65 ms /    42 tokens (    9.47 ms per token,   105.62 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     400.84 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1101 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     395.96 ms /    42 tokens (    9.43 ms per token,   106.07 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     397.31 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1102 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.76 ms /    42 tokens (    9.47 ms per token,   105.59 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.09 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1103 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     387.72 ms /    42 tokens (    9.23 ms per token,   108.33 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     389.04 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1104 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.53 ms /    42 tokens (    9.47 ms per token,   105.65 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.01 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1105 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     558.59 ms /    42 tokens (   13.30 ms per token,    75.19 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     559.94 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1106 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.44 ms /    42 tokens (    9.46 ms per token,   105.68 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     400.06 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1107 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     391.12 ms /    39 tokens (   10.03 ms per token,    99.71 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     392.34 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1108 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     387.07 ms /    39 tokens (    9.92 ms per token,   100.76 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     389.49 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1109 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     392.77 ms /    39 tokens (   10.07 ms per token,    99.30 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     394.08 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1110 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     393.19 ms /    39 tokens (   10.08 ms per token,    99.19 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     394.80 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1111 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     392.35 ms /    39 tokens (   10.06 ms per token,    99.40 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     393.57 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1112 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.92 ms /    39 tokens (   10.20 ms per token,    98.01 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.28 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1113 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     389.78 ms /    39 tokens (    9.99 ms per token,   100.06 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     391.02 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1114 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 47 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     388.81 ms /    47 tokens (    8.27 ms per token,   120.88 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     390.64 ms /    48 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1115 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 47 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     394.32 ms /    47 tokens (    8.39 ms per token,   119.19 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     395.75 ms /    48 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1116 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 47 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.35 ms /    47 tokens (    8.45 ms per token,   118.28 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.99 ms /    48 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1117 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 47 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     618.14 ms /    47 tokens (   13.15 ms per token,    76.03 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     619.61 ms /    48 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1118 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 47 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     396.80 ms /    47 tokens (    8.44 ms per token,   118.45 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.20 ms /    48 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1119 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 47 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.36 ms /    47 tokens (    8.48 ms per token,   117.98 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.79 ms /    48 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1120 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 47 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     396.81 ms /    47 tokens (    8.44 ms per token,   118.44 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.20 ms /    48 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1121 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 48 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.44 ms /    48 tokens (    8.30 ms per token,   120.47 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.88 ms /    49 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1122 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 48 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.39 ms /    48 tokens (    8.30 ms per token,   120.49 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.76 ms /    49 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1123 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 48 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     399.14 ms /    48 tokens (    8.32 ms per token,   120.26 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     402.42 ms /    49 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1124 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 48 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     395.50 ms /    48 tokens (    8.24 ms per token,   121.36 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     396.97 ms /    49 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1125 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 48 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.39 ms /    48 tokens (    8.28 ms per token,   120.79 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.78 ms /    49 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1126 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 48 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.96 ms /    48 tokens (    8.29 ms per token,   120.62 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.33 ms /    49 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1127 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 48 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     396.92 ms /    48 tokens (    8.27 ms per token,   120.93 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.59 ms /    49 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1128 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 48 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     615.26 ms /    48 tokens (   12.82 ms per token,    78.02 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     616.71 ms /    49 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1129 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 48 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     396.57 ms /    48 tokens (    8.26 ms per token,   121.04 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.10 ms /    49 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1130 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 48 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     399.92 ms /    48 tokens (    8.33 ms per token,   120.03 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     401.38 ms /    49 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1131 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 48 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.78 ms /    48 tokens (    8.31 ms per token,   120.37 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     401.66 ms /    49 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1132 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 48 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     392.37 ms /    48 tokens (    8.17 ms per token,   122.33 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     395.98 ms /    49 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1133 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 48 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     394.05 ms /    48 tokens (    8.21 ms per token,   121.81 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     395.65 ms /    49 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1134 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 48 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     390.28 ms /    48 tokens (    8.13 ms per token,   122.99 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     391.88 ms /    49 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1135 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 48 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     399.21 ms /    48 tokens (    8.32 ms per token,   120.24 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     400.84 ms /    49 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1136 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 48 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     391.76 ms /    48 tokens (    8.16 ms per token,   122.52 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     395.22 ms /    49 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1137 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 48 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     394.00 ms /    48 tokens (    8.21 ms per token,   121.83 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     395.41 ms /    49 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1138 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 48 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     394.27 ms /    48 tokens (    8.21 ms per token,   121.74 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     395.53 ms /    49 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1139 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 48 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.85 ms /    48 tokens (    8.29 ms per token,   120.65 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.44 ms /    49 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1140 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 48 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.25 ms /    48 tokens (    8.30 ms per token,   120.53 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.71 ms /    49 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1141 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 47 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.00 ms /    47 tokens (    8.45 ms per token,   118.39 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.44 ms /    48 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1142 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 47 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.54 ms /    47 tokens (    8.46 ms per token,   118.23 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.00 ms /    48 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1143 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 47 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     396.96 ms /    47 tokens (    8.45 ms per token,   118.40 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.45 ms /    48 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1144 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 47 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     393.62 ms /    47 tokens (    8.37 ms per token,   119.41 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     394.95 ms /    48 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1145 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 47 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.82 ms /    47 tokens (    8.46 ms per token,   118.14 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.38 ms /    48 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1146 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 47 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     562.52 ms /    47 tokens (   11.97 ms per token,    83.55 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     564.00 ms /    48 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1147 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 47 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     395.47 ms /    47 tokens (    8.41 ms per token,   118.84 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     396.96 ms /    48 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1148 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.59 ms /    46 tokens (    8.64 ms per token,   115.70 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.95 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1149 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     399.65 ms /    46 tokens (    8.69 ms per token,   115.10 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     403.01 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1150 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     394.95 ms /    46 tokens (    8.59 ms per token,   116.47 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     396.33 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1151 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.92 ms /    46 tokens (    8.67 ms per token,   115.31 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     402.32 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1152 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     395.62 ms /    46 tokens (    8.60 ms per token,   116.27 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     396.97 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1153 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.30 ms /    46 tokens (    8.64 ms per token,   115.78 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.67 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1154 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.57 ms /    46 tokens (    8.64 ms per token,   115.70 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.98 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1155 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     399.37 ms /    46 tokens (    8.68 ms per token,   115.18 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     400.77 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1156 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.46 ms /    46 tokens (    8.66 ms per token,   115.45 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.79 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1157 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     396.42 ms /    43 tokens (    9.22 ms per token,   108.47 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     397.67 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1158 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.63 ms /    43 tokens (    9.27 ms per token,   107.87 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.95 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1159 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     554.86 ms /    43 tokens (   12.90 ms per token,    77.50 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     556.13 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1160 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     396.73 ms /    43 tokens (    9.23 ms per token,   108.38 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.39 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1161 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     395.43 ms /    43 tokens (    9.20 ms per token,   108.74 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     396.77 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1162 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     392.48 ms /    43 tokens (    9.13 ms per token,   109.56 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     395.08 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1163 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     393.76 ms /    43 tokens (    9.16 ms per token,   109.20 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     395.10 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1164 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     399.50 ms /    44 tokens (    9.08 ms per token,   110.14 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     401.07 ms /    45 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1165 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     388.90 ms /    44 tokens (    8.84 ms per token,   113.14 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     390.24 ms /    45 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1166 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     396.20 ms /    44 tokens (    9.00 ms per token,   111.05 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     397.77 ms /    45 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1167 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     400.90 ms /    44 tokens (    9.11 ms per token,   109.75 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     404.16 ms /    45 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1168 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     396.11 ms /    44 tokens (    9.00 ms per token,   111.08 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     397.69 ms /    45 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1169 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     399.49 ms /    44 tokens (    9.08 ms per token,   110.14 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     400.83 ms /    45 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1170 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     399.07 ms /    44 tokens (    9.07 ms per token,   110.26 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     400.63 ms /    45 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1171 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     391.19 ms /    44 tokens (    8.89 ms per token,   112.48 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     392.60 ms /    45 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1172 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     392.04 ms /    44 tokens (    8.91 ms per token,   112.23 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     393.61 ms /    45 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1173 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.59 ms /    44 tokens (    9.06 ms per token,   110.39 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.98 ms /    45 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1174 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.09 ms /    44 tokens (    9.05 ms per token,   110.53 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.71 ms /    45 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1175 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     396.50 ms /    44 tokens (    9.01 ms per token,   110.97 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.76 ms /    45 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1176 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     546.97 ms /    44 tokens (   12.43 ms per token,    80.44 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     548.21 ms /    45 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1177 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     395.00 ms /    41 tokens (    9.63 ms per token,   103.80 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     396.33 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1178 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.03 ms /    41 tokens (    9.71 ms per token,   103.01 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.31 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1179 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.73 ms /    41 tokens (    9.73 ms per token,   102.83 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.95 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1180 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.68 ms /    41 tokens (    9.70 ms per token,   103.10 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.99 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1181 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.05 ms /    41 tokens (    9.71 ms per token,   103.00 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.43 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1182 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     389.08 ms /    44 tokens (    8.84 ms per token,   113.09 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     390.49 ms /    45 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1183 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.25 ms /    44 tokens (    9.03 ms per token,   110.76 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     400.58 ms /    45 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1184 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     595.06 ms /    44 tokens (   13.52 ms per token,    73.94 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     596.35 ms /    45 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1185 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     399.23 ms /    44 tokens (    9.07 ms per token,   110.21 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     400.75 ms /    45 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1186 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     390.87 ms /    44 tokens (    8.88 ms per token,   112.57 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     392.25 ms /    45 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1187 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.16 ms /    44 tokens (    9.05 ms per token,   110.51 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.63 ms /    45 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1188 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     390.86 ms /    42 tokens (    9.31 ms per token,   107.45 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     392.09 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1189 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.86 ms /    42 tokens (    9.47 ms per token,   105.57 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.32 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1190 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     389.41 ms /    42 tokens (    9.27 ms per token,   107.85 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     390.68 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1191 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     396.92 ms /    42 tokens (    9.45 ms per token,   105.82 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.15 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1192 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.84 ms /    42 tokens (    9.50 ms per token,   105.31 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     400.11 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1193 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     551.90 ms /    42 tokens (   13.14 ms per token,    76.10 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     553.19 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1194 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     396.93 ms /    42 tokens (    9.45 ms per token,   105.81 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.33 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1195 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 37 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     396.50 ms /    37 tokens (   10.72 ms per token,    93.32 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     397.86 ms /    38 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1196 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 37 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     385.88 ms /    37 tokens (   10.43 ms per token,    95.89 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     387.11 ms /    38 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1197 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 37 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     396.78 ms /    37 tokens (   10.72 ms per token,    93.25 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.01 ms /    38 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1198 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 37 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     396.74 ms /    37 tokens (   10.72 ms per token,    93.26 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     397.92 ms /    38 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1199 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 37 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     386.89 ms /    37 tokens (   10.46 ms per token,    95.63 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     388.34 ms /    38 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1200 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 37 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     390.71 ms /    37 tokens (   10.56 ms per token,    94.70 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     391.96 ms /    38 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1201 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     385.59 ms /    39 tokens (    9.89 ms per token,   101.14 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     386.97 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1202 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     544.69 ms /    39 tokens (   13.97 ms per token,    71.60 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     545.84 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1203 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     386.79 ms /    39 tokens (    9.92 ms per token,   100.83 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     388.00 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1204 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.74 ms /    39 tokens (   10.20 ms per token,    98.05 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.99 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1205 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     387.26 ms /    39 tokens (    9.93 ms per token,   100.71 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     388.49 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1206 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.14 ms /    39 tokens (   10.21 ms per token,    97.96 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.38 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1207 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     387.39 ms /    39 tokens (    9.93 ms per token,   100.67 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     388.69 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1208 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     577.48 ms /    39 tokens (   14.81 ms per token,    67.54 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     578.72 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1209 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 45 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.88 ms /    45 tokens (    8.86 ms per token,   112.82 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     400.26 ms /    46 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1210 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 45 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.59 ms /    45 tokens (    8.86 ms per token,   112.90 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     400.02 ms /    46 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1211 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 45 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.26 ms /    45 tokens (    8.85 ms per token,   112.99 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.60 ms /    46 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1212 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 45 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.17 ms /    45 tokens (    8.85 ms per token,   113.02 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.53 ms /    46 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1213 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 45 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.81 ms /    45 tokens (    8.86 ms per token,   112.84 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     400.17 ms /    46 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1214 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 45 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.96 ms /    45 tokens (    8.84 ms per token,   113.08 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.34 ms /    46 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1215 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 45 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.75 ms /    45 tokens (    8.86 ms per token,   112.85 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     400.04 ms /    46 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1216 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 45 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     534.20 ms /    45 tokens (   11.87 ms per token,    84.24 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     535.61 ms /    46 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1217 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     394.09 ms /    39 tokens (   10.10 ms per token,    98.96 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     395.30 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1218 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     387.76 ms /    39 tokens (    9.94 ms per token,   100.58 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     390.14 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1219 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     394.33 ms /    39 tokens (   10.11 ms per token,    98.90 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     397.37 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1220 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     386.44 ms /    39 tokens (    9.91 ms per token,   100.92 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     387.91 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1221 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     386.21 ms /    39 tokens (    9.90 ms per token,   100.98 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     387.40 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1222 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     386.22 ms /    39 tokens (    9.90 ms per token,   100.98 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     387.84 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1223 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     392.12 ms /    39 tokens (   10.05 ms per token,    99.46 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     393.39 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1224 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     386.90 ms /    39 tokens (    9.92 ms per token,   100.80 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     388.49 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1225 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 47 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     551.41 ms /    47 tokens (   11.73 ms per token,    85.24 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     552.79 ms /    48 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1226 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 47 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.75 ms /    47 tokens (    8.48 ms per token,   117.87 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     405.69 ms /    48 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1227 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 47 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     395.26 ms /    47 tokens (    8.41 ms per token,   118.91 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.65 ms /    48 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1228 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 47 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     394.45 ms /    47 tokens (    8.39 ms per token,   119.15 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     395.94 ms /    48 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1229 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 47 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     396.99 ms /    47 tokens (    8.45 ms per token,   118.39 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.45 ms /    48 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1230 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 47 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     396.65 ms /    47 tokens (    8.44 ms per token,   118.49 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.09 ms /    48 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1231 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     396.75 ms /    41 tokens (    9.68 ms per token,   103.34 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.88 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1232 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     386.68 ms /    41 tokens (    9.43 ms per token,   106.03 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     387.96 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1233 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.06 ms /    41 tokens (    9.68 ms per token,   103.26 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.42 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1234 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     388.41 ms /    41 tokens (    9.47 ms per token,   105.56 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     389.73 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1235 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     395.61 ms /    41 tokens (    9.65 ms per token,   103.64 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     397.05 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1236 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     386.98 ms /    41 tokens (    9.44 ms per token,   105.95 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     388.27 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1237 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     717.49 ms /    41 tokens (   17.50 ms per token,    57.14 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     718.84 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1238 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 38 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     396.56 ms /    38 tokens (   10.44 ms per token,    95.83 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.59 ms /    39 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1239 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 38 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     390.63 ms /    38 tokens (   10.28 ms per token,    97.28 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     391.95 ms /    39 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1240 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 38 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     393.88 ms /    38 tokens (   10.37 ms per token,    96.48 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     395.16 ms /    39 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1241 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 38 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     387.35 ms /    38 tokens (   10.19 ms per token,    98.10 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     388.99 ms /    39 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1242 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 38 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     393.94 ms /    38 tokens (   10.37 ms per token,    96.46 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     395.19 ms /    39 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1243 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 38 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     386.86 ms /    38 tokens (   10.18 ms per token,    98.23 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     388.48 ms /    39 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1244 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     394.11 ms /    39 tokens (   10.11 ms per token,    98.96 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     395.62 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1245 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     390.32 ms /    39 tokens (   10.01 ms per token,    99.92 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     392.22 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1246 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     396.45 ms /    39 tokens (   10.17 ms per token,    98.37 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     397.72 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1247 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     387.16 ms /    39 tokens (    9.93 ms per token,   100.73 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     388.90 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1248 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     512.70 ms /    39 tokens (   13.15 ms per token,    76.07 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     513.98 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1249 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     409.93 ms /    39 tokens (   10.51 ms per token,    95.14 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     420.50 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1250 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.27 ms /    39 tokens (   10.21 ms per token,    97.92 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     401.27 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1251 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     386.31 ms /    43 tokens (    8.98 ms per token,   111.31 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     387.59 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1252 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     388.93 ms /    43 tokens (    9.04 ms per token,   110.56 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     390.29 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1253 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     396.61 ms /    43 tokens (    9.22 ms per token,   108.42 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.37 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1254 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     399.06 ms /    43 tokens (    9.28 ms per token,   107.75 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     400.96 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1255 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.75 ms /    43 tokens (    9.25 ms per token,   108.11 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.47 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1256 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     396.87 ms /    43 tokens (    9.23 ms per token,   108.35 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.19 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1257 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     386.61 ms /    41 tokens (    9.43 ms per token,   106.05 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     387.95 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1258 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.18 ms /    41 tokens (    9.69 ms per token,   103.23 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.51 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1259 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.79 ms /    41 tokens (    9.73 ms per token,   102.81 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     400.08 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1260 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     387.93 ms /    41 tokens (    9.46 ms per token,   105.69 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     389.20 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1261 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     579.91 ms /    41 tokens (   14.14 ms per token,    70.70 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     581.30 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1262 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     387.13 ms /    41 tokens (    9.44 ms per token,   105.91 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     388.36 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1263 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.15 ms /    41 tokens (    9.69 ms per token,   103.24 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.38 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1264 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     387.58 ms /    41 tokens (    9.45 ms per token,   105.78 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     388.81 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1265 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.19 ms /    41 tokens (    9.69 ms per token,   103.23 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.46 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1266 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     388.07 ms /    41 tokens (    9.47 ms per token,   105.65 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     389.35 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1267 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     730.12 ms /    41 tokens (   17.81 ms per token,    56.16 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     731.43 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1268 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     388.07 ms /    41 tokens (    9.47 ms per token,   105.65 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     389.17 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1269 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     389.27 ms /    41 tokens (    9.49 ms per token,   105.32 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     390.58 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1270 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     384.21 ms /    41 tokens (    9.37 ms per token,   106.71 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     385.15 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1271 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     394.26 ms /    41 tokens (    9.62 ms per token,   103.99 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     397.44 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1272 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     395.12 ms /    41 tokens (    9.64 ms per token,   103.77 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     396.23 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1273 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     391.25 ms /    41 tokens (    9.54 ms per token,   104.79 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     392.71 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1274 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     390.77 ms /    41 tokens (    9.53 ms per token,   104.92 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     391.98 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1275 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     695.13 ms /    41 tokens (   16.95 ms per token,    58.98 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     696.54 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1276 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     593.38 ms /    41 tokens (   14.47 ms per token,    69.10 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     594.92 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1277 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     396.33 ms /    41 tokens (    9.67 ms per token,   103.45 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     397.70 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1278 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.62 ms /    41 tokens (    9.72 ms per token,   102.85 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.95 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1279 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.86 ms /    41 tokens (    9.70 ms per token,   103.05 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.17 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1280 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     388.45 ms /    44 tokens (    8.83 ms per token,   113.27 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     389.91 ms /    45 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1281 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     390.60 ms /    44 tokens (    8.88 ms per token,   112.65 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     392.19 ms /    45 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1282 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.69 ms /    44 tokens (    9.06 ms per token,   110.36 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     401.38 ms /    45 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1283 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     535.54 ms /    44 tokens (   12.17 ms per token,    82.16 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     536.95 ms /    45 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1284 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.17 ms /    44 tokens (    9.03 ms per token,   110.78 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.50 ms /    45 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1285 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.35 ms /    44 tokens (    9.03 ms per token,   110.73 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.65 ms /    45 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1286 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     399.55 ms /    44 tokens (    9.08 ms per token,   110.13 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     401.25 ms /    45 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1287 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 49 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     395.08 ms /    49 tokens (    8.06 ms per token,   124.03 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.04 ms /    50 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1288 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 49 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     396.91 ms /    49 tokens (    8.10 ms per token,   123.45 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.44 ms /    50 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1289 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 49 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     393.06 ms /    49 tokens (    8.02 ms per token,   124.66 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     394.49 ms /    50 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1290 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 49 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     395.86 ms /    49 tokens (    8.08 ms per token,   123.78 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     397.37 ms /    50 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1291 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 49 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     401.83 ms /    49 tokens (    8.20 ms per token,   121.94 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     403.27 ms /    50 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1292 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 49 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.05 ms /    49 tokens (    8.12 ms per token,   123.10 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.47 ms /    50 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1293 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 49 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     392.60 ms /    49 tokens (    8.01 ms per token,   124.81 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     394.10 ms /    50 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1294 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 49 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     396.91 ms /    49 tokens (    8.10 ms per token,   123.45 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.56 ms /    50 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1295 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     393.80 ms /    46 tokens (    8.56 ms per token,   116.81 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     395.23 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1296 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.39 ms /    46 tokens (    8.64 ms per token,   115.76 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.90 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1297 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     390.01 ms /    46 tokens (    8.48 ms per token,   117.94 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     391.42 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1298 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     626.83 ms /    46 tokens (   13.63 ms per token,    73.38 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     628.25 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1299 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     399.18 ms /    46 tokens (    8.68 ms per token,   115.24 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     400.60 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1300 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     399.07 ms /    46 tokens (    8.68 ms per token,   115.27 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     400.50 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1301 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.31 ms /    46 tokens (    8.66 ms per token,   115.49 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.68 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1302 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.31 ms /    46 tokens (    8.66 ms per token,   115.49 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.73 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1303 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     394.48 ms /    41 tokens (    9.62 ms per token,   103.93 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     395.76 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1304 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.73 ms /    41 tokens (    9.73 ms per token,   102.83 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.99 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1305 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     386.42 ms /    41 tokens (    9.42 ms per token,   106.10 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     387.67 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1306 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     516.03 ms /    41 tokens (   12.59 ms per token,    79.45 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     529.38 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1307 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.43 ms /    41 tokens (    9.72 ms per token,   102.90 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.74 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1308 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     387.02 ms /    41 tokens (    9.44 ms per token,   105.94 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     388.34 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1309 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     394.23 ms /    41 tokens (    9.62 ms per token,   104.00 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     395.63 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1310 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.08 ms /    41 tokens (    9.71 ms per token,   102.99 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.35 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1311 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     386.84 ms /    41 tokens (    9.44 ms per token,   105.99 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     388.18 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1312 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     581.08 ms /    41 tokens (   14.17 ms per token,    70.56 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     582.52 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1313 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.10 ms /    41 tokens (    9.71 ms per token,   102.99 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.55 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1314 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     389.74 ms /    41 tokens (    9.51 ms per token,   105.20 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     391.01 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1315 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     396.60 ms /    41 tokens (    9.67 ms per token,   103.38 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     397.90 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1316 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     395.52 ms /    41 tokens (    9.65 ms per token,   103.66 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.75 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1317 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     396.24 ms /    41 tokens (    9.66 ms per token,   103.47 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     397.91 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1318 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     396.22 ms /    41 tokens (    9.66 ms per token,   103.48 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     397.54 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1319 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     395.28 ms /    41 tokens (    9.64 ms per token,   103.72 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     396.79 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1320 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     387.76 ms /    41 tokens (    9.46 ms per token,   105.74 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     389.07 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1321 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     396.19 ms /    41 tokens (    9.66 ms per token,   103.49 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.75 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1322 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.20 ms /    41 tokens (    9.69 ms per token,   103.22 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     400.36 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1323 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     396.05 ms /    41 tokens (    9.66 ms per token,   103.52 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     397.62 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1324 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     386.62 ms /    40 tokens (    9.67 ms per token,   103.46 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     387.85 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1325 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     389.97 ms /    40 tokens (    9.75 ms per token,   102.57 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     391.65 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1326 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     394.63 ms /    40 tokens (    9.87 ms per token,   101.36 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     395.98 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1327 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.23 ms /    40 tokens (    9.96 ms per token,   100.44 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.92 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1328 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     538.90 ms /    40 tokens (   13.47 ms per token,    74.23 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     540.12 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1329 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.03 ms /    40 tokens (    9.95 ms per token,   100.49 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.31 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1330 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     389.10 ms /    40 tokens (    9.73 ms per token,   102.80 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     392.23 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1331 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     385.84 ms /    40 tokens (    9.65 ms per token,   103.67 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     386.70 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1332 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     399.32 ms /    40 tokens (    9.98 ms per token,   100.17 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     402.41 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1333 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     395.17 ms /    40 tokens (    9.88 ms per token,   101.22 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     396.30 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1334 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     388.28 ms /    40 tokens (    9.71 ms per token,   103.02 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     389.59 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1335 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     396.11 ms /    40 tokens (    9.90 ms per token,   100.98 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     397.41 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1336 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     387.44 ms /    40 tokens (    9.69 ms per token,   103.24 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     388.80 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1337 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     516.32 ms /    40 tokens (   12.91 ms per token,    77.47 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     517.35 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1338 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.62 ms /    40 tokens (    9.94 ms per token,   100.60 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.98 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1339 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     386.90 ms /    40 tokens (    9.67 ms per token,   103.39 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     388.16 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1340 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.37 ms /    42 tokens (    9.48 ms per token,   105.43 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     401.70 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1341 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     394.06 ms /    42 tokens (    9.38 ms per token,   106.58 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     397.29 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1342 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     395.64 ms /    42 tokens (    9.42 ms per token,   106.16 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.78 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1343 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     394.72 ms /    42 tokens (    9.40 ms per token,   106.40 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     397.31 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1344 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     389.39 ms /    42 tokens (    9.27 ms per token,   107.86 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     390.69 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1345 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     396.08 ms /    42 tokens (    9.43 ms per token,   106.04 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     397.37 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1346 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.65 ms /    42 tokens (    9.49 ms per token,   105.35 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.94 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1347 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.78 ms /    43 tokens (    9.27 ms per token,   107.83 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     400.10 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1348 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.07 ms /    43 tokens (    9.26 ms per token,   108.02 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.40 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1349 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.89 ms /    43 tokens (    9.28 ms per token,   107.80 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     400.55 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1350 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     528.49 ms /    43 tokens (   12.29 ms per token,    81.36 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     529.76 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1351 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.11 ms /    43 tokens (    9.24 ms per token,   108.28 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.41 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1352 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     399.97 ms /    43 tokens (    9.30 ms per token,   107.51 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     401.47 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1353 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     388.98 ms /    43 tokens (    9.05 ms per token,   110.55 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     390.29 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1354 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     395.94 ms /    43 tokens (    9.21 ms per token,   108.60 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.51 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1355 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     392.74 ms /    43 tokens (    9.13 ms per token,   109.49 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     394.10 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1356 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     393.28 ms /    43 tokens (    9.15 ms per token,   109.34 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     394.65 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1357 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     388.93 ms /    43 tokens (    9.04 ms per token,   110.56 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     390.33 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1358 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     396.55 ms /    43 tokens (    9.22 ms per token,   108.44 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.11 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1359 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     391.25 ms /    43 tokens (    9.10 ms per token,   109.90 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     392.57 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1360 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     396.75 ms /    43 tokens (    9.23 ms per token,   108.38 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.23 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1361 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     540.84 ms /    43 tokens (   12.58 ms per token,    79.51 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     542.18 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1362 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     389.03 ms /    43 tokens (    9.05 ms per token,   110.53 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     390.06 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1363 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     393.93 ms /    43 tokens (    9.16 ms per token,   109.16 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     395.26 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1364 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     393.40 ms /    43 tokens (    9.15 ms per token,   109.30 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     394.62 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1365 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 48 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     389.65 ms /    48 tokens (    8.12 ms per token,   123.19 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     391.19 ms /    49 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1366 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 48 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     387.86 ms /    48 tokens (    8.08 ms per token,   123.76 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     389.01 ms /    49 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1367 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 48 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     623.05 ms /    48 tokens (   12.98 ms per token,    77.04 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     624.54 ms /    49 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1368 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 48 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     395.55 ms /    48 tokens (    8.24 ms per token,   121.35 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     397.07 ms /    49 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1369 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 48 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     396.67 ms /    48 tokens (    8.26 ms per token,   121.01 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.12 ms /    49 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1370 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 48 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     396.79 ms /    48 tokens (    8.27 ms per token,   120.97 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.23 ms /    49 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1371 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 48 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.50 ms /    48 tokens (    8.30 ms per token,   120.45 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.97 ms /    49 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1372 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     395.32 ms /    39 tokens (   10.14 ms per token,    98.66 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     396.60 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1373 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     387.20 ms /    39 tokens (    9.93 ms per token,   100.72 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     388.45 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1374 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     550.34 ms /    39 tokens (   14.11 ms per token,    70.86 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     551.74 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1375 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     386.49 ms /    39 tokens (    9.91 ms per token,   100.91 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     387.86 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1376 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     393.87 ms /    39 tokens (   10.10 ms per token,    99.02 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     396.94 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1377 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     396.75 ms /    39 tokens (   10.17 ms per token,    98.30 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.31 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1378 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     390.21 ms /    39 tokens (   10.01 ms per token,    99.95 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     393.34 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1379 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     396.32 ms /    39 tokens (   10.16 ms per token,    98.41 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     397.69 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1380 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     390.23 ms /    39 tokens (   10.01 ms per token,    99.94 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     391.54 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1381 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     396.53 ms /    39 tokens (   10.17 ms per token,    98.35 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     397.92 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1382 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     388.11 ms /    39 tokens (    9.95 ms per token,   100.49 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     389.46 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1383 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.08 ms /    39 tokens (   10.18 ms per token,    98.22 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.54 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1384 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     703.84 ms /    39 tokens (   18.05 ms per token,    55.41 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     705.27 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1385 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     385.25 ms /    39 tokens (    9.88 ms per token,   101.23 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     386.54 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1386 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.81 ms /    39 tokens (   10.20 ms per token,    98.04 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.16 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1387 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     385.98 ms /    39 tokens (    9.90 ms per token,   101.04 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     387.30 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1388 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.79 ms /    39 tokens (   10.20 ms per token,    98.04 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.04 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1389 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     388.98 ms /    39 tokens (    9.97 ms per token,   100.26 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     392.07 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1390 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     711.07 ms /    39 tokens (   18.23 ms per token,    54.85 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     712.38 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1391 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     385.93 ms /    39 tokens (    9.90 ms per token,   101.06 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     387.23 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1392 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     396.60 ms /    39 tokens (   10.17 ms per token,    98.34 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.68 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1393 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     395.63 ms /    39 tokens (   10.14 ms per token,    98.58 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     396.88 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1394 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     386.65 ms /    39 tokens (    9.91 ms per token,   100.87 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     388.12 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1395 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.23 ms /    39 tokens (   10.19 ms per token,    98.18 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.51 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1396 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     386.88 ms /    39 tokens (    9.92 ms per token,   100.81 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     388.37 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1397 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     390.21 ms /    39 tokens (   10.01 ms per token,    99.95 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     391.45 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1398 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     564.90 ms /    39 tokens (   14.48 ms per token,    69.04 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     566.15 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1399 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     387.26 ms /    39 tokens (    9.93 ms per token,   100.71 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     388.53 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1400 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.27 ms /    39 tokens (   10.19 ms per token,    98.17 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.57 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1401 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     387.76 ms /    39 tokens (    9.94 ms per token,   100.58 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     389.03 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1402 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     396.31 ms /    39 tokens (   10.16 ms per token,    98.41 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     397.60 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1403 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     386.98 ms /    39 tokens (    9.92 ms per token,   100.78 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     388.28 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1404 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     730.81 ms /    39 tokens (   18.74 ms per token,    53.37 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     733.22 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1405 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     392.12 ms /    39 tokens (   10.05 ms per token,    99.46 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     393.88 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1406 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     395.35 ms /    39 tokens (   10.14 ms per token,    98.65 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     396.66 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1407 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     387.67 ms /    39 tokens (    9.94 ms per token,   100.60 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     390.14 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1408 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     393.25 ms /    39 tokens (   10.08 ms per token,    99.17 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     394.52 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1409 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.72 ms /    39 tokens (   10.20 ms per token,    98.06 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.13 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1410 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     393.97 ms /    39 tokens (   10.10 ms per token,    98.99 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     395.22 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1411 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     387.30 ms /    39 tokens (    9.93 ms per token,   100.70 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     388.75 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1412 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     388.15 ms /    39 tokens (    9.95 ms per token,   100.48 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     389.37 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1413 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     386.47 ms /    39 tokens (    9.91 ms per token,   100.91 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     388.07 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1414 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     588.43 ms /    39 tokens (   15.09 ms per token,    66.28 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     589.67 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1415 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     386.91 ms /    43 tokens (    9.00 ms per token,   111.14 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     388.21 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1416 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.81 ms /    43 tokens (    9.25 ms per token,   108.09 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.14 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1417 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     399.28 ms /    43 tokens (    9.29 ms per token,   107.69 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     400.60 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1418 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     399.97 ms /    43 tokens (    9.30 ms per token,   107.51 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     402.74 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1419 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     388.54 ms /    43 tokens (    9.04 ms per token,   110.67 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     389.93 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1420 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.60 ms /    43 tokens (    9.25 ms per token,   108.15 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     400.66 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1421 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     399.65 ms /    43 tokens (    9.29 ms per token,   107.59 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     400.95 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1422 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     401.29 ms /    44 tokens (    9.12 ms per token,   109.65 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     404.56 ms /    45 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1423 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.47 ms /    44 tokens (    9.03 ms per token,   110.70 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.79 ms /    45 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1424 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     388.77 ms /    44 tokens (    8.84 ms per token,   113.18 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     391.47 ms /    45 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1425 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.50 ms /    44 tokens (    9.03 ms per token,   110.69 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.87 ms /    45 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1426 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.85 ms /    44 tokens (    9.06 ms per token,   110.32 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     400.26 ms /    45 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1427 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.49 ms /    44 tokens (    9.03 ms per token,   110.70 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.85 ms /    45 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1428 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.76 ms /    44 tokens (    9.04 ms per token,   110.62 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.15 ms /    45 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1429 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     650.23 ms /    44 tokens (   14.78 ms per token,    67.67 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     651.56 ms /    45 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1430 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     396.96 ms /    39 tokens (   10.18 ms per token,    98.25 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.76 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1431 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     388.94 ms /    39 tokens (    9.97 ms per token,   100.27 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     390.27 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1432 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     385.21 ms /    39 tokens (    9.88 ms per token,   101.24 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     386.60 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1433 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     396.51 ms /    39 tokens (   10.17 ms per token,    98.36 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     397.84 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1434 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     387.83 ms /    39 tokens (    9.94 ms per token,   100.56 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     389.19 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1435 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.23 ms /    39 tokens (   10.19 ms per token,    98.18 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.50 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1436 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     388.79 ms /    39 tokens (    9.97 ms per token,   100.31 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     390.14 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1437 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     394.69 ms /    42 tokens (    9.40 ms per token,   106.41 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     396.16 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1438 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.44 ms /    42 tokens (    9.49 ms per token,   105.41 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.95 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1439 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     704.04 ms /    42 tokens (   16.76 ms per token,    59.66 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     705.57 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1440 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.03 ms /    42 tokens (    9.45 ms per token,   105.79 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.37 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1441 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.74 ms /    42 tokens (    9.47 ms per token,   105.60 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.38 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1442 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     399.63 ms /    42 tokens (    9.52 ms per token,   105.10 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     402.85 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1443 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.06 ms /    42 tokens (    9.45 ms per token,   105.78 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.37 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1444 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 38 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     392.77 ms /    38 tokens (   10.34 ms per token,    96.75 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     393.96 ms /    39 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1445 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 38 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     389.88 ms /    38 tokens (   10.26 ms per token,    97.47 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     391.10 ms /    39 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1446 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 38 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     386.13 ms /    38 tokens (   10.16 ms per token,    98.41 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     387.37 ms /    39 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1447 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 38 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     395.68 ms /    38 tokens (   10.41 ms per token,    96.04 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     396.86 ms /    39 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1448 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 38 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     389.43 ms /    38 tokens (   10.25 ms per token,    97.58 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     390.80 ms /    39 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1449 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 38 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     681.86 ms /    38 tokens (   17.94 ms per token,    55.73 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     683.10 ms /    39 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1450 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 38 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     396.18 ms /    38 tokens (   10.43 ms per token,    95.92 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     397.39 ms /    39 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1451 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 38 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     384.44 ms /    38 tokens (   10.12 ms per token,    98.85 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     393.42 ms /    39 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1452 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     389.92 ms /    39 tokens (   10.00 ms per token,   100.02 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     391.18 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1453 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     396.16 ms /    39 tokens (   10.16 ms per token,    98.45 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.58 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1454 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     394.02 ms /    39 tokens (   10.10 ms per token,    98.98 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     395.35 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1455 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.24 ms /    39 tokens (   10.19 ms per token,    98.18 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.52 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1456 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.06 ms /    39 tokens (   10.21 ms per token,    97.97 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.36 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1457 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     385.96 ms /    39 tokens (    9.90 ms per token,   101.05 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     387.22 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1458 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     394.62 ms /    39 tokens (   10.12 ms per token,    98.83 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     395.88 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1459 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     600.38 ms /    39 tokens (   15.39 ms per token,    64.96 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     601.77 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1460 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     396.80 ms /    39 tokens (   10.17 ms per token,    98.29 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.02 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1461 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.34 ms /    39 tokens (   10.19 ms per token,    98.15 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.94 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1462 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     390.21 ms /    39 tokens (   10.01 ms per token,    99.95 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     391.43 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1463 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     386.04 ms /    39 tokens (    9.90 ms per token,   101.03 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     387.33 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1464 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.73 ms /    39 tokens (   10.20 ms per token,    98.06 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.99 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1465 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     386.11 ms /    39 tokens (    9.90 ms per token,   101.01 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     387.35 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1466 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.11 ms /    39 tokens (   10.18 ms per token,    98.21 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.78 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1467 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     394.55 ms /    39 tokens (   10.12 ms per token,    98.85 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     397.63 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1468 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.25 ms /    44 tokens (    9.05 ms per token,   110.48 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.54 ms /    45 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1469 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.13 ms /    44 tokens (    9.03 ms per token,   110.79 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.51 ms /    45 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1470 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.58 ms /    44 tokens (    9.06 ms per token,   110.39 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.89 ms /    45 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1471 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.42 ms /    44 tokens (    9.03 ms per token,   110.72 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.76 ms /    45 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1472 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     550.16 ms /    44 tokens (   12.50 ms per token,    79.98 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     551.52 ms /    45 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1473 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     394.27 ms /    44 tokens (    8.96 ms per token,   111.60 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     395.57 ms /    45 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1474 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.55 ms /    44 tokens (    9.04 ms per token,   110.68 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.16 ms /    45 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1475 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     394.13 ms /    44 tokens (    8.96 ms per token,   111.64 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     397.49 ms /    45 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1476 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 49 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     400.26 ms /    49 tokens (    8.17 ms per token,   122.42 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     401.95 ms /    50 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1477 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 49 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     395.41 ms /    49 tokens (    8.07 ms per token,   123.92 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     396.96 ms /    50 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1478 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 49 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     395.23 ms /    49 tokens (    8.07 ms per token,   123.98 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     396.37 ms /    50 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1479 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 49 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     393.57 ms /    49 tokens (    8.03 ms per token,   124.50 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     395.03 ms /    50 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1480 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 49 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     396.11 ms /    49 tokens (    8.08 ms per token,   123.70 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     397.27 ms /    50 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1481 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 49 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     400.15 ms /    49 tokens (    8.17 ms per token,   122.45 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     403.66 ms /    50 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1482 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 49 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     394.46 ms /    49 tokens (    8.05 ms per token,   124.22 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     395.69 ms /    50 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1483 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 45 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     390.44 ms /    45 tokens (    8.68 ms per token,   115.26 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     393.22 ms /    46 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1484 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 45 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     391.83 ms /    45 tokens (    8.71 ms per token,   114.85 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     392.94 ms /    46 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1485 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 45 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     387.91 ms /    45 tokens (    8.62 ms per token,   116.01 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     389.26 ms /    46 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1486 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 45 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     394.79 ms /    45 tokens (    8.77 ms per token,   113.99 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     396.17 ms /    46 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1487 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 45 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.78 ms /    45 tokens (    8.86 ms per token,   112.84 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     402.17 ms /    46 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1488 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.21 ms /    39 tokens (   10.21 ms per token,    97.94 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     401.24 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1489 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.10 ms /    39 tokens (   10.21 ms per token,    97.97 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     401.07 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1490 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     388.00 ms /    39 tokens (    9.95 ms per token,   100.51 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     389.36 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1491 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.91 ms /    39 tokens (   10.23 ms per token,    97.77 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     402.02 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1492 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     385.79 ms /    39 tokens (    9.89 ms per token,   101.09 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     387.05 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1493 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.28 ms /    39 tokens (   10.21 ms per token,    97.92 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     401.31 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1494 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 39 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.42 ms /    39 tokens (   10.19 ms per token,    98.13 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.65 ms /    40 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1495 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     390.16 ms /    42 tokens (    9.29 ms per token,   107.65 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     393.38 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1496 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     389.67 ms /    42 tokens (    9.28 ms per token,   107.78 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     390.90 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1497 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     396.89 ms /    42 tokens (    9.45 ms per token,   105.82 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.22 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1498 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     399.44 ms /    42 tokens (    9.51 ms per token,   105.15 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     400.81 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1499 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     582.75 ms /    42 tokens (   13.88 ms per token,    72.07 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     584.23 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1500 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     399.12 ms /    42 tokens (    9.50 ms per token,   105.23 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     400.39 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1501 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     399.33 ms /    42 tokens (    9.51 ms per token,   105.18 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     401.04 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1502 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     396.81 ms /    42 tokens (    9.45 ms per token,   105.84 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.22 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1503 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 48 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     393.70 ms /    48 tokens (    8.20 ms per token,   121.92 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     396.56 ms /    49 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1504 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 48 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     391.94 ms /    48 tokens (    8.17 ms per token,   122.47 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     393.75 ms /    49 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1505 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 48 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     389.47 ms /    48 tokens (    8.11 ms per token,   123.25 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     391.02 ms /    49 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1506 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 48 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.19 ms /    48 tokens (    8.27 ms per token,   120.85 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.61 ms /    49 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1507 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 48 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.17 ms /    48 tokens (    8.27 ms per token,   120.85 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.90 ms /    49 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1508 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 48 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.63 ms /    48 tokens (    8.30 ms per token,   120.41 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     402.14 ms /    49 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1509 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 48 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     395.08 ms /    48 tokens (    8.23 ms per token,   121.49 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     396.71 ms /    49 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1510 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 48 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     400.73 ms /    48 tokens (    8.35 ms per token,   119.78 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     402.27 ms /    49 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1511 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     390.57 ms /    40 tokens (    9.76 ms per token,   102.41 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     392.07 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1512 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     386.64 ms /    40 tokens (    9.67 ms per token,   103.45 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     387.89 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1513 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     396.54 ms /    40 tokens (    9.91 ms per token,   100.87 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.26 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1514 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     390.09 ms /    40 tokens (    9.75 ms per token,   102.54 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     391.36 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1515 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.96 ms /    40 tokens (    9.95 ms per token,   100.51 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.25 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1516 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     395.14 ms /    40 tokens (    9.88 ms per token,   101.23 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.18 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1517 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     386.65 ms /    40 tokens (    9.67 ms per token,   103.45 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     387.83 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1518 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     390.55 ms /    46 tokens (    8.49 ms per token,   117.78 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     391.94 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1519 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     400.30 ms /    46 tokens (    8.70 ms per token,   114.92 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     403.51 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1520 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     546.28 ms /    46 tokens (   11.88 ms per token,    84.21 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     547.60 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1521 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.66 ms /    46 tokens (    8.64 ms per token,   115.68 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.12 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1522 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.01 ms /    46 tokens (    8.65 ms per token,   115.57 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.45 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1523 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 49 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.88 ms /    49 tokens (    8.14 ms per token,   122.85 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     400.30 ms /    50 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1524 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 49 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.10 ms /    49 tokens (    8.12 ms per token,   123.08 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     401.43 ms /    50 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1525 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 49 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     395.56 ms /    49 tokens (    8.07 ms per token,   123.87 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.17 ms /    50 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1526 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 49 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     392.20 ms /    49 tokens (    8.00 ms per token,   124.93 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     393.57 ms /    50 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1527 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 49 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     396.40 ms /    49 tokens (    8.09 ms per token,   123.61 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     397.89 ms /    50 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1528 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 49 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     396.99 ms /    49 tokens (    8.10 ms per token,   123.43 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.46 ms /    50 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1529 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 49 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     391.75 ms /    49 tokens (    7.99 ms per token,   125.08 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     393.23 ms /    50 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1530 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 49 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     396.53 ms /    49 tokens (    8.09 ms per token,   123.57 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     397.99 ms /    50 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1531 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     396.26 ms /    46 tokens (    8.61 ms per token,   116.08 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.60 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1532 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     544.58 ms /    46 tokens (   11.84 ms per token,    84.47 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     545.90 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1533 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     396.84 ms /    46 tokens (    8.63 ms per token,   115.92 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.31 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1534 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.36 ms /    46 tokens (    8.66 ms per token,   115.47 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.79 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1535 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     388.55 ms /    46 tokens (    8.45 ms per token,   118.39 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     389.97 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1536 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     395.71 ms /    46 tokens (    8.60 ms per token,   116.25 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     397.16 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1537 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     392.84 ms /    46 tokens (    8.54 ms per token,   117.09 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     396.20 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1538 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     394.98 ms /    46 tokens (    8.59 ms per token,   116.46 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     397.68 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1539 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     391.92 ms /    40 tokens (    9.80 ms per token,   102.06 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     394.98 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1540 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     395.11 ms /    40 tokens (    9.88 ms per token,   101.24 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     396.50 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1541 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     391.54 ms /    40 tokens (    9.79 ms per token,   102.16 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     392.81 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1542 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.82 ms /    40 tokens (    9.95 ms per token,   100.55 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.45 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1543 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     394.79 ms /    40 tokens (    9.87 ms per token,   101.32 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     397.93 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1544 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.17 ms /    40 tokens (    9.93 ms per token,   100.71 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.70 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1545 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     390.33 ms /    40 tokens (    9.76 ms per token,   102.48 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     391.61 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1546 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.53 ms /    40 tokens (    9.94 ms per token,   100.62 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.92 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1547 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     395.23 ms /    41 tokens (    9.64 ms per token,   103.74 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     396.56 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1548 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     389.68 ms /    41 tokens (    9.50 ms per token,   105.22 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     391.38 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1549 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     540.56 ms /    41 tokens (   13.18 ms per token,    75.85 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     541.89 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1550 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     396.00 ms /    41 tokens (    9.66 ms per token,   103.54 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     397.30 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1551 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     399.14 ms /    41 tokens (    9.74 ms per token,   102.72 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     402.32 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1552 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     396.55 ms /    41 tokens (    9.67 ms per token,   103.39 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     397.85 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1553 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     386.80 ms /    41 tokens (    9.43 ms per token,   106.00 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     388.09 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1554 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     395.10 ms /    42 tokens (    9.41 ms per token,   106.30 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     396.40 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1555 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.67 ms /    42 tokens (    9.49 ms per token,   105.35 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.93 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1556 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.40 ms /    42 tokens (    9.49 ms per token,   105.42 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.72 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1557 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     551.89 ms /    42 tokens (   13.14 ms per token,    76.10 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     553.22 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1558 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.11 ms /    42 tokens (    9.46 ms per token,   105.76 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.34 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1559 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     399.16 ms /    42 tokens (    9.50 ms per token,   105.22 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     402.36 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1560 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     396.01 ms /    42 tokens (    9.43 ms per token,   106.06 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     397.29 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1561 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     399.27 ms /    42 tokens (    9.51 ms per token,   105.19 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     400.68 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1562 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.87 ms /    41 tokens (    9.70 ms per token,   103.05 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.19 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1563 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.95 ms /    41 tokens (    9.73 ms per token,   102.77 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     400.25 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1564 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     387.39 ms /    41 tokens (    9.45 ms per token,   105.84 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     388.70 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1565 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.06 ms /    41 tokens (    9.71 ms per token,   103.00 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     400.64 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1566 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 52 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     708.64 ms /    52 tokens (   13.63 ms per token,    73.38 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     710.34 ms /    53 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1567 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 52 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     395.50 ms /    52 tokens (    7.61 ms per token,   131.48 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     397.03 ms /    53 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1568 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 52 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.87 ms /    52 tokens (    7.65 ms per token,   130.69 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     401.36 ms /    53 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1569 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 52 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     391.58 ms /    52 tokens (    7.53 ms per token,   132.79 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     392.96 ms /    53 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1570 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 52 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     394.25 ms /    52 tokens (    7.58 ms per token,   131.90 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     395.78 ms /    53 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1571 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 52 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     394.12 ms /    52 tokens (    7.58 ms per token,   131.94 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     395.64 ms /    53 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1572 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 52 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     395.47 ms /    52 tokens (    7.61 ms per token,   131.49 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.83 ms /    53 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1573 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 52 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.03 ms /    52 tokens (    7.64 ms per token,   130.97 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     400.68 ms /    53 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1574 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 52 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     393.43 ms /    52 tokens (    7.57 ms per token,   132.17 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     395.05 ms /    53 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1575 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     392.73 ms /    46 tokens (    8.54 ms per token,   117.13 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     395.14 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1576 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.23 ms /    46 tokens (    8.66 ms per token,   115.51 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.60 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1577 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.88 ms /    46 tokens (    8.67 ms per token,   115.32 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     400.32 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1578 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.60 ms /    46 tokens (    8.67 ms per token,   115.40 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.99 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1579 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.44 ms /    46 tokens (    8.64 ms per token,   115.74 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.85 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1580 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.89 ms /    46 tokens (    8.67 ms per token,   115.32 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     402.25 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1581 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.11 ms /    46 tokens (    8.63 ms per token,   115.84 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.52 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1582 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.09 ms /    46 tokens (    8.65 ms per token,   115.55 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.48 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1583 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.25 ms /    46 tokens (    8.66 ms per token,   115.50 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.61 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1584 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.82 ms /    46 tokens (    8.65 ms per token,   115.63 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     400.55 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1585 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     390.37 ms /    46 tokens (    8.49 ms per token,   117.84 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     391.81 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1586 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     391.88 ms /    46 tokens (    8.52 ms per token,   117.38 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     393.27 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1587 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     396.85 ms /    46 tokens (    8.63 ms per token,   115.91 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.23 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1588 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     547.81 ms /    46 tokens (   11.91 ms per token,    83.97 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     549.19 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1589 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     396.33 ms /    46 tokens (    8.62 ms per token,   116.06 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     397.69 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1590 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.24 ms /    46 tokens (    8.64 ms per token,   115.80 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.84 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1591 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.14 ms /    46 tokens (    8.66 ms per token,   115.54 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.55 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1592 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.72 ms /    46 tokens (    8.65 ms per token,   115.66 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.52 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1593 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     392.66 ms /    46 tokens (    8.54 ms per token,   117.15 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     394.05 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1594 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     396.90 ms /    46 tokens (    8.63 ms per token,   115.90 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     400.02 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1595 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     392.81 ms /    46 tokens (    8.54 ms per token,   117.10 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     394.33 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1596 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     396.89 ms /    46 tokens (    8.63 ms per token,   115.90 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.43 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1597 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.70 ms /    46 tokens (    8.67 ms per token,   115.38 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     400.07 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1598 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.87 ms /    46 tokens (    8.65 ms per token,   115.62 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.66 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1599 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     394.99 ms /    46 tokens (    8.59 ms per token,   116.46 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     396.39 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1600 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.36 ms /    46 tokens (    8.64 ms per token,   115.76 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.87 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1601 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     393.73 ms /    46 tokens (    8.56 ms per token,   116.83 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     395.12 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1602 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.78 ms /    46 tokens (    8.65 ms per token,   115.64 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.53 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1603 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.01 ms /    46 tokens (    8.63 ms per token,   115.87 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.47 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1604 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.65 ms /    46 tokens (    8.67 ms per token,   115.39 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     400.24 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1605 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     394.57 ms /    46 tokens (    8.58 ms per token,   116.58 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     395.96 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1606 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     550.38 ms /    46 tokens (   11.96 ms per token,    83.58 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     551.72 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1607 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     396.02 ms /    46 tokens (    8.61 ms per token,   116.15 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     397.51 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1608 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.11 ms /    46 tokens (    8.63 ms per token,   115.84 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.53 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1609 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.16 ms /    46 tokens (    8.63 ms per token,   115.82 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.54 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1610 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.97 ms /    46 tokens (    8.67 ms per token,   115.30 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     402.36 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1611 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.06 ms /    46 tokens (    8.65 ms per token,   115.56 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.43 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1612 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.77 ms /    46 tokens (    8.67 ms per token,   115.35 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     400.23 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1613 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     394.79 ms /    46 tokens (    8.58 ms per token,   116.52 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     396.16 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1614 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     396.60 ms /    46 tokens (    8.62 ms per token,   115.98 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     397.99 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1615 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.03 ms /    46 tokens (    8.65 ms per token,   115.57 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.44 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1616 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.39 ms /    46 tokens (    8.64 ms per token,   115.76 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.76 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1617 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     396.56 ms /    46 tokens (    8.62 ms per token,   116.00 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.29 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1618 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     544.03 ms /    46 tokens (   11.83 ms per token,    84.55 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     545.41 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1619 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.75 ms /    46 tokens (    8.65 ms per token,   115.65 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.17 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1620 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     394.76 ms /    46 tokens (    8.58 ms per token,   116.53 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     396.20 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1621 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     396.99 ms /    46 tokens (    8.63 ms per token,   115.87 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.60 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1622 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     400.39 ms /    46 tokens (    8.70 ms per token,   114.89 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     403.85 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1623 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     399.74 ms /    46 tokens (    8.69 ms per token,   115.07 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     401.28 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1624 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     400.29 ms /    46 tokens (    8.70 ms per token,   114.92 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     401.66 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1625 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     400.55 ms /    46 tokens (    8.71 ms per token,   114.84 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     402.70 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1626 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     394.33 ms /    46 tokens (    8.57 ms per token,   116.65 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     395.72 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1627 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.29 ms /    46 tokens (    8.66 ms per token,   115.49 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     400.07 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1628 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     394.19 ms /    46 tokens (    8.57 ms per token,   116.70 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     395.65 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1629 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.97 ms /    46 tokens (    8.65 ms per token,   115.59 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.79 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1630 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     393.96 ms /    46 tokens (    8.56 ms per token,   116.76 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     397.30 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1631 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     395.22 ms /    46 tokens (    8.59 ms per token,   116.39 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     396.95 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1632 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.47 ms /    46 tokens (    8.66 ms per token,   115.44 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.88 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1633 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     396.12 ms /    46 tokens (    8.61 ms per token,   116.13 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     397.68 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1634 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     392.61 ms /    46 tokens (    8.53 ms per token,   117.17 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     396.06 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1635 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     399.26 ms /    46 tokens (    8.68 ms per token,   115.21 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     400.59 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1636 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.72 ms /    46 tokens (    8.65 ms per token,   115.66 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.12 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1637 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     396.41 ms /    46 tokens (    8.62 ms per token,   116.04 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     397.83 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1638 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     392.33 ms /    46 tokens (    8.53 ms per token,   117.25 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     393.73 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1639 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     396.69 ms /    46 tokens (    8.62 ms per token,   115.96 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     397.98 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1640 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.12 ms /    46 tokens (    8.65 ms per token,   115.54 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.54 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1641 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     395.51 ms /    46 tokens (    8.60 ms per token,   116.30 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     396.56 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1642 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     393.73 ms /    46 tokens (    8.56 ms per token,   116.83 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     397.11 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1643 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     396.17 ms /    46 tokens (    8.61 ms per token,   116.11 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     397.61 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1644 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     395.93 ms /    46 tokens (    8.61 ms per token,   116.18 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.34 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1645 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     395.14 ms /    46 tokens (    8.59 ms per token,   116.41 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     396.56 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1646 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     393.93 ms /    46 tokens (    8.56 ms per token,   116.77 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     395.34 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1647 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.94 ms /    46 tokens (    8.65 ms per token,   115.60 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.35 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1648 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.39 ms /    46 tokens (    8.66 ms per token,   115.47 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     401.37 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1649 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     401.59 ms /    46 tokens (    8.73 ms per token,   114.54 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     405.08 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1650 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     399.57 ms /    46 tokens (    8.69 ms per token,   115.12 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     401.00 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1651 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.37 ms /    46 tokens (    8.64 ms per token,   115.76 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.82 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1652 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.73 ms /    46 tokens (    8.67 ms per token,   115.37 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     402.14 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1653 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     395.50 ms /    46 tokens (    8.60 ms per token,   116.31 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     396.82 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1654 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.92 ms /    46 tokens (    8.65 ms per token,   115.60 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.33 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1655 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.29 ms /    46 tokens (    8.66 ms per token,   115.49 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.74 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1656 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     399.14 ms /    46 tokens (    8.68 ms per token,   115.25 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     400.86 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1657 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     394.03 ms /    46 tokens (    8.57 ms per token,   116.74 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     395.43 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1658 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     396.65 ms /    46 tokens (    8.62 ms per token,   115.97 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.10 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1659 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     556.57 ms /    46 tokens (   12.10 ms per token,    82.65 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     557.89 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1660 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.72 ms /    46 tokens (    8.65 ms per token,   115.66 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.42 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1661 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     395.94 ms /    46 tokens (    8.61 ms per token,   116.18 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     397.34 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1662 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.35 ms /    46 tokens (    8.66 ms per token,   115.48 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     400.06 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1663 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     399.92 ms /    46 tokens (    8.69 ms per token,   115.02 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     401.33 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1664 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     400.34 ms /    46 tokens (    8.70 ms per token,   114.90 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     403.02 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1665 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     392.34 ms /    46 tokens (    8.53 ms per token,   117.25 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     393.71 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1666 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.28 ms /    46 tokens (    8.66 ms per token,   115.50 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     400.98 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1667 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     393.51 ms /    46 tokens (    8.55 ms per token,   116.90 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     394.92 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1668 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     395.92 ms /    46 tokens (    8.61 ms per token,   116.18 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     397.76 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1669 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     392.06 ms /    46 tokens (    8.52 ms per token,   117.33 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     393.38 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1670 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     396.94 ms /    46 tokens (    8.63 ms per token,   115.89 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.86 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1671 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     392.23 ms /    46 tokens (    8.53 ms per token,   117.28 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     393.72 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1672 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     396.65 ms /    46 tokens (    8.62 ms per token,   115.97 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.23 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1673 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     392.33 ms /    46 tokens (    8.53 ms per token,   117.25 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     393.73 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1674 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     396.15 ms /    46 tokens (    8.61 ms per token,   116.12 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.40 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1675 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     392.26 ms /    46 tokens (    8.53 ms per token,   117.27 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     393.65 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1676 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     394.89 ms /    46 tokens (    8.58 ms per token,   116.49 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     396.42 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1677 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     392.11 ms /    46 tokens (    8.52 ms per token,   117.31 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     393.50 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1678 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.49 ms /    46 tokens (    8.64 ms per token,   115.73 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.28 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1679 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     704.83 ms /    46 tokens (   15.32 ms per token,    65.26 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     706.25 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1680 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     399.53 ms /    46 tokens (    8.69 ms per token,   115.14 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     403.07 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1681 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     395.38 ms /    46 tokens (    8.60 ms per token,   116.34 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     396.96 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1682 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.89 ms /    46 tokens (    8.65 ms per token,   115.61 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.28 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1683 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     399.56 ms /    46 tokens (    8.69 ms per token,   115.13 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     401.00 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1684 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     393.21 ms /    46 tokens (    8.55 ms per token,   116.99 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     394.64 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1685 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     396.33 ms /    46 tokens (    8.62 ms per token,   116.07 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     397.96 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1686 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.24 ms /    46 tokens (    8.66 ms per token,   115.51 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.62 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1687 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.04 ms /    46 tokens (    8.63 ms per token,   115.86 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.78 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1688 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     390.42 ms /    46 tokens (    8.49 ms per token,   117.82 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     393.77 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1689 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     396.22 ms /    46 tokens (    8.61 ms per token,   116.10 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     397.99 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1690 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     393.58 ms /    46 tokens (    8.56 ms per token,   116.87 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     395.01 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1691 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.23 ms /    46 tokens (    8.64 ms per token,   115.80 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.96 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1692 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.08 ms /    46 tokens (    8.65 ms per token,   115.55 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.42 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1693 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.62 ms /    46 tokens (    8.67 ms per token,   115.40 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     400.64 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1694 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     392.91 ms /    46 tokens (    8.54 ms per token,   117.08 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     394.33 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1695 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.53 ms /    46 tokens (    8.66 ms per token,   115.43 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     401.24 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1696 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     393.13 ms /    46 tokens (    8.55 ms per token,   117.01 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     394.49 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1697 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     395.98 ms /    43 tokens (    9.21 ms per token,   108.59 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     397.51 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1698 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.65 ms /    43 tokens (    9.27 ms per token,   107.87 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     400.00 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1699 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     400.03 ms /    43 tokens (    9.30 ms per token,   107.49 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     401.70 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1700 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.93 ms /    43 tokens (    9.25 ms per token,   108.06 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.33 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1701 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     390.69 ms /    43 tokens (    9.09 ms per token,   110.06 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     392.25 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1702 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.65 ms /    43 tokens (    9.25 ms per token,   108.13 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.10 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1703 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     399.00 ms /    43 tokens (    9.28 ms per token,   107.77 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     400.45 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1704 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     389.59 ms /    41 tokens (    9.50 ms per token,   105.24 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     390.91 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1705 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     545.17 ms /    41 tokens (   13.30 ms per token,    75.21 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     546.42 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1706 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     388.50 ms /    41 tokens (    9.48 ms per token,   105.53 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     391.16 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1707 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.56 ms /    41 tokens (    9.72 ms per token,   102.87 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.87 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1708 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     387.20 ms /    41 tokens (    9.44 ms per token,   105.89 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     388.51 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1709 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.19 ms /    41 tokens (    9.69 ms per token,   103.23 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.47 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1710 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     387.49 ms /    41 tokens (    9.45 ms per token,   105.81 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     388.85 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1711 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.41 ms /    41 tokens (    9.72 ms per token,   102.91 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     401.58 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1712 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     522.19 ms /    43 tokens (   12.14 ms per token,    82.35 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     523.69 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1713 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     394.92 ms /    43 tokens (    9.18 ms per token,   108.88 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     396.35 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1714 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.97 ms /    43 tokens (    9.28 ms per token,   107.78 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     400.47 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1715 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     389.76 ms /    43 tokens (    9.06 ms per token,   110.32 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     391.16 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1716 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 43 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.39 ms /    43 tokens (    9.24 ms per token,   108.21 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.70 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1717 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     395.07 ms /    40 tokens (    9.88 ms per token,   101.25 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     396.29 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1718 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     392.53 ms /    40 tokens (    9.81 ms per token,   101.90 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     393.78 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1719 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     387.42 ms /    40 tokens (    9.69 ms per token,   103.25 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     389.99 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1720 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     636.52 ms /    40 tokens (   15.91 ms per token,    62.84 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     637.84 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1721 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     396.10 ms /    40 tokens (    9.90 ms per token,   100.98 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     397.38 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1722 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     387.04 ms /    40 tokens (    9.68 ms per token,   103.35 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     388.04 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1723 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     389.05 ms /    40 tokens (    9.73 ms per token,   102.81 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     390.42 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1724 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     392.76 ms /    40 tokens (    9.82 ms per token,   101.84 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     393.92 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1725 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     556.81 ms /    44 tokens (   12.65 ms per token,    79.02 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     558.30 ms /    45 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1726 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     396.47 ms /    44 tokens (    9.01 ms per token,   110.98 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     397.85 ms /    45 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1727 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.07 ms /    44 tokens (    9.05 ms per token,   110.53 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.39 ms /    45 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1728 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.69 ms /    44 tokens (    9.06 ms per token,   110.36 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     400.05 ms /    45 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1729 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.04 ms /    44 tokens (    9.05 ms per token,   110.54 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     401.34 ms /    45 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1730 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     394.50 ms /    44 tokens (    8.97 ms per token,   111.54 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     395.98 ms /    45 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1731 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 44 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.39 ms /    44 tokens (    9.05 ms per token,   110.44 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.86 ms /    45 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1732 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     395.95 ms /    41 tokens (    9.66 ms per token,   103.55 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     397.34 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1733 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     389.61 ms /    41 tokens (    9.50 ms per token,   105.23 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     392.15 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1734 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     517.40 ms /    41 tokens (   12.62 ms per token,    79.24 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     518.76 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1735 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.94 ms /    41 tokens (    9.71 ms per token,   103.03 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.18 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1736 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.46 ms /    41 tokens (    9.72 ms per token,   102.90 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.79 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1737 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.83 ms /    41 tokens (    9.73 ms per token,   102.80 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     400.23 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1738 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     393.46 ms /    41 tokens (    9.60 ms per token,   104.20 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     394.74 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1739 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 41 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     399.49 ms /    41 tokens (    9.74 ms per token,   102.63 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     400.83 ms /    42 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1740 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     399.11 ms /    42 tokens (    9.50 ms per token,   105.24 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     400.42 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1741 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     399.63 ms /    42 tokens (    9.52 ms per token,   105.10 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     401.01 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1742 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     393.25 ms /    42 tokens (    9.36 ms per token,   106.80 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     394.57 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1743 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     635.27 ms /    42 tokens (   15.13 ms per token,    66.11 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     636.57 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1744 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.30 ms /    42 tokens (    9.48 ms per token,   105.45 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.63 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1745 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     387.53 ms /    42 tokens (    9.23 ms per token,   108.38 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     388.64 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1746 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     387.38 ms /    42 tokens (    9.22 ms per token,   108.42 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     388.65 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1747 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     385.53 ms /    42 tokens (    9.18 ms per token,   108.94 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     386.83 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1748 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     393.79 ms /    42 tokens (    9.38 ms per token,   106.66 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     395.16 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1749 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     396.03 ms /    42 tokens (    9.43 ms per token,   106.05 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     397.16 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1750 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     599.92 ms /    42 tokens (   14.28 ms per token,    70.01 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     601.51 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1751 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.45 ms /    42 tokens (    9.49 ms per token,   105.41 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     401.66 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1752 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     396.38 ms /    42 tokens (    9.44 ms per token,   105.96 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     397.67 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1753 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.07 ms /    42 tokens (    9.48 ms per token,   105.51 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.53 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1754 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.07 ms /    42 tokens (    9.48 ms per token,   105.51 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.45 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1755 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 42 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     387.07 ms /    42 tokens (    9.22 ms per token,   108.51 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     388.39 ms /    43 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1756 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 49 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     401.03 ms /    49 tokens (    8.18 ms per token,   122.19 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     404.55 ms /    50 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1757 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 49 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     401.80 ms /    49 tokens (    8.20 ms per token,   121.95 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     403.29 ms /    50 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1758 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 49 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.36 ms /    49 tokens (    8.11 ms per token,   123.31 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.96 ms /    50 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1759 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 49 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     394.60 ms /    49 tokens (    8.05 ms per token,   124.18 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     396.15 ms /    50 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1760 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 49 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     395.66 ms /    49 tokens (    8.07 ms per token,   123.84 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     397.05 ms /    50 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1761 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 49 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.40 ms /    49 tokens (    8.11 ms per token,   123.30 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.77 ms /    50 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1762 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 49 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.05 ms /    49 tokens (    8.12 ms per token,   123.10 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.41 ms /    50 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1763 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 49 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.69 ms /    49 tokens (    8.12 ms per token,   123.21 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.14 ms /    50 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1764 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 49 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     665.68 ms /    49 tokens (   13.59 ms per token,    73.61 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     667.08 ms /    50 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1765 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 49 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     398.18 ms /    49 tokens (    8.13 ms per token,   123.06 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.67 ms /    50 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1766 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 49 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     396.38 ms /    49 tokens (    8.09 ms per token,   123.62 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     397.90 ms /    50 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1767 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 49 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     396.17 ms /    49 tokens (    8.09 ms per token,   123.69 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     397.67 ms /    50 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1768 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 49 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.09 ms /    49 tokens (    8.10 ms per token,   123.40 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.57 ms /    50 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1769 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.68 ms /    46 tokens (    8.65 ms per token,   115.67 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     401.04 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1770 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     392.01 ms /    46 tokens (    8.52 ms per token,   117.34 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     393.48 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1771 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     399.77 ms /    46 tokens (    8.69 ms per token,   115.07 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     401.32 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1772 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.67 ms /    46 tokens (    8.65 ms per token,   115.67 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     399.03 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1773 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     399.25 ms /    46 tokens (    8.68 ms per token,   115.22 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     400.59 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1774 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     516.94 ms /    46 tokens (   11.24 ms per token,    88.98 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     518.38 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1775 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     396.35 ms /    46 tokens (    8.62 ms per token,   116.06 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     397.85 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1776 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     396.88 ms /    46 tokens (    8.63 ms per token,   115.90 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.29 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1777 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     396.45 ms /    46 tokens (    8.62 ms per token,   116.03 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     397.88 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1778 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 48 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     397.11 ms /    48 tokens (    8.27 ms per token,   120.87 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.52 ms /    49 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1779 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 48 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     396.59 ms /    48 tokens (    8.26 ms per token,   121.03 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     398.04 ms /    49 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1780 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 48 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     390.51 ms /    48 tokens (    8.14 ms per token,   122.92 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     391.94 ms /    49 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1781 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 48 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     395.54 ms /    48 tokens (    8.24 ms per token,   121.35 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     396.94 ms /    49 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1782 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 48 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     396.19 ms /    48 tokens (    8.25 ms per token,   121.16 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     397.58 ms /    49 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1783 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 48 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     531.88 ms /    48 tokens (   11.08 ms per token,    90.25 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     533.38 ms /    49 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1784 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 48 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     395.38 ms /    48 tokens (    8.24 ms per token,   121.40 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     396.92 ms /    49 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1785 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 48 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     391.95 ms /    48 tokens (    8.17 ms per token,   122.46 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     393.39 ms /    49 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1786 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 48 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     401.13 ms /    48 tokens (    8.36 ms per token,   119.66 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     404.56 ms /    49 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1787 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     393.14 ms /    46 tokens (    8.55 ms per token,   117.01 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     394.84 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1788 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     399.10 ms /    46 tokens (    8.68 ms per token,   115.26 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     400.71 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1789 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     399.01 ms /    46 tokens (    8.67 ms per token,   115.29 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     400.62 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1790 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     399.46 ms /    46 tokens (    8.68 ms per token,   115.16 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     402.80 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1791 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     393.07 ms /    46 tokens (    8.54 ms per token,   117.03 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     394.25 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1792 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     391.31 ms /    46 tokens (    8.51 ms per token,   117.55 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     392.76 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1793 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     395.68 ms /    46 tokens (    8.60 ms per token,   116.26 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     396.75 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1794 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     392.56 ms /    46 tokens (    8.53 ms per token,   117.18 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     394.04 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1795 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 46 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     395.95 ms /    46 tokens (    8.61 ms per token,   116.18 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     397.14 ms /    47 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1796 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     390.59 ms /    40 tokens (    9.76 ms per token,   102.41 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     391.88 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1797 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     387.63 ms /    40 tokens (    9.69 ms per token,   103.19 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     389.22 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1798 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     389.35 ms /    40 tokens (    9.73 ms per token,   102.74 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     390.63 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1799 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     384.22 ms /    40 tokens (    9.61 ms per token,   104.11 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     385.28 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1800 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     575.37 ms /    40 tokens (   14.38 ms per token,    69.52 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     576.68 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1801 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 48 prefix-match hit, remaining 40 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     386.55 ms /    40 tokens (    9.66 ms per token,   103.48 tokens per second)\n",
      "llama_perf_context_print:        eval time =       0.00 ms /     1 runs   (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:       total time =     387.84 ms /    41 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1802 done\n"
     ]
    }
   ],
   "source": [
    "df['yes'] = None\n",
    "df['no'] = None\n",
    "df['inference_time'] = None\n",
    "for i in df.index:\n",
    "    prompt = generate_fact_prompt_2(df.loc[i,\"Date\"],df.loc[i,'question'])\n",
    "    st = time.time()\n",
    "    resp = llm.create_chat_completion(\n",
    "    messages=prompt,\n",
    "    max_tokens=1,\n",
    "    temperature=0.0,\n",
    "    logprobs=True,\n",
    "    top_logprobs=5\n",
    "    )\n",
    "    \n",
    "\n",
    "    # response = tokenizer.decode(outputs[:, inputs[\"input_ids\"].shape[1]:][0], skip_special_tokens=True)\n",
    "    df.loc[i,'inference_time'] = time.time() - st\n",
    "\n",
    "    p_yes,p_no = get_yes_no(resp)\n",
    "\n",
    "\n",
    "    # print(f\"\\n\\n{prompt[0]['content']}\\n{prompt[1]['content']}\")\n",
    "    # print( {w: float(probs[tokenizer.encode(w, add_special_tokens=False)[0]])\n",
    "    #     for w in [\"Yes\", \"No\"]})\n",
    "    # df.loc[(question, date_str),\"event_date\"] = event_date\n",
    "    df.loc[i,\"yes\"] = p_yes\n",
    "    df.loc[i,\"no\"] = p_no\n",
    "    time.sleep(1)\n",
    "\n",
    "    print(f\"Question {i} done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('LLama_1Token_quantized_3.1-8B.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(len(df)+1):\n",
    "#     prompt = generate_fact_prompt_2(df.loc[i,\"Date\"],df.loc[i,'question'])\n",
    "#     st = time.time()\n",
    "#     resp = llm.create_chat_completion(\n",
    "#     messages=prompt,\n",
    "#     max_tokens=1,\n",
    "#     temperature=0.0,\n",
    "#     logprobs=True,\n",
    "#     top_logprobs=4\n",
    "#     )\n",
    "    \n",
    "\n",
    "#     # response = tokenizer.decode(outputs[:, inputs[\"input_ids\"].shape[1]:][0], skip_special_tokens=True)\n",
    "#     df.loc[i,'inference_time'] = time.time() - st\n",
    "\n",
    "#     p_yes,p_no = get_yes_no(resp)\n",
    "\n",
    "\n",
    "#     # print(f\"\\n\\n{prompt[0]['content']}\\n{prompt[1]['content']}\")\n",
    "#     # print( {w: float(probs[tokenizer.encode(w, add_special_tokens=False)[0]])\n",
    "#     #     for w in [\"Yes\", \"No\"]})\n",
    "#     # df.loc[(question, date_str),\"event_date\"] = event_date\n",
    "#     df.loc[i,\"yes\"] = p_yes\n",
    "#     df.loc[i,\"no\"] = p_no\n",
    "#     time.sleep(1.5)\n",
    "#     print(f\"Question {i} done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv(\"All_1token_values.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>question</th>\n",
       "      <th>Date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>$ARB above $1.25 on April 7?</td>\n",
       "      <td>March 31, 2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>$ARB above $1.25 on April 7?</td>\n",
       "      <td>April 30, 2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>$ARB above $1.25 on April 7?</td>\n",
       "      <td>April 30, 2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Another African coup by October 31?</td>\n",
       "      <td>August 31, 2023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Another African coup by October 31?</td>\n",
       "      <td>September 30, 2023</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                             question                Date\n",
       "0           0         $ARB above $1.25 on April 7?      March 31, 2023\n",
       "1           1         $ARB above $1.25 on April 7?      April 30, 2023\n",
       "2           2         $ARB above $1.25 on April 7?      April 30, 2023\n",
       "3           3  Another African coup by October 31?     August 31, 2023\n",
       "4           4  Another African coup by October 31?  September 30, 2023"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_long = pd.read_csv(\"precut_clean_llama.csv\")\n",
    "df_long.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_fact_prompt_4(date,question):\n",
    "\n",
    "    message1 = [\n",
    "    {\n",
    "        \"role\": \"system\",\n",
    "        \"content\": (\n",
    "            f\"You have to give a yes or no answer to each question based on your best knowledge and reasoning including and up to {date}. \"\n",
    "            \"Give specific factors you consider and you must end with yes/no and a period. I want no language of uncertainty.\"\n",
    "        )\n",
    "    },\n",
    "    {\"role\": \"user\", \"content\": f\"{question}\"}\n",
    "    ]\n",
    "    return message1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_fact_prompt_5(date,question):\n",
    " message = [\n",
    "  {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": f\"You are a prediction assistant. • Use only information, data, and events dated on or before {date}. • You begin every reply with factors considered and must conclude a one-word prediction—either “yes.” or “no.” • Do not add anything else.\"\n",
    "  },\n",
    "  {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": f\"Today is {date}.\"\n",
    "  },\n",
    "  {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": f\"{question}\"\n",
    "  }\n",
    "]\n",
    " return message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_yes_no_ans_500(resp):\n",
    "    token_df = resp['choices'][0]['logprobs']['content'][-2]\n",
    "    ans = token_df['token'].strip()\n",
    "    pyes= 0\n",
    "    pno = 0\n",
    "    for info in (token_df['top_logprobs']):\n",
    "        if info['token'].strip().lower() == 'yes':\n",
    "            pyes += math.exp(info['logprob'])\n",
    "        if info['token'].strip().lower() =='no':\n",
    "            pno += math.exp(info['logprob'])\n",
    "    return pyes,pno,ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 26 prefix-match hit, remaining 94 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     863.76 ms /    94 tokens (    9.19 ms per token,   108.83 tokens per second)\n",
      "llama_perf_context_print:        eval time =    5405.83 ms /    59 runs   (   91.62 ms per token,    10.91 tokens per second)\n",
      "llama_perf_context_print:       total time =    6291.11 ms /   153 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 0 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 44 prefix-match hit, remaining 76 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     729.05 ms /    76 tokens (    9.59 ms per token,   104.25 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4370.34 ms /    48 runs   (   91.05 ms per token,    10.98 tokens per second)\n",
      "llama_perf_context_print:       total time =    5114.15 ms /   124 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 1 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 119 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    4587.01 ms /    49 runs   (   93.61 ms per token,    10.68 tokens per second)\n",
      "llama_perf_context_print:       total time =    4595.14 ms /    50 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 2 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 44 prefix-match hit, remaining 72 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     736.83 ms /    72 tokens (   10.23 ms per token,    97.72 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4424.99 ms /    49 runs   (   90.31 ms per token,    11.07 tokens per second)\n",
      "llama_perf_context_print:       total time =    5171.57 ms /   121 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 3 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 44 prefix-match hit, remaining 72 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     730.67 ms /    72 tokens (   10.15 ms per token,    98.54 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4436.38 ms /    49 runs   (   90.54 ms per token,    11.05 tokens per second)\n",
      "llama_perf_context_print:       total time =    5177.27 ms /   121 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 4 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 44 prefix-match hit, remaining 72 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     761.37 ms /    72 tokens (   10.57 ms per token,    94.57 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4431.97 ms /    49 runs   (   90.45 ms per token,    11.06 tokens per second)\n",
      "llama_perf_context_print:       total time =    5202.43 ms /   121 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 5 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 115 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    4758.27 ms /    50 runs   (   95.17 ms per token,    10.51 tokens per second)\n",
      "llama_perf_context_print:       total time =    4766.30 ms /    51 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 6 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 44 prefix-match hit, remaining 76 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     852.39 ms /    76 tokens (   11.22 ms per token,    89.16 tokens per second)\n",
      "llama_perf_context_print:        eval time =    6682.99 ms /    74 runs   (   90.31 ms per token,    11.07 tokens per second)\n",
      "llama_perf_context_print:       total time =    7548.67 ms /   150 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 7 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 44 prefix-match hit, remaining 76 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     730.68 ms /    76 tokens (    9.61 ms per token,   104.01 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3983.84 ms /    44 runs   (   90.54 ms per token,    11.04 tokens per second)\n",
      "llama_perf_context_print:       total time =    4723.38 ms /   120 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 8 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 119 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    4401.62 ms /    45 runs   (   97.81 ms per token,    10.22 tokens per second)\n",
      "llama_perf_context_print:       total time =    4409.65 ms /    46 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 9 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 44 prefix-match hit, remaining 74 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     847.16 ms /    74 tokens (   11.45 ms per token,    87.35 tokens per second)\n",
      "llama_perf_context_print:        eval time =    8406.98 ms /    93 runs   (   90.40 ms per token,    11.06 tokens per second)\n",
      "llama_perf_context_print:       total time =    9271.24 ms /   167 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 10 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 44 prefix-match hit, remaining 74 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     833.56 ms /    74 tokens (   11.26 ms per token,    88.78 tokens per second)\n",
      "llama_perf_context_print:        eval time =    8404.04 ms /    93 runs   (   90.37 ms per token,    11.07 tokens per second)\n",
      "llama_perf_context_print:       total time =    9254.34 ms /   167 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 11 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 44 prefix-match hit, remaining 74 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     865.38 ms /    74 tokens (   11.69 ms per token,    85.51 tokens per second)\n",
      "llama_perf_context_print:        eval time =    5791.35 ms /    64 runs   (   90.49 ms per token,    11.05 tokens per second)\n",
      "llama_perf_context_print:       total time =    6671.23 ms /   138 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 12 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 44 prefix-match hit, remaining 74 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     729.34 ms /    74 tokens (    9.86 ms per token,   101.46 tokens per second)\n",
      "llama_perf_context_print:        eval time =    5067.80 ms /    56 runs   (   90.50 ms per token,    11.05 tokens per second)\n",
      "llama_perf_context_print:       total time =    5808.24 ms /   130 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 13 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 44 prefix-match hit, remaining 74 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     727.08 ms /    74 tokens (    9.83 ms per token,   101.78 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4783.14 ms /    53 runs   (   90.25 ms per token,    11.08 tokens per second)\n",
      "llama_perf_context_print:       total time =    5522.31 ms /   127 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 14 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 44 prefix-match hit, remaining 74 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     728.70 ms /    74 tokens (    9.85 ms per token,   101.55 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4787.01 ms /    53 runs   (   90.32 ms per token,    11.07 tokens per second)\n",
      "llama_perf_context_print:       total time =    5525.79 ms /   127 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 15 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 117 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    5021.21 ms /    54 runs   (   92.99 ms per token,    10.75 tokens per second)\n",
      "llama_perf_context_print:       total time =    5030.22 ms /    55 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 16 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 44 prefix-match hit, remaining 74 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     913.48 ms /    74 tokens (   12.34 ms per token,    81.01 tokens per second)\n",
      "llama_perf_context_print:        eval time =    9867.76 ms /   109 runs   (   90.53 ms per token,    11.05 tokens per second)\n",
      "llama_perf_context_print:       total time =   10805.25 ms /   183 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 17 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 44 prefix-match hit, remaining 74 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     926.19 ms /    74 tokens (   12.52 ms per token,    79.90 tokens per second)\n",
      "llama_perf_context_print:        eval time =    8488.43 ms /    94 runs   (   90.30 ms per token,    11.07 tokens per second)\n",
      "llama_perf_context_print:       total time =    9431.13 ms /   168 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 18 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 44 prefix-match hit, remaining 74 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     722.84 ms /    74 tokens (    9.77 ms per token,   102.37 tokens per second)\n",
      "llama_perf_context_print:        eval time =    6233.56 ms /    69 runs   (   90.34 ms per token,    11.07 tokens per second)\n",
      "llama_perf_context_print:       total time =    6968.64 ms /   143 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 19 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 117 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    6462.58 ms /    70 runs   (   92.32 ms per token,    10.83 tokens per second)\n",
      "llama_perf_context_print:       total time =    6473.95 ms /    71 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 20 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 44 prefix-match hit, remaining 74 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     736.07 ms /    74 tokens (    9.95 ms per token,   100.53 tokens per second)\n",
      "llama_perf_context_print:        eval time =    5516.95 ms /    61 runs   (   90.44 ms per token,    11.06 tokens per second)\n",
      "llama_perf_context_print:       total time =    6266.50 ms /   135 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 21 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 44 prefix-match hit, remaining 74 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     829.21 ms /    74 tokens (   11.21 ms per token,    89.24 tokens per second)\n",
      "llama_perf_context_print:        eval time =    5692.06 ms /    63 runs   (   90.35 ms per token,    11.07 tokens per second)\n",
      "llama_perf_context_print:       total time =    6533.88 ms /   137 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 22 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 44 prefix-match hit, remaining 74 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     728.77 ms /    74 tokens (    9.85 ms per token,   101.54 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4160.26 ms /    46 runs   (   90.44 ms per token,    11.06 tokens per second)\n",
      "llama_perf_context_print:       total time =    4899.88 ms /   120 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 23 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 117 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    4413.97 ms /    47 runs   (   93.91 ms per token,    10.65 tokens per second)\n",
      "llama_perf_context_print:       total time =    4421.83 ms /    48 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 24 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 44 prefix-match hit, remaining 75 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     723.89 ms /    75 tokens (    9.65 ms per token,   103.61 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3533.32 ms /    39 runs   (   90.60 ms per token,    11.04 tokens per second)\n",
      "llama_perf_context_print:       total time =    4265.13 ms /   114 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 25 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 44 prefix-match hit, remaining 75 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     724.08 ms /    75 tokens (    9.65 ms per token,   103.58 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2723.86 ms /    30 runs   (   90.80 ms per token,    11.01 tokens per second)\n",
      "llama_perf_context_print:       total time =    3455.41 ms /   105 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 26 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 118 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    2956.97 ms /    31 runs   (   95.39 ms per token,    10.48 tokens per second)\n",
      "llama_perf_context_print:       total time =    2965.87 ms /    32 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 27 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 44 prefix-match hit, remaining 76 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     726.33 ms /    76 tokens (    9.56 ms per token,   104.64 tokens per second)\n",
      "llama_perf_context_print:        eval time =    5787.78 ms /    64 runs   (   90.43 ms per token,    11.06 tokens per second)\n",
      "llama_perf_context_print:       total time =    6530.42 ms /   140 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 28 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 44 prefix-match hit, remaining 76 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     731.86 ms /    76 tokens (    9.63 ms per token,   103.84 tokens per second)\n",
      "llama_perf_context_print:        eval time =    7102.22 ms /    78 runs   (   91.05 ms per token,    10.98 tokens per second)\n",
      "llama_perf_context_print:       total time =    7848.30 ms /   154 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 29 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 119 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    7333.46 ms /    79 runs   (   92.83 ms per token,    10.77 tokens per second)\n",
      "llama_perf_context_print:       total time =    7350.77 ms /    80 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 30 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 44 prefix-match hit, remaining 74 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     729.26 ms /    74 tokens (    9.85 ms per token,   101.47 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2181.00 ms /    24 runs   (   90.88 ms per token,    11.00 tokens per second)\n",
      "llama_perf_context_print:       total time =    2916.96 ms /    98 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 31 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 44 prefix-match hit, remaining 74 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     725.79 ms /    74 tokens (    9.81 ms per token,   101.96 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4706.97 ms /    52 runs   (   90.52 ms per token,    11.05 tokens per second)\n",
      "llama_perf_context_print:       total time =    5442.48 ms /   126 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 32 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 117 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    4946.02 ms /    53 runs   (   93.32 ms per token,    10.72 tokens per second)\n",
      "llama_perf_context_print:       total time =    4954.65 ms /    54 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 33 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 44 prefix-match hit, remaining 80 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     729.73 ms /    80 tokens (    9.12 ms per token,   109.63 tokens per second)\n",
      "llama_perf_context_print:        eval time =    5686.40 ms /    63 runs   (   90.26 ms per token,    11.08 tokens per second)\n",
      "llama_perf_context_print:       total time =    6427.72 ms /   143 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 34 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 44 prefix-match hit, remaining 80 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     723.94 ms /    80 tokens (    9.05 ms per token,   110.51 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4617.82 ms /    51 runs   (   90.55 ms per token,    11.04 tokens per second)\n",
      "llama_perf_context_print:       total time =    5352.56 ms /   131 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 35 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 123 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    4865.19 ms /    52 runs   (   93.56 ms per token,    10.69 tokens per second)\n",
      "llama_perf_context_print:       total time =    4875.05 ms /    53 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 36 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 44 prefix-match hit, remaining 74 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     748.06 ms /    74 tokens (   10.11 ms per token,    98.92 tokens per second)\n",
      "llama_perf_context_print:        eval time =    7590.39 ms /    84 runs   (   90.36 ms per token,    11.07 tokens per second)\n",
      "llama_perf_context_print:       total time =    8353.74 ms /   158 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 37 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 44 prefix-match hit, remaining 74 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     733.92 ms /    74 tokens (    9.92 ms per token,   100.83 tokens per second)\n",
      "llama_perf_context_print:        eval time =    7592.07 ms /    84 runs   (   90.38 ms per token,    11.06 tokens per second)\n",
      "llama_perf_context_print:       total time =    8341.22 ms /   158 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 38 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 117 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    7820.88 ms /    85 runs   (   92.01 ms per token,    10.87 tokens per second)\n",
      "llama_perf_context_print:       total time =    7834.34 ms /    86 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 39 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 44 prefix-match hit, remaining 74 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     909.19 ms /    74 tokens (   12.29 ms per token,    81.39 tokens per second)\n",
      "llama_perf_context_print:        eval time =    6089.55 ms /    67 runs   (   90.89 ms per token,    11.00 tokens per second)\n",
      "llama_perf_context_print:       total time =    7010.75 ms /   141 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 40 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 44 prefix-match hit, remaining 74 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     720.44 ms /    74 tokens (    9.74 ms per token,   102.72 tokens per second)\n",
      "llama_perf_context_print:        eval time =    6768.60 ms /    75 runs   (   90.25 ms per token,    11.08 tokens per second)\n",
      "llama_perf_context_print:       total time =    7502.48 ms /   149 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 41 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 44 prefix-match hit, remaining 74 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     719.38 ms /    74 tokens (    9.72 ms per token,   102.87 tokens per second)\n",
      "llama_perf_context_print:        eval time =    6059.62 ms /    67 runs   (   90.44 ms per token,    11.06 tokens per second)\n",
      "llama_perf_context_print:       total time =    6791.63 ms /   141 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 42 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 44 prefix-match hit, remaining 74 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     744.80 ms /    74 tokens (   10.06 ms per token,    99.36 tokens per second)\n",
      "llama_perf_context_print:        eval time =    7137.93 ms /    79 runs   (   90.35 ms per token,    11.07 tokens per second)\n",
      "llama_perf_context_print:       total time =    7896.40 ms /   153 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 43 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 44 prefix-match hit, remaining 74 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     733.14 ms /    74 tokens (    9.91 ms per token,   100.94 tokens per second)\n",
      "llama_perf_context_print:        eval time =    6980.11 ms /    77 runs   (   90.65 ms per token,    11.03 tokens per second)\n",
      "llama_perf_context_print:       total time =    7726.78 ms /   151 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 44 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 117 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    7314.70 ms /    78 runs   (   93.78 ms per token,    10.66 tokens per second)\n",
      "llama_perf_context_print:       total time =    7328.04 ms /    79 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 45 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 44 prefix-match hit, remaining 74 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     725.77 ms /    74 tokens (    9.81 ms per token,   101.96 tokens per second)\n",
      "llama_perf_context_print:        eval time =    7039.94 ms /    78 runs   (   90.26 ms per token,    11.08 tokens per second)\n",
      "llama_perf_context_print:       total time =    7779.85 ms /   152 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 46 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 44 prefix-match hit, remaining 74 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     910.43 ms /    74 tokens (   12.30 ms per token,    81.28 tokens per second)\n",
      "llama_perf_context_print:        eval time =    6236.62 ms /    69 runs   (   90.39 ms per token,    11.06 tokens per second)\n",
      "llama_perf_context_print:       total time =    7160.69 ms /   143 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 47 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 44 prefix-match hit, remaining 74 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     747.38 ms /    74 tokens (   10.10 ms per token,    99.01 tokens per second)\n",
      "llama_perf_context_print:        eval time =    7670.05 ms /    61 runs   (  125.74 ms per token,     7.95 tokens per second)\n",
      "llama_perf_context_print:       total time =    8431.21 ms /   135 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 48 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 117 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    7906.43 ms /    62 runs   (  127.52 ms per token,     7.84 tokens per second)\n",
      "llama_perf_context_print:       total time =    7918.77 ms /    63 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 49 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 44 prefix-match hit, remaining 74 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     876.32 ms /    74 tokens (   11.84 ms per token,    84.44 tokens per second)\n",
      "llama_perf_context_print:        eval time =    5263.67 ms /    44 runs   (  119.63 ms per token,     8.36 tokens per second)\n",
      "llama_perf_context_print:       total time =    6150.30 ms /   118 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 50 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 44 prefix-match hit, remaining 74 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     922.01 ms /    74 tokens (   12.46 ms per token,    80.26 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2443.67 ms /    27 runs   (   90.51 ms per token,    11.05 tokens per second)\n",
      "llama_perf_context_print:       total time =    3372.66 ms /   101 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 51 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 44 prefix-match hit, remaining 74 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     751.02 ms /    74 tokens (   10.15 ms per token,    98.53 tokens per second)\n",
      "llama_perf_context_print:        eval time =    5143.78 ms /    44 runs   (  116.90 ms per token,     8.55 tokens per second)\n",
      "llama_perf_context_print:       total time =    5903.61 ms /   118 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 52 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 44 prefix-match hit, remaining 73 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     735.20 ms /    73 tokens (   10.07 ms per token,    99.29 tokens per second)\n",
      "llama_perf_context_print:        eval time =    8899.07 ms /    77 runs   (  115.57 ms per token,     8.65 tokens per second)\n",
      "llama_perf_context_print:       total time =    9648.60 ms /   150 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 53 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 44 prefix-match hit, remaining 73 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     909.96 ms /    73 tokens (   12.47 ms per token,    80.22 tokens per second)\n",
      "llama_perf_context_print:        eval time =   10571.01 ms /    97 runs   (  108.98 ms per token,     9.18 tokens per second)\n",
      "llama_perf_context_print:       total time =   11498.70 ms /   170 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 54 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 116 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =   10089.16 ms /    98 runs   (  102.95 ms per token,     9.71 tokens per second)\n",
      "llama_perf_context_print:       total time =   10106.03 ms /    99 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 55 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 44 prefix-match hit, remaining 77 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     854.05 ms /    77 tokens (   11.09 ms per token,    90.16 tokens per second)\n",
      "llama_perf_context_print:        eval time =    8137.61 ms /    80 runs   (  101.72 ms per token,     9.83 tokens per second)\n",
      "llama_perf_context_print:       total time =    9006.34 ms /   157 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 56 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 44 prefix-match hit, remaining 77 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     735.76 ms /    77 tokens (    9.56 ms per token,   104.65 tokens per second)\n",
      "llama_perf_context_print:        eval time =    7873.15 ms /    77 runs   (  102.25 ms per token,     9.78 tokens per second)\n",
      "llama_perf_context_print:       total time =    8623.31 ms /   154 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 57 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 120 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    8174.80 ms /    78 runs   (  104.81 ms per token,     9.54 tokens per second)\n",
      "llama_perf_context_print:       total time =    8188.08 ms /    79 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 58 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 44 prefix-match hit, remaining 77 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     721.30 ms /    77 tokens (    9.37 ms per token,   106.75 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2894.72 ms /    32 runs   (   90.46 ms per token,    11.05 tokens per second)\n",
      "llama_perf_context_print:       total time =    3623.81 ms /   109 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 59 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 44 prefix-match hit, remaining 77 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     736.88 ms /    77 tokens (    9.57 ms per token,   104.49 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2901.65 ms /    32 runs   (   90.68 ms per token,    11.03 tokens per second)\n",
      "llama_perf_context_print:       total time =    3645.78 ms /   109 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 60 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 120 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    3137.96 ms /    33 runs   (   95.09 ms per token,    10.52 tokens per second)\n",
      "llama_perf_context_print:       total time =    3143.91 ms /    34 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 61 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 44 prefix-match hit, remaining 80 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     751.42 ms /    80 tokens (    9.39 ms per token,   106.47 tokens per second)\n",
      "llama_perf_context_print:        eval time =   17490.66 ms /   150 runs   (  116.60 ms per token,     8.58 tokens per second)\n",
      "llama_perf_context_print:       total time =   18272.50 ms /   230 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 62 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 44 prefix-match hit, remaining 80 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     735.00 ms /    80 tokens (    9.19 ms per token,   108.84 tokens per second)\n",
      "llama_perf_context_print:        eval time =    9915.53 ms /    97 runs   (  102.22 ms per token,     9.78 tokens per second)\n",
      "llama_perf_context_print:       total time =   10689.85 ms /   177 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 63 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 123 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =   10123.92 ms /    98 runs   (  103.31 ms per token,     9.68 tokens per second)\n",
      "llama_perf_context_print:       total time =   10149.25 ms /    99 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 64 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 44 prefix-match hit, remaining 79 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     728.40 ms /    79 tokens (    9.22 ms per token,   108.46 tokens per second)\n",
      "llama_perf_context_print:        eval time =    9646.18 ms /    94 runs   (  102.62 ms per token,     9.74 tokens per second)\n",
      "llama_perf_context_print:       total time =   10393.42 ms /   173 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 65 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 44 prefix-match hit, remaining 79 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     926.00 ms /    79 tokens (   11.72 ms per token,    85.31 tokens per second)\n",
      "llama_perf_context_print:        eval time =   11628.35 ms /   109 runs   (  106.68 ms per token,     9.37 tokens per second)\n",
      "llama_perf_context_print:       total time =   12574.21 ms /   188 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 66 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 44 prefix-match hit, remaining 79 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     914.57 ms /    79 tokens (   11.58 ms per token,    86.38 tokens per second)\n",
      "llama_perf_context_print:        eval time =    5433.50 ms /    60 runs   (   90.56 ms per token,    11.04 tokens per second)\n",
      "llama_perf_context_print:       total time =    6360.02 ms /   139 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 67 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 122 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    6813.84 ms /    61 runs   (  111.70 ms per token,     8.95 tokens per second)\n",
      "llama_perf_context_print:       total time =    6826.83 ms /    62 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 68 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 44 prefix-match hit, remaining 79 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     733.48 ms /    79 tokens (    9.28 ms per token,   107.71 tokens per second)\n",
      "llama_perf_context_print:        eval time =   14000.86 ms /   124 runs   (  112.91 ms per token,     8.86 tokens per second)\n",
      "llama_perf_context_print:       total time =   14757.92 ms /   203 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 69 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 44 prefix-match hit, remaining 79 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     851.79 ms /    79 tokens (   10.78 ms per token,    92.75 tokens per second)\n",
      "llama_perf_context_print:        eval time =   12081.35 ms /   112 runs   (  107.87 ms per token,     9.27 tokens per second)\n",
      "llama_perf_context_print:       total time =   12954.26 ms /   191 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 70 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 122 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =   12285.06 ms /   113 runs   (  108.72 ms per token,     9.20 tokens per second)\n",
      "llama_perf_context_print:       total time =   12303.48 ms /   114 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 71 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 44 prefix-match hit, remaining 82 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     732.25 ms /    82 tokens (    8.93 ms per token,   111.98 tokens per second)\n",
      "llama_perf_context_print:        eval time =   17039.20 ms /   157 runs   (  108.53 ms per token,     9.21 tokens per second)\n",
      "llama_perf_context_print:       total time =   17801.94 ms /   239 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 72 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 44 prefix-match hit, remaining 82 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     733.99 ms /    82 tokens (    8.95 ms per token,   111.72 tokens per second)\n",
      "llama_perf_context_print:        eval time =   10832.52 ms /   108 runs   (  100.30 ms per token,     9.97 tokens per second)\n",
      "llama_perf_context_print:       total time =   11586.86 ms /   190 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 73 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 125 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =   11576.32 ms /   109 runs   (  106.20 ms per token,     9.42 tokens per second)\n",
      "llama_perf_context_print:       total time =   11594.47 ms /   110 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 74 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 44 prefix-match hit, remaining 77 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     726.93 ms /    77 tokens (    9.44 ms per token,   105.92 tokens per second)\n",
      "llama_perf_context_print:        eval time =    5885.25 ms /    65 runs   (   90.54 ms per token,    11.04 tokens per second)\n",
      "llama_perf_context_print:       total time =    6624.14 ms /   142 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 75 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 44 prefix-match hit, remaining 77 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     735.73 ms /    77 tokens (    9.55 ms per token,   104.66 tokens per second)\n",
      "llama_perf_context_print:        eval time =    8042.57 ms /    76 runs   (  105.82 ms per token,     9.45 tokens per second)\n",
      "llama_perf_context_print:       total time =    8799.38 ms /   153 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 76 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 44 prefix-match hit, remaining 77 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     786.26 ms /    77 tokens (   10.21 ms per token,    97.93 tokens per second)\n",
      "llama_perf_context_print:        eval time =    9867.35 ms /    88 runs   (  112.13 ms per token,     8.92 tokens per second)\n",
      "llama_perf_context_print:       total time =   10673.32 ms /   165 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 77 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 120 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =   10045.05 ms /    89 runs   (  112.87 ms per token,     8.86 tokens per second)\n",
      "llama_perf_context_print:       total time =   10061.08 ms /    90 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 78 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 44 prefix-match hit, remaining 79 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     732.63 ms /    79 tokens (    9.27 ms per token,   107.83 tokens per second)\n",
      "llama_perf_context_print:        eval time =    7291.02 ms /    68 runs   (  107.22 ms per token,     9.33 tokens per second)\n",
      "llama_perf_context_print:       total time =    8040.79 ms /   147 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 79 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 44 prefix-match hit, remaining 79 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     741.98 ms /    79 tokens (    9.39 ms per token,   106.47 tokens per second)\n",
      "llama_perf_context_print:        eval time =    7360.40 ms /    68 runs   (  108.24 ms per token,     9.24 tokens per second)\n",
      "llama_perf_context_print:       total time =    8115.61 ms /   147 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 80 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 44 prefix-match hit, remaining 79 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     738.03 ms /    79 tokens (    9.34 ms per token,   107.04 tokens per second)\n",
      "llama_perf_context_print:        eval time =    7772.83 ms /    72 runs   (  107.96 ms per token,     9.26 tokens per second)\n",
      "llama_perf_context_print:       total time =    8526.13 ms /   151 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 81 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 122 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    7894.93 ms /    73 runs   (  108.15 ms per token,     9.25 tokens per second)\n",
      "llama_perf_context_print:       total time =    7914.66 ms /    74 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 82 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 44 prefix-match hit, remaining 76 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     727.99 ms /    76 tokens (    9.58 ms per token,   104.40 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3807.92 ms /    42 runs   (   90.66 ms per token,    11.03 tokens per second)\n",
      "llama_perf_context_print:       total time =    4545.04 ms /   118 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 83 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 44 prefix-match hit, remaining 76 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     748.21 ms /    76 tokens (    9.84 ms per token,   101.58 tokens per second)\n",
      "llama_perf_context_print:        eval time =   11449.68 ms /    98 runs   (  116.83 ms per token,     8.56 tokens per second)\n",
      "llama_perf_context_print:       total time =   12216.62 ms /   174 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 84 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 44 prefix-match hit, remaining 77 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     734.75 ms /    77 tokens (    9.54 ms per token,   104.80 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4258.92 ms /    47 runs   (   90.62 ms per token,    11.04 tokens per second)\n",
      "llama_perf_context_print:       total time =    5003.13 ms /   124 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 85 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 44 prefix-match hit, remaining 77 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     733.72 ms /    77 tokens (    9.53 ms per token,   104.94 tokens per second)\n",
      "llama_perf_context_print:        eval time =    5616.38 ms /    50 runs   (  112.33 ms per token,     8.90 tokens per second)\n",
      "llama_perf_context_print:       total time =    6359.80 ms /   127 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 86 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 120 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    5831.29 ms /    51 runs   (  114.34 ms per token,     8.75 tokens per second)\n",
      "llama_perf_context_print:       total time =    5840.41 ms /    52 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 87 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 44 prefix-match hit, remaining 75 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     728.65 ms /    75 tokens (    9.72 ms per token,   102.93 tokens per second)\n",
      "llama_perf_context_print:        eval time =    5725.83 ms /    51 runs   (  112.27 ms per token,     8.91 tokens per second)\n",
      "llama_perf_context_print:       total time =    6465.49 ms /   126 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 88 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 44 prefix-match hit, remaining 75 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     729.84 ms /    75 tokens (    9.73 ms per token,   102.76 tokens per second)\n",
      "llama_perf_context_print:        eval time =    5811.11 ms /    51 runs   (  113.94 ms per token,     8.78 tokens per second)\n",
      "llama_perf_context_print:       total time =    6552.07 ms /   126 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 89 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 44 prefix-match hit, remaining 75 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     919.78 ms /    75 tokens (   12.26 ms per token,    81.54 tokens per second)\n",
      "llama_perf_context_print:        eval time =    5768.89 ms /    51 runs   (  113.12 ms per token,     8.84 tokens per second)\n",
      "llama_perf_context_print:       total time =    6705.64 ms /   126 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 90 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 44 prefix-match hit, remaining 75 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     744.46 ms /    75 tokens (    9.93 ms per token,   100.74 tokens per second)\n",
      "llama_perf_context_print:        eval time =    6661.76 ms /    60 runs   (  111.03 ms per token,     9.01 tokens per second)\n",
      "llama_perf_context_print:       total time =    7418.69 ms /   135 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 91 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 118 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    6859.44 ms /    61 runs   (  112.45 ms per token,     8.89 tokens per second)\n",
      "llama_perf_context_print:       total time =    6870.00 ms /    62 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 92 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 44 prefix-match hit, remaining 74 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     839.66 ms /    74 tokens (   11.35 ms per token,    88.13 tokens per second)\n",
      "llama_perf_context_print:        eval time =    9002.28 ms /    78 runs   (  115.41 ms per token,     8.66 tokens per second)\n",
      "llama_perf_context_print:       total time =    9857.52 ms /   152 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 93 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 44 prefix-match hit, remaining 74 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     870.44 ms /    74 tokens (   11.76 ms per token,    85.01 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3811.94 ms /    42 runs   (   90.76 ms per token,    11.02 tokens per second)\n",
      "llama_perf_context_print:       total time =    4690.76 ms /   116 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 94 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 117 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    4073.88 ms /    43 runs   (   94.74 ms per token,    10.56 tokens per second)\n",
      "llama_perf_context_print:       total time =    4081.48 ms /    44 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 95 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 44 prefix-match hit, remaining 78 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     735.66 ms /    78 tokens (    9.43 ms per token,   106.03 tokens per second)\n",
      "llama_perf_context_print:        eval time =    8739.59 ms /    76 runs   (  114.99 ms per token,     8.70 tokens per second)\n",
      "llama_perf_context_print:       total time =    9489.59 ms /   154 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 96 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 44 prefix-match hit, remaining 78 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     899.03 ms /    78 tokens (   11.53 ms per token,    86.76 tokens per second)\n",
      "llama_perf_context_print:        eval time =    8524.63 ms /    76 runs   (  112.17 ms per token,     8.92 tokens per second)\n",
      "llama_perf_context_print:       total time =    9438.35 ms /   154 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 97 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 121 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    8284.68 ms /    77 runs   (  107.59 ms per token,     9.29 tokens per second)\n",
      "llama_perf_context_print:       total time =    8297.75 ms /    78 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 98 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 44 prefix-match hit, remaining 84 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     738.44 ms /    84 tokens (    8.79 ms per token,   113.75 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4357.31 ms /    48 runs   (   90.78 ms per token,    11.02 tokens per second)\n",
      "llama_perf_context_print:       total time =    5106.00 ms /   132 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 99 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 44 prefix-match hit, remaining 84 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     751.24 ms /    84 tokens (    8.94 ms per token,   111.82 tokens per second)\n",
      "llama_perf_context_print:        eval time =    5255.34 ms /    48 runs   (  109.49 ms per token,     9.13 tokens per second)\n",
      "llama_perf_context_print:       total time =    6018.54 ms /   132 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 100 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 44 prefix-match hit, remaining 84 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     742.17 ms /    84 tokens (    8.84 ms per token,   113.18 tokens per second)\n",
      "llama_perf_context_print:        eval time =    8579.02 ms /    76 runs   (  112.88 ms per token,     8.86 tokens per second)\n",
      "llama_perf_context_print:       total time =    9337.06 ms /   160 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 101 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 44 prefix-match hit, remaining 84 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     731.59 ms /    84 tokens (    8.71 ms per token,   114.82 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4374.78 ms /    48 runs   (   91.14 ms per token,    10.97 tokens per second)\n",
      "llama_perf_context_print:       total time =    5115.51 ms /   132 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 102 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 44 prefix-match hit, remaining 84 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     739.71 ms /    84 tokens (    8.81 ms per token,   113.56 tokens per second)\n",
      "llama_perf_context_print:        eval time =    5851.81 ms /    52 runs   (  112.53 ms per token,     8.89 tokens per second)\n",
      "llama_perf_context_print:       total time =    6606.34 ms /   136 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 103 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 127 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    5917.43 ms /    51 runs   (  116.03 ms per token,     8.62 tokens per second)\n",
      "llama_perf_context_print:       total time =    5927.04 ms /    52 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 104 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 44 prefix-match hit, remaining 74 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     730.53 ms /    74 tokens (    9.87 ms per token,   101.30 tokens per second)\n",
      "llama_perf_context_print:        eval time =    5595.96 ms /    49 runs   (  114.20 ms per token,     8.76 tokens per second)\n",
      "llama_perf_context_print:       total time =    6337.67 ms /   123 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 105 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 44 prefix-match hit, remaining 74 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     733.11 ms /    74 tokens (    9.91 ms per token,   100.94 tokens per second)\n",
      "llama_perf_context_print:        eval time =    5318.79 ms /    46 runs   (  115.63 ms per token,     8.65 tokens per second)\n",
      "llama_perf_context_print:       total time =    6061.64 ms /   120 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 106 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 44 prefix-match hit, remaining 74 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     745.67 ms /    74 tokens (   10.08 ms per token,    99.24 tokens per second)\n",
      "llama_perf_context_print:        eval time =    5612.20 ms /    49 runs   (  114.53 ms per token,     8.73 tokens per second)\n",
      "llama_perf_context_print:       total time =    6369.26 ms /   123 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 107 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 117 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    5825.18 ms /    50 runs   (  116.50 ms per token,     8.58 tokens per second)\n",
      "llama_perf_context_print:       total time =    5834.98 ms /    51 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 108 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 44 prefix-match hit, remaining 77 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     739.19 ms /    77 tokens (    9.60 ms per token,   104.17 tokens per second)\n",
      "llama_perf_context_print:        eval time =    6260.22 ms /    56 runs   (  111.79 ms per token,     8.95 tokens per second)\n",
      "llama_perf_context_print:       total time =    7011.78 ms /   133 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 109 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 44 prefix-match hit, remaining 77 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     969.56 ms /    77 tokens (   12.59 ms per token,    79.42 tokens per second)\n",
      "llama_perf_context_print:        eval time =    6454.78 ms /    58 runs   (  111.29 ms per token,     8.99 tokens per second)\n",
      "llama_perf_context_print:       total time =    7435.35 ms /   135 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 110 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 44 prefix-match hit, remaining 77 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     817.55 ms /    77 tokens (   10.62 ms per token,    94.18 tokens per second)\n",
      "llama_perf_context_print:        eval time =    6647.77 ms /    60 runs   (  110.80 ms per token,     9.03 tokens per second)\n",
      "llama_perf_context_print:       total time =    7477.73 ms /   137 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 111 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 44 prefix-match hit, remaining 77 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     743.30 ms /    77 tokens (    9.65 ms per token,   103.59 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3807.41 ms /    41 runs   (   92.86 ms per token,    10.77 tokens per second)\n",
      "llama_perf_context_print:       total time =    4558.83 ms /   118 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 112 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 44 prefix-match hit, remaining 78 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     734.99 ms /    78 tokens (    9.42 ms per token,   106.12 tokens per second)\n",
      "llama_perf_context_print:        eval time =    6462.95 ms /    58 runs   (  111.43 ms per token,     8.97 tokens per second)\n",
      "llama_perf_context_print:       total time =    7210.45 ms /   136 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 113 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 44 prefix-match hit, remaining 78 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     947.20 ms /    78 tokens (   12.14 ms per token,    82.35 tokens per second)\n",
      "llama_perf_context_print:        eval time =    6363.26 ms /    57 runs   (  111.64 ms per token,     8.96 tokens per second)\n",
      "llama_perf_context_print:       total time =    7322.79 ms /   135 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 114 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 44 prefix-match hit, remaining 78 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     735.56 ms /    78 tokens (    9.43 ms per token,   106.04 tokens per second)\n",
      "llama_perf_context_print:        eval time =    5694.54 ms /    50 runs   (  113.89 ms per token,     8.78 tokens per second)\n",
      "llama_perf_context_print:       total time =    6439.50 ms /   128 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 115 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 44 prefix-match hit, remaining 78 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     742.95 ms /    78 tokens (    9.53 ms per token,   104.99 tokens per second)\n",
      "llama_perf_context_print:        eval time =    7520.48 ms /    64 runs   (  117.51 ms per token,     8.51 tokens per second)\n",
      "llama_perf_context_print:       total time =    8276.03 ms /   142 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 116 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 44 prefix-match hit, remaining 78 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     735.79 ms /    78 tokens (    9.43 ms per token,   106.01 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3815.49 ms /    42 runs   (   90.84 ms per token,    11.01 tokens per second)\n",
      "llama_perf_context_print:       total time =    4560.14 ms /   120 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 117 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 44 prefix-match hit, remaining 78 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     738.47 ms /    78 tokens (    9.47 ms per token,   105.62 tokens per second)\n",
      "llama_perf_context_print:        eval time =    6191.85 ms /    55 runs   (  112.58 ms per token,     8.88 tokens per second)\n",
      "llama_perf_context_print:       total time =    6942.27 ms /   133 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 118 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 44 prefix-match hit, remaining 86 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     755.44 ms /    86 tokens (    8.78 ms per token,   113.84 tokens per second)\n",
      "llama_perf_context_print:        eval time =    9484.67 ms /    81 runs   (  117.09 ms per token,     8.54 tokens per second)\n",
      "llama_perf_context_print:       total time =   10256.88 ms /   167 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 119 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 44 prefix-match hit, remaining 86 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     861.77 ms /    86 tokens (   10.02 ms per token,    99.80 tokens per second)\n",
      "llama_perf_context_print:        eval time =    9300.33 ms /    81 runs   (  114.82 ms per token,     8.71 tokens per second)\n",
      "llama_perf_context_print:       total time =   10176.59 ms /   167 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 120 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 129 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    9268.19 ms /    82 runs   (  113.03 ms per token,     8.85 tokens per second)\n",
      "llama_perf_context_print:       total time =    9281.55 ms /    83 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 121 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 104 prefix-match hit, remaining 18 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     368.43 ms /    18 tokens (   20.47 ms per token,    48.86 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3174.58 ms /    35 runs   (   90.70 ms per token,    11.03 tokens per second)\n",
      "llama_perf_context_print:       total time =    3549.51 ms /    53 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 122 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 44 prefix-match hit, remaining 78 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     744.23 ms /    78 tokens (    9.54 ms per token,   104.81 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3082.36 ms /    34 runs   (   90.66 ms per token,    11.03 tokens per second)\n",
      "llama_perf_context_print:       total time =    3843.81 ms /   112 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 123 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 121 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    3353.68 ms /    35 runs   (   95.82 ms per token,    10.44 tokens per second)\n",
      "llama_perf_context_print:       total time =    3360.24 ms /    36 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 124 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 44 prefix-match hit, remaining 78 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     740.41 ms /    78 tokens (    9.49 ms per token,   105.35 tokens per second)\n",
      "llama_perf_context_print:        eval time =    5732.84 ms /    51 runs   (  112.41 ms per token,     8.90 tokens per second)\n",
      "llama_perf_context_print:       total time =    6487.38 ms /   129 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 125 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 44 prefix-match hit, remaining 78 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     919.65 ms /    78 tokens (   11.79 ms per token,    84.81 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3180.13 ms /    35 runs   (   90.86 ms per token,    11.01 tokens per second)\n",
      "llama_perf_context_print:       total time =    4107.47 ms /   113 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 126 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 44 prefix-match hit, remaining 78 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     853.06 ms /    78 tokens (   10.94 ms per token,    91.44 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4771.12 ms /    41 runs   (  116.37 ms per token,     8.59 tokens per second)\n",
      "llama_perf_context_print:       total time =    5632.16 ms /   119 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 127 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 44 prefix-match hit, remaining 78 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     738.90 ms /    78 tokens (    9.47 ms per token,   105.56 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3925.24 ms /    41 runs   (   95.74 ms per token,    10.45 tokens per second)\n",
      "llama_perf_context_print:       total time =    4673.10 ms /   119 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 128 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 44 prefix-match hit, remaining 78 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     745.73 ms /    78 tokens (    9.56 ms per token,   104.60 tokens per second)\n",
      "llama_perf_context_print:        eval time =    5076.05 ms /    44 runs   (  115.36 ms per token,     8.67 tokens per second)\n",
      "llama_perf_context_print:       total time =    5831.30 ms /   122 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 129 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 44 prefix-match hit, remaining 78 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     738.56 ms /    78 tokens (    9.47 ms per token,   105.61 tokens per second)\n",
      "llama_perf_context_print:        eval time =   10180.31 ms /    85 runs   (  119.77 ms per token,     8.35 tokens per second)\n",
      "llama_perf_context_print:       total time =   10939.99 ms /   163 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 130 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 121 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =   10030.24 ms /    86 runs   (  116.63 ms per token,     8.57 tokens per second)\n",
      "llama_perf_context_print:       total time =   10045.09 ms /    87 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 131 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 44 prefix-match hit, remaining 78 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     734.41 ms /    78 tokens (    9.42 ms per token,   106.21 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3719.08 ms /    41 runs   (   90.71 ms per token,    11.02 tokens per second)\n",
      "llama_perf_context_print:       total time =    4462.22 ms /   119 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 132 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 44 prefix-match hit, remaining 78 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     743.35 ms /    78 tokens (    9.53 ms per token,   104.93 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3807.90 ms /    41 runs   (   92.88 ms per token,    10.77 tokens per second)\n",
      "llama_perf_context_print:       total time =    4559.54 ms /   119 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 133 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 44 prefix-match hit, remaining 78 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     739.02 ms /    78 tokens (    9.47 ms per token,   105.54 tokens per second)\n",
      "llama_perf_context_print:        eval time =    9507.40 ms /    83 runs   (  114.55 ms per token,     8.73 tokens per second)\n",
      "llama_perf_context_print:       total time =   10261.24 ms /   161 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 134 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 121 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    9671.13 ms /    84 runs   (  115.13 ms per token,     8.69 tokens per second)\n",
      "llama_perf_context_print:       total time =    9693.51 ms /    85 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 135 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 44 prefix-match hit, remaining 73 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     731.30 ms /    73 tokens (   10.02 ms per token,    99.82 tokens per second)\n",
      "llama_perf_context_print:        eval time =   10254.23 ms /    91 runs   (  112.68 ms per token,     8.87 tokens per second)\n",
      "llama_perf_context_print:       total time =   11002.24 ms /   164 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 136 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 44 prefix-match hit, remaining 73 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     914.49 ms /    73 tokens (   12.53 ms per token,    79.83 tokens per second)\n",
      "llama_perf_context_print:        eval time =    7838.10 ms /    74 runs   (  105.92 ms per token,     9.44 tokens per second)\n",
      "llama_perf_context_print:       total time =    8765.83 ms /   147 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 137 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 116 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    8102.09 ms /    75 runs   (  108.03 ms per token,     9.26 tokens per second)\n",
      "llama_perf_context_print:       total time =    8116.30 ms /    76 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 138 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 44 prefix-match hit, remaining 75 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     736.33 ms /    75 tokens (    9.82 ms per token,   101.86 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3083.42 ms /    34 runs   (   90.69 ms per token,    11.03 tokens per second)\n",
      "llama_perf_context_print:       total time =    3827.61 ms /   109 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 139 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 44 prefix-match hit, remaining 75 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     802.45 ms /    75 tokens (   10.70 ms per token,    93.46 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3345.60 ms /    34 runs   (   98.40 ms per token,    10.16 tokens per second)\n",
      "llama_perf_context_print:       total time =    4155.40 ms /   109 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 140 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 118 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    3313.40 ms /    35 runs   (   94.67 ms per token,    10.56 tokens per second)\n",
      "llama_perf_context_print:       total time =    3319.65 ms /    36 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 141 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 44 prefix-match hit, remaining 80 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     945.57 ms /    80 tokens (   11.82 ms per token,    84.61 tokens per second)\n",
      "llama_perf_context_print:        eval time =    6230.11 ms /    55 runs   (  113.27 ms per token,     8.83 tokens per second)\n",
      "llama_perf_context_print:       total time =    7186.72 ms /   135 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 142 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 44 prefix-match hit, remaining 80 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     744.30 ms /    80 tokens (    9.30 ms per token,   107.48 tokens per second)\n",
      "llama_perf_context_print:        eval time =    8144.53 ms /    69 runs   (  118.04 ms per token,     8.47 tokens per second)\n",
      "llama_perf_context_print:       total time =    8901.63 ms /   149 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 143 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 123 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    7844.30 ms /    70 runs   (  112.06 ms per token,     8.92 tokens per second)\n",
      "llama_perf_context_print:       total time =    7858.63 ms /    71 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 144 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 44 prefix-match hit, remaining 80 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     728.84 ms /    80 tokens (    9.11 ms per token,   109.76 tokens per second)\n",
      "llama_perf_context_print:        eval time =    7616.62 ms /    68 runs   (  112.01 ms per token,     8.93 tokens per second)\n",
      "llama_perf_context_print:       total time =    8361.48 ms /   148 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 145 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 44 prefix-match hit, remaining 80 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     736.92 ms /    80 tokens (    9.21 ms per token,   108.56 tokens per second)\n",
      "llama_perf_context_print:        eval time =    8573.45 ms /    75 runs   (  114.31 ms per token,     8.75 tokens per second)\n",
      "llama_perf_context_print:       total time =    9325.51 ms /   155 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 146 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 123 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    8415.45 ms /    76 runs   (  110.73 ms per token,     9.03 tokens per second)\n",
      "llama_perf_context_print:       total time =    8429.80 ms /    77 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 147 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 44 prefix-match hit, remaining 75 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     733.03 ms /    75 tokens (    9.77 ms per token,   102.32 tokens per second)\n",
      "llama_perf_context_print:        eval time =    9102.13 ms /    81 runs   (  112.37 ms per token,     8.90 tokens per second)\n",
      "llama_perf_context_print:       total time =    9854.35 ms /   156 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 148 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 44 prefix-match hit, remaining 75 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     731.13 ms /    75 tokens (    9.75 ms per token,   102.58 tokens per second)\n",
      "llama_perf_context_print:        eval time =    9338.45 ms /    83 runs   (  112.51 ms per token,     8.89 tokens per second)\n",
      "llama_perf_context_print:       total time =   10085.36 ms /   158 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 149 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 44 prefix-match hit, remaining 75 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     735.06 ms /    75 tokens (    9.80 ms per token,   102.03 tokens per second)\n",
      "llama_perf_context_print:        eval time =    2824.74 ms /    31 runs   (   91.12 ms per token,    10.97 tokens per second)\n",
      "llama_perf_context_print:       total time =    3568.61 ms /   106 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 150 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 118 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    3046.95 ms /    32 runs   (   95.22 ms per token,    10.50 tokens per second)\n",
      "llama_perf_context_print:       total time =    3052.94 ms /    33 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 151 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 44 prefix-match hit, remaining 84 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     750.83 ms /    84 tokens (    8.94 ms per token,   111.88 tokens per second)\n",
      "llama_perf_context_print:        eval time =    9411.08 ms /    80 runs   (  117.64 ms per token,     8.50 tokens per second)\n",
      "llama_perf_context_print:       total time =   10178.60 ms /   164 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 152 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 44 prefix-match hit, remaining 84 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     733.35 ms /    84 tokens (    8.73 ms per token,   114.54 tokens per second)\n",
      "llama_perf_context_print:        eval time =   11651.40 ms /   100 runs   (  116.51 ms per token,     8.58 tokens per second)\n",
      "llama_perf_context_print:       total time =   12404.51 ms /   184 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 153 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 127 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =   11442.36 ms /   101 runs   (  113.29 ms per token,     8.83 tokens per second)\n",
      "llama_perf_context_print:       total time =   11461.26 ms /   102 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 154 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 104 prefix-match hit, remaining 17 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     369.62 ms /    17 tokens (   21.74 ms per token,    45.99 tokens per second)\n",
      "llama_perf_context_print:        eval time =    8195.67 ms /    78 runs   (  105.07 ms per token,     9.52 tokens per second)\n",
      "llama_perf_context_print:       total time =    8578.58 ms /    95 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 155 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 44 prefix-match hit, remaining 77 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     921.08 ms /    77 tokens (   11.96 ms per token,    83.60 tokens per second)\n",
      "llama_perf_context_print:        eval time =    9268.15 ms /    85 runs   (  109.04 ms per token,     9.17 tokens per second)\n",
      "llama_perf_context_print:       total time =   10205.11 ms /   162 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 156 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 120 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    9623.26 ms /    86 runs   (  111.90 ms per token,     8.94 tokens per second)\n",
      "llama_perf_context_print:       total time =    9637.68 ms /    87 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 157 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 44 prefix-match hit, remaining 79 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     835.77 ms /    79 tokens (   10.58 ms per token,    94.52 tokens per second)\n",
      "llama_perf_context_print:        eval time =    5209.87 ms /    52 runs   (  100.19 ms per token,     9.98 tokens per second)\n",
      "llama_perf_context_print:       total time =    6056.44 ms /   131 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 158 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 44 prefix-match hit, remaining 79 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     733.32 ms /    79 tokens (    9.28 ms per token,   107.73 tokens per second)\n",
      "llama_perf_context_print:        eval time =    7186.59 ms /    66 runs   (  108.89 ms per token,     9.18 tokens per second)\n",
      "llama_perf_context_print:       total time =    7933.85 ms /   145 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 159 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 44 prefix-match hit, remaining 79 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     740.63 ms /    79 tokens (    9.38 ms per token,   106.67 tokens per second)\n",
      "llama_perf_context_print:        eval time =    7305.36 ms /    66 runs   (  110.69 ms per token,     9.03 tokens per second)\n",
      "llama_perf_context_print:       total time =    8058.84 ms /   145 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 160 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 122 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    7440.96 ms /    67 runs   (  111.06 ms per token,     9.00 tokens per second)\n",
      "llama_perf_context_print:       total time =    7453.64 ms /    68 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 161 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 44 prefix-match hit, remaining 73 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     738.00 ms /    73 tokens (   10.11 ms per token,    98.92 tokens per second)\n",
      "llama_perf_context_print:        eval time =    6248.87 ms /    56 runs   (  111.59 ms per token,     8.96 tokens per second)\n",
      "llama_perf_context_print:       total time =    7001.39 ms /   129 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 162 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 44 prefix-match hit, remaining 73 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     742.06 ms /    73 tokens (   10.17 ms per token,    98.37 tokens per second)\n",
      "llama_perf_context_print:        eval time =    5848.19 ms /    51 runs   (  114.67 ms per token,     8.72 tokens per second)\n",
      "llama_perf_context_print:       total time =    6604.62 ms /   124 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 163 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 116 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    6182.95 ms /    52 runs   (  118.90 ms per token,     8.41 tokens per second)\n",
      "llama_perf_context_print:       total time =    6192.83 ms /    53 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 164 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 44 prefix-match hit, remaining 73 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     730.46 ms /    73 tokens (   10.01 ms per token,    99.94 tokens per second)\n",
      "llama_perf_context_print:        eval time =    6165.04 ms /    55 runs   (  112.09 ms per token,     8.92 tokens per second)\n",
      "llama_perf_context_print:       total time =    6907.09 ms /   128 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 165 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 44 prefix-match hit, remaining 73 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     741.56 ms /    73 tokens (   10.16 ms per token,    98.44 tokens per second)\n",
      "llama_perf_context_print:        eval time =    9772.10 ms /    83 runs   (  117.74 ms per token,     8.49 tokens per second)\n",
      "llama_perf_context_print:       total time =   10528.79 ms /   156 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 166 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 116 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    9690.72 ms /    84 runs   (  115.37 ms per token,     8.67 tokens per second)\n",
      "llama_perf_context_print:       total time =    9705.90 ms /    85 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 167 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 44 prefix-match hit, remaining 75 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     926.28 ms /    75 tokens (   12.35 ms per token,    80.97 tokens per second)\n",
      "llama_perf_context_print:        eval time =    7337.70 ms /    68 runs   (  107.91 ms per token,     9.27 tokens per second)\n",
      "llama_perf_context_print:       total time =    8276.99 ms /   143 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 168 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 44 prefix-match hit, remaining 75 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     728.23 ms /    75 tokens (    9.71 ms per token,   102.99 tokens per second)\n",
      "llama_perf_context_print:        eval time =    6875.07 ms /    63 runs   (  109.13 ms per token,     9.16 tokens per second)\n",
      "llama_perf_context_print:       total time =    7615.42 ms /   138 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 169 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 118 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    7128.89 ms /    64 runs   (  111.39 ms per token,     8.98 tokens per second)\n",
      "llama_perf_context_print:       total time =    7140.15 ms /    65 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 170 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 44 prefix-match hit, remaining 80 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     742.15 ms /    80 tokens (    9.28 ms per token,   107.79 tokens per second)\n",
      "llama_perf_context_print:        eval time =    5187.10 ms /    46 runs   (  112.76 ms per token,     8.87 tokens per second)\n",
      "llama_perf_context_print:       total time =    5942.10 ms /   126 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 171 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 44 prefix-match hit, remaining 80 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     742.00 ms /    80 tokens (    9.28 ms per token,   107.82 tokens per second)\n",
      "llama_perf_context_print:        eval time =    5252.34 ms /    46 runs   (  114.18 ms per token,     8.76 tokens per second)\n",
      "llama_perf_context_print:       total time =    6003.55 ms /   126 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 172 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 123 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    4514.32 ms /    47 runs   (   96.05 ms per token,    10.41 tokens per second)\n",
      "llama_perf_context_print:       total time =    4527.11 ms /    48 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 173 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 44 prefix-match hit, remaining 75 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     734.23 ms /    75 tokens (    9.79 ms per token,   102.15 tokens per second)\n",
      "llama_perf_context_print:        eval time =    3622.21 ms /    40 runs   (   90.56 ms per token,    11.04 tokens per second)\n",
      "llama_perf_context_print:       total time =    4364.38 ms /   115 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 174 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 44 prefix-match hit, remaining 75 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =    2063.48 ms /    75 tokens (   27.51 ms per token,    36.35 tokens per second)\n",
      "llama_perf_context_print:        eval time =    5402.34 ms /    37 runs   (  146.01 ms per token,     6.85 tokens per second)\n",
      "llama_perf_context_print:       total time =    7588.11 ms /   112 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 175 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 44 prefix-match hit, remaining 75 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =     783.98 ms /    75 tokens (   10.45 ms per token,    95.67 tokens per second)\n",
      "llama_perf_context_print:        eval time =    4885.69 ms /    35 runs   (  139.59 ms per token,     7.16 tokens per second)\n",
      "llama_perf_context_print:       total time =    5697.84 ms /   110 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 176 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 118 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =    2546.23 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =    3674.12 ms /    36 runs   (  102.06 ms per token,     9.80 tokens per second)\n",
      "llama_perf_context_print:       total time =    3692.73 ms /    37 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question 177 done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 104 prefix-match hit, remaining 16 prompt tokens to eval\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[105]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m prompt = generate_fact_prompt_5(df_long.loc[i,\u001b[33m\"\u001b[39m\u001b[33mDate\u001b[39m\u001b[33m\"\u001b[39m],df_long.loc[i,\u001b[33m'\u001b[39m\u001b[33mquestion\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m      6\u001b[39m st = time.time()\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m resp = llm.create_chat_completion(\n\u001b[32m      8\u001b[39m messages=prompt,\n\u001b[32m      9\u001b[39m max_tokens=\u001b[32m500\u001b[39m,\n\u001b[32m     10\u001b[39m temperature=\u001b[32m0.0\u001b[39m,\n\u001b[32m     11\u001b[39m logprobs=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m     12\u001b[39m top_logprobs=\u001b[32m10\u001b[39m\n\u001b[32m     13\u001b[39m )\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# response = tokenizer.decode(outputs[:, inputs[\"input_ids\"].shape[1]:][0], skip_special_tokens=True)\u001b[39;00m\n\u001b[32m     19\u001b[39m p_yes,p_no,ans = get_yes_no_ans_500(resp)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/llama/lib/python3.11/site-packages/llama_cpp/llama.py:2001\u001b[39m, in \u001b[36mLlama.create_chat_completion\u001b[39m\u001b[34m(self, messages, functions, function_call, tools, tool_choice, temperature, top_p, top_k, min_p, typical_p, stream, stop, seed, response_format, max_tokens, presence_penalty, frequency_penalty, repeat_penalty, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, logits_processor, grammar, logit_bias, logprobs, top_logprobs)\u001b[39m\n\u001b[32m   1963\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Generate a chat completion from a list of messages.\u001b[39;00m\n\u001b[32m   1964\u001b[39m \n\u001b[32m   1965\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1994\u001b[39m \u001b[33;03m    Generated chat completion or a stream of chat completion chunks.\u001b[39;00m\n\u001b[32m   1995\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1996\u001b[39m handler = (\n\u001b[32m   1997\u001b[39m     \u001b[38;5;28mself\u001b[39m.chat_handler\n\u001b[32m   1998\u001b[39m     \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._chat_handlers.get(\u001b[38;5;28mself\u001b[39m.chat_format)\n\u001b[32m   1999\u001b[39m     \u001b[38;5;129;01mor\u001b[39;00m llama_chat_format.get_chat_completion_handler(\u001b[38;5;28mself\u001b[39m.chat_format)\n\u001b[32m   2000\u001b[39m )\n\u001b[32m-> \u001b[39m\u001b[32m2001\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m handler(\n\u001b[32m   2002\u001b[39m     llama=\u001b[38;5;28mself\u001b[39m,\n\u001b[32m   2003\u001b[39m     messages=messages,\n\u001b[32m   2004\u001b[39m     functions=functions,\n\u001b[32m   2005\u001b[39m     function_call=function_call,\n\u001b[32m   2006\u001b[39m     tools=tools,\n\u001b[32m   2007\u001b[39m     tool_choice=tool_choice,\n\u001b[32m   2008\u001b[39m     temperature=temperature,\n\u001b[32m   2009\u001b[39m     top_p=top_p,\n\u001b[32m   2010\u001b[39m     top_k=top_k,\n\u001b[32m   2011\u001b[39m     min_p=min_p,\n\u001b[32m   2012\u001b[39m     typical_p=typical_p,\n\u001b[32m   2013\u001b[39m     logprobs=logprobs,\n\u001b[32m   2014\u001b[39m     top_logprobs=top_logprobs,\n\u001b[32m   2015\u001b[39m     stream=stream,\n\u001b[32m   2016\u001b[39m     stop=stop,\n\u001b[32m   2017\u001b[39m     seed=seed,\n\u001b[32m   2018\u001b[39m     response_format=response_format,\n\u001b[32m   2019\u001b[39m     max_tokens=max_tokens,\n\u001b[32m   2020\u001b[39m     presence_penalty=presence_penalty,\n\u001b[32m   2021\u001b[39m     frequency_penalty=frequency_penalty,\n\u001b[32m   2022\u001b[39m     repeat_penalty=repeat_penalty,\n\u001b[32m   2023\u001b[39m     tfs_z=tfs_z,\n\u001b[32m   2024\u001b[39m     mirostat_mode=mirostat_mode,\n\u001b[32m   2025\u001b[39m     mirostat_tau=mirostat_tau,\n\u001b[32m   2026\u001b[39m     mirostat_eta=mirostat_eta,\n\u001b[32m   2027\u001b[39m     model=model,\n\u001b[32m   2028\u001b[39m     logits_processor=logits_processor,\n\u001b[32m   2029\u001b[39m     grammar=grammar,\n\u001b[32m   2030\u001b[39m     logit_bias=logit_bias,\n\u001b[32m   2031\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/llama/lib/python3.11/site-packages/llama_cpp/llama_chat_format.py:662\u001b[39m, in \u001b[36mchat_formatter_to_chat_completion_handler.<locals>.chat_completion_handler\u001b[39m\u001b[34m(llama, messages, functions, function_call, tools, tool_choice, temperature, top_p, top_k, min_p, typical_p, stream, stop, seed, response_format, max_tokens, presence_penalty, frequency_penalty, repeat_penalty, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, logits_processor, grammar, logit_bias, logprobs, top_logprobs, **kwargs)\u001b[39m\n\u001b[32m    657\u001b[39m             \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mstr\u001b[39m(e), file=sys.stderr)\n\u001b[32m    658\u001b[39m         grammar = llama_grammar.LlamaGrammar.from_string(\n\u001b[32m    659\u001b[39m             llama_grammar.JSON_GBNF, verbose=llama.verbose\n\u001b[32m    660\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m662\u001b[39m completion_or_chunks = llama.create_completion(\n\u001b[32m    663\u001b[39m     prompt=prompt,\n\u001b[32m    664\u001b[39m     temperature=temperature,\n\u001b[32m    665\u001b[39m     top_p=top_p,\n\u001b[32m    666\u001b[39m     top_k=top_k,\n\u001b[32m    667\u001b[39m     min_p=min_p,\n\u001b[32m    668\u001b[39m     typical_p=typical_p,\n\u001b[32m    669\u001b[39m     logprobs=top_logprobs \u001b[38;5;28;01mif\u001b[39;00m logprobs \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    670\u001b[39m     stream=stream,\n\u001b[32m    671\u001b[39m     stop=stop,\n\u001b[32m    672\u001b[39m     seed=seed,\n\u001b[32m    673\u001b[39m     max_tokens=max_tokens,\n\u001b[32m    674\u001b[39m     presence_penalty=presence_penalty,\n\u001b[32m    675\u001b[39m     frequency_penalty=frequency_penalty,\n\u001b[32m    676\u001b[39m     repeat_penalty=repeat_penalty,\n\u001b[32m    677\u001b[39m     tfs_z=tfs_z,\n\u001b[32m    678\u001b[39m     mirostat_mode=mirostat_mode,\n\u001b[32m    679\u001b[39m     mirostat_tau=mirostat_tau,\n\u001b[32m    680\u001b[39m     mirostat_eta=mirostat_eta,\n\u001b[32m    681\u001b[39m     model=model,\n\u001b[32m    682\u001b[39m     logits_processor=logits_processor,\n\u001b[32m    683\u001b[39m     stopping_criteria=stopping_criteria,\n\u001b[32m    684\u001b[39m     grammar=grammar,\n\u001b[32m    685\u001b[39m     logit_bias=logit_bias,\n\u001b[32m    686\u001b[39m )\n\u001b[32m    687\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tool \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    688\u001b[39m     tool_name = tool[\u001b[33m\"\u001b[39m\u001b[33mfunction\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/llama/lib/python3.11/site-packages/llama_cpp/llama.py:1835\u001b[39m, in \u001b[36mLlama.create_completion\u001b[39m\u001b[34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[39m\n\u001b[32m   1833\u001b[39m     chunks: Iterator[CreateCompletionStreamResponse] = completion_or_chunks\n\u001b[32m   1834\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m chunks\n\u001b[32m-> \u001b[39m\u001b[32m1835\u001b[39m completion: Completion = \u001b[38;5;28mnext\u001b[39m(completion_or_chunks)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m   1836\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m completion\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/llama/lib/python3.11/site-packages/llama_cpp/llama.py:1320\u001b[39m, in \u001b[36mLlama._create_completion\u001b[39m\u001b[34m(self, prompt, suffix, max_tokens, temperature, top_p, min_p, typical_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, seed, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor, grammar, logit_bias)\u001b[39m\n\u001b[32m   1318\u001b[39m finish_reason = \u001b[33m\"\u001b[39m\u001b[33mlength\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1319\u001b[39m multibyte_fix = \u001b[32m0\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1320\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.generate(\n\u001b[32m   1321\u001b[39m     prompt_tokens,\n\u001b[32m   1322\u001b[39m     top_k=top_k,\n\u001b[32m   1323\u001b[39m     top_p=top_p,\n\u001b[32m   1324\u001b[39m     min_p=min_p,\n\u001b[32m   1325\u001b[39m     typical_p=typical_p,\n\u001b[32m   1326\u001b[39m     temp=temperature,\n\u001b[32m   1327\u001b[39m     tfs_z=tfs_z,\n\u001b[32m   1328\u001b[39m     mirostat_mode=mirostat_mode,\n\u001b[32m   1329\u001b[39m     mirostat_tau=mirostat_tau,\n\u001b[32m   1330\u001b[39m     mirostat_eta=mirostat_eta,\n\u001b[32m   1331\u001b[39m     frequency_penalty=frequency_penalty,\n\u001b[32m   1332\u001b[39m     presence_penalty=presence_penalty,\n\u001b[32m   1333\u001b[39m     repeat_penalty=repeat_penalty,\n\u001b[32m   1334\u001b[39m     stopping_criteria=stopping_criteria,\n\u001b[32m   1335\u001b[39m     logits_processor=logits_processor,\n\u001b[32m   1336\u001b[39m     grammar=grammar,\n\u001b[32m   1337\u001b[39m ):\n\u001b[32m   1338\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m llama_cpp.llama_token_is_eog(\u001b[38;5;28mself\u001b[39m._model.vocab, token):\n\u001b[32m   1339\u001b[39m         text = \u001b[38;5;28mself\u001b[39m.detokenize(completion_tokens, prev_tokens=prompt_tokens)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/llama/lib/python3.11/site-packages/llama_cpp/llama.py:912\u001b[39m, in \u001b[36mLlama.generate\u001b[39m\u001b[34m(self, tokens, top_k, top_p, min_p, typical_p, temp, repeat_penalty, reset, frequency_penalty, presence_penalty, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, penalize_nl, logits_processor, stopping_criteria, grammar)\u001b[39m\n\u001b[32m    910\u001b[39m \u001b[38;5;66;03m# Eval and sample\u001b[39;00m\n\u001b[32m    911\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m912\u001b[39m     \u001b[38;5;28mself\u001b[39m.eval(tokens)\n\u001b[32m    913\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m sample_idx < \u001b[38;5;28mself\u001b[39m.n_tokens:\n\u001b[32m    914\u001b[39m         token = \u001b[38;5;28mself\u001b[39m.sample(\n\u001b[32m    915\u001b[39m             top_k=top_k,\n\u001b[32m    916\u001b[39m             top_p=top_p,\n\u001b[32m   (...)\u001b[39m\u001b[32m    930\u001b[39m             idx=sample_idx,\n\u001b[32m    931\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/llama/lib/python3.11/site-packages/llama_cpp/llama.py:646\u001b[39m, in \u001b[36mLlama.eval\u001b[39m\u001b[34m(self, tokens)\u001b[39m\n\u001b[32m    642\u001b[39m n_tokens = \u001b[38;5;28mlen\u001b[39m(batch)\n\u001b[32m    643\u001b[39m \u001b[38;5;28mself\u001b[39m._batch.set_batch(\n\u001b[32m    644\u001b[39m     batch=batch, n_past=n_past, logits_all=\u001b[38;5;28mself\u001b[39m.context_params.logits_all\n\u001b[32m    645\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m646\u001b[39m \u001b[38;5;28mself\u001b[39m._ctx.decode(\u001b[38;5;28mself\u001b[39m._batch)\n\u001b[32m    647\u001b[39m \u001b[38;5;66;03m# Save tokens\u001b[39;00m\n\u001b[32m    648\u001b[39m \u001b[38;5;28mself\u001b[39m.input_ids[n_past : n_past + n_tokens] = batch\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/anaconda3/envs/llama/lib/python3.11/site-packages/llama_cpp/_internals.py:306\u001b[39m, in \u001b[36mLlamaContext.decode\u001b[39m\u001b[34m(self, batch)\u001b[39m\n\u001b[32m    305\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecode\u001b[39m(\u001b[38;5;28mself\u001b[39m, batch: LlamaBatch):\n\u001b[32m--> \u001b[39m\u001b[32m306\u001b[39m     return_code = llama_cpp.llama_decode(\n\u001b[32m    307\u001b[39m         \u001b[38;5;28mself\u001b[39m.ctx,\n\u001b[32m    308\u001b[39m         batch.batch,\n\u001b[32m    309\u001b[39m     )\n\u001b[32m    310\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m return_code != \u001b[32m0\u001b[39m:\n\u001b[32m    311\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mllama_decode returned \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreturn_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "df_long['yes'] = None\n",
    "df_long['no'] = None\n",
    "df_long['inference_time'] = None\n",
    "for i in df.index:\n",
    "    prompt = generate_fact_prompt_5(df_long.loc[i,\"Date\"],df_long.loc[i,'question'])\n",
    "    st = time.time()\n",
    "    resp = llm.create_chat_completion(\n",
    "    messages=prompt,\n",
    "    max_tokens=500,\n",
    "    temperature=0.0,\n",
    "    logprobs=True,\n",
    "    top_logprobs=10\n",
    "    )\n",
    "    \n",
    "\n",
    "    # response = tokenizer.decode(outputs[:, inputs[\"input_ids\"].shape[1]:][0], skip_special_tokens=True)\n",
    "    \n",
    "\n",
    "    p_yes,p_no,ans = get_yes_no_ans_500(resp)\n",
    "\n",
    "\n",
    "    # print(f\"\\n\\n{prompt[0]['content']}\\n{prompt[1]['content']}\")\n",
    "    # print( {w: float(probs[tokenizer.encode(w, add_special_tokens=False)[0]])\n",
    "    #     for w in [\"Yes\", \"No\"]})\n",
    "    # df.loc[(question, date_str),\"event_date\"] = event_date\n",
    "    df_long.loc[i,'output_content'] = resp['choices'][0]['message']['content']\n",
    "    df_long.loc[i,\"yes\"] = p_yes\n",
    "    df_long.loc[i,\"no\"] = p_no\n",
    "    df_long.loc[i,\"outputAns\"] = ans\n",
    "    df_long.loc[i,'inference_time'] = time.time() - st\n",
    "    time.sleep(1)\n",
    "\n",
    "    print(f\"Question {i} done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>question</th>\n",
       "      <th>Date</th>\n",
       "      <th>yes</th>\n",
       "      <th>no</th>\n",
       "      <th>inference_time</th>\n",
       "      <th>output_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>$ARB above $1.25 on April 7?</td>\n",
       "      <td>March 31, 2023</td>\n",
       "      <td>0.164356</td>\n",
       "      <td>0.812259</td>\n",
       "      <td>22.137652</td>\n",
       "      <td>Factors considered: \\n- ARB (Arbitrage) price ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>$ARB above $1.25 on April 7?</td>\n",
       "      <td>April 30, 2023</td>\n",
       "      <td>0.550298</td>\n",
       "      <td>0.428006</td>\n",
       "      <td>8.454086</td>\n",
       "      <td>Factors considered: \\n- Historical data of the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>$ARB above $1.25 on April 7?</td>\n",
       "      <td>April 30, 2023</td>\n",
       "      <td>0.550079</td>\n",
       "      <td>0.428221</td>\n",
       "      <td>7.844462</td>\n",
       "      <td>Factors considered: \\n- Historical data of the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Another African coup by October 31?</td>\n",
       "      <td>August 31, 2023</td>\n",
       "      <td>0.612945</td>\n",
       "      <td>0.366395</td>\n",
       "      <td>8.83307</td>\n",
       "      <td>Factors considered: \\n- Recent coups in Africa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Another African coup by October 31?</td>\n",
       "      <td>September 30, 2023</td>\n",
       "      <td>0.561227</td>\n",
       "      <td>0.41879</td>\n",
       "      <td>8.653901</td>\n",
       "      <td>Factors considered: \\n- Recent coups in Africa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>Another African coup by October 31?</td>\n",
       "      <td>October 31, 2023</td>\n",
       "      <td>0.599836</td>\n",
       "      <td>0.376797</td>\n",
       "      <td>8.521496</td>\n",
       "      <td>Factors considered: \\n- Recent coups in Africa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>Another African coup by October 31?</td>\n",
       "      <td>October 31, 2023</td>\n",
       "      <td>0.599872</td>\n",
       "      <td>0.376758</td>\n",
       "      <td>8.050637</td>\n",
       "      <td>Factors considered: \\n- Recent coups in Africa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>Arbitrum airdrop by March 31st?</td>\n",
       "      <td>February 28, 2023</td>\n",
       "      <td>0.518543</td>\n",
       "      <td>0.46311</td>\n",
       "      <td>12.539258</td>\n",
       "      <td>Factors considered: \\n- Arbitrum's previous ai...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                             question                Date  \\\n",
       "0           0         $ARB above $1.25 on April 7?      March 31, 2023   \n",
       "1           1         $ARB above $1.25 on April 7?      April 30, 2023   \n",
       "2           2         $ARB above $1.25 on April 7?      April 30, 2023   \n",
       "3           3  Another African coup by October 31?     August 31, 2023   \n",
       "4           4  Another African coup by October 31?  September 30, 2023   \n",
       "5           5  Another African coup by October 31?    October 31, 2023   \n",
       "6           6  Another African coup by October 31?    October 31, 2023   \n",
       "7           7      Arbitrum airdrop by March 31st?   February 28, 2023   \n",
       "\n",
       "        yes        no inference_time  \\\n",
       "0  0.164356  0.812259      22.137652   \n",
       "1  0.550298  0.428006       8.454086   \n",
       "2  0.550079  0.428221       7.844462   \n",
       "3  0.612945  0.366395        8.83307   \n",
       "4  0.561227   0.41879       8.653901   \n",
       "5  0.599836  0.376797       8.521496   \n",
       "6  0.599872  0.376758       8.050637   \n",
       "7  0.518543   0.46311      12.539258   \n",
       "\n",
       "                                      output_content  \n",
       "0  Factors considered: \\n- ARB (Arbitrage) price ...  \n",
       "1  Factors considered: \\n- Historical data of the...  \n",
       "2  Factors considered: \\n- Historical data of the...  \n",
       "3  Factors considered: \\n- Recent coups in Africa...  \n",
       "4  Factors considered: \\n- Recent coups in Africa...  \n",
       "5  Factors considered: \\n- Recent coups in Africa...  \n",
       "6  Factors considered: \\n- Recent coups in Africa...  \n",
       "7  Factors considered: \\n- Arbitrum's previous ai...  "
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_long.to_csv(\"Llama_500_Tokens_3.18B.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i=0\n",
    "# prompt = generate_fact_prompt_5(df.loc[i,\"Date\"],df.loc[i,'question'])\n",
    "# st = time.time()\n",
    "# resp = llm.create_chat_completion(\n",
    "# messages=prompt,\n",
    "# max_tokens=500,\n",
    "# temperature=0.0,\n",
    "# logprobs=True,\n",
    "# top_logprobs=10\n",
    "# )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resp['choices'][0]['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resp['choices'][0]['logprobs']['content'][-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
