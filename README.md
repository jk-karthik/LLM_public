# LLM Experiments: Hugging Face, Llama & Ollama + Polymarket Scraper

A consolidated set of Jupyter notebooks for working with local and hosted LLMs and scraping market data.

**Included notebooks**
- **Hugging_FACE_setup.ipynb** — environment setup and quickstart for Hugging Face.
- **LLama-3B_model.ipynb** — load/run a 3B Llama model for inference/training demos.
- **Quantized_ Llama_3.1-8B.ipynb** — run a quantized Llama (e.g., GGUF/4/8-bit) locally.
- **ollama_test_setup.ipynb** — install and test **Ollama** models locally.
- **PolyMarket_scrapper_v2.ipynb** — scrape/query Polymarket markets for analysis.

> Generated on 2025-08-20.


## Requirements

**Also commonly required (inferred from imports):**
```bash
pip install accelerate matplotlib numpy pandas requests torch transformers
```


## Quick Start

1. **Create an environment (recommended):**
   ```bash
   conda create -n llm-expts python=3.11 -y
   conda activate llm-expts
   ```

2. **Install dependencies:**
   Install the packages under **Requirements** above. For GPU acceleration, make sure you have a compatible PyTorch build (CUDA/ROCm).

3. **Run Jupyter:**
   ```bash
   pip install jupyter
   jupyter lab  # or: jupyter notebook
   ```

4. **Open a notebook and run cells top-to-bottom.**


## Notebook Map

| Notebook | Purpose | Key Sections |
|---|---|---|
| `Hugging_FACE_setup.ipynb` | Set up Hugging Face environment and quickstart for transformers and tokenizers. | Better Way, Notebook: Testing Ollama Functionality, PipeLine Text generation, Forcing answers using chat templates, Complement Question, Prompt Sensitivity |
| `LLama-3B_model.ipynb` | Test and set up Ollama local LLM runtime; run prompts against local models. | Notebook: Testing Ollama Functionality, Fact Checking, Cut of Date for LLama 3.2-3B is Dec 2023, Reasons for no variance in predictions, is basically a mixture of below factors, Fact Checking 2 |
| `ollama_test_setup.ipynb` | Test and set up Ollama local LLM runtime; run prompts against local models. | Notebook: Testing Ollama Functionality, Customizations, num_ctx (not usefull for us) = Sets the size of the context window used to generate the next token, repeat_last_n (Can help) - if the model starts repeating a specific phrase or word within its single response, repeat_last_n will help to penalize that repetition, repeat_penalty - penality above (Sets how strongly to penalize repetitions. A higher value (e.g., 1.5) will penalize repetitions more strongly) |
| `PolyMarket_scrapper_v2.ipynb` | Scrape or query Polymarket markets and structure the data for analysis. | — |
| `Quantized_ Llama_3.1-8B.ipynb` | Run a quantized Llama model (GGUF/4-bit/8-bit) locally for inference. | Notebook: Testing Llama Cpp Functionality |


## Notebook Details

### Better Way
# Better Way
**File:** `Hugging_FACE_setup.ipynb`
**Purpose:** Set up Hugging Face environment and quickstart for transformers and tokenizers.
**Imports observed:** accelerate, torch, torchaudio, torchgen, torchvision, transformers, warnings
**Sections:** Better Way, Notebook: Testing Ollama Functionality, PipeLine Text generation, Forcing answers using chat templates, Complement Question, Prompt Sensitivity, Testing different questions, Use generation to get the Logits, above logits for each generation setp(5 steps above) is conditioned on previous input+ generated tokens

---

### Notebook: Testing Ollama Functionality
# Notebook: Testing Ollama Functionality **Objective.** Generate event outcome probabilities and investigate the market events prediction probabilities generated by the 3.2B Model 
**Key Results.** - Models with less no of parameters (like 3.2B) have issues relating to prompt conditioning caused by Date‑conditioning leak, Instruction drift ,Contradiction. - Test out models with more parameters, quantized versions to confirm use case requirement
**File:** `LLama-3B_model.ipynb`
**Purpose:** Test and set up Ollama local LLM runtime; run prompts against local models.
**Imports observed:** accelerate, datetime, dateutil, matplotlib, numpy, pandas, time, torch, torchaudio, torchgen, torchvision, transformers, warnings
**Sections:** Notebook: Testing Ollama Functionality, Fact Checking, Cut of Date for LLama 3.2-3B is Dec 2023, Reasons for no variance in predictions, is basically a mixture of below factors, Fact Checking 2, Better demo of the issue at Hand, Check 3, All in all the model is not able to answer the questions correctly. Severely limited by model power (no of parameters)

---

### Notebook: Testing Ollama Functionality
# Notebook: Testing Ollama Functionality **Objective.** Look for all customizable features in generative process and conclude if it meets the requirements for the project use case 
**File:** `ollama_test_setup.ipynb`
**Purpose:** Test and set up Ollama local LLM runtime; run prompts against local models.
**Imports observed:** json, langchain, requests, torch, torchaudio, torchgen, torchvision, transformers
**Sections:** Notebook: Testing Ollama Functionality, Customizations, num_ctx (not usefull for us) = Sets the size of the context window used to generate the next token, repeat_last_n (Can help) - if the model starts repeating a specific phrase or word within its single response, repeat_last_n will help to penalize that repetition, repeat_penalty - penality above (Sets how strongly to penalize repetitions. A higher value (e.g., 1.5) will penalize repetitions more strongly), Stopping after a specific token, ### Increasing the temperature will make the model answer more creatively , Kind of similar to setting seed (getting determinstic outputs by forcing the model to always choose the most likely next token), num_predict = Maximum number of tokens to predict when generating text.

---

### PolyMarket_scrapper_v2
**File:** `PolyMarket_scrapper_v2.ipynb`
**Purpose:** Scrape or query Polymarket markets and structure the data for analysis.
**Imports observed:** datetime, json, os, pandas, requests, time

---

### Notebook: Testing Llama Cpp Functionality
# Notebook: Testing Llama Cpp Functionality
**File:** `Quantized_ Llama_3.1-8B.ipynb`
**Purpose:** Run a quantized Llama model (GGUF/4-bit/8-bit) locally for inference.
**Imports observed:** accelerate, datetime, dateutil, llama_cpp, math, matplotlib, numpy, pandas, time, torch, torchgen, warnings
**Sections:** Notebook: Testing Llama Cpp Functionality


## Usage Tips

- **GPU vs CPU:** Llama-based notebooks benefit greatly from a GPU. For CPU-only runs, prefer smaller/quantized models (e.g., 4-bit GGUF).  
- **Model Weights:** Respect model licenses. Some weights may require an access token (e.g., Hugging Face).  
- **Ollama:** Install Ollama and pull models (e.g., `ollama pull llama3`) before running the test notebook.  
- **Polymarket:** Rate limits and schema can change. Cache responses to avoid repeated calls.  
- **Reproducibility:** Set random seeds where available; log package versions (`pip freeze > requirements.txt`).


## Repository Structure

```
.
├── Hugging_FACE_setup.ipynb
├── LLama-3B_model.ipynb
├── Quantized_ Llama_3.1-8B.ipynb
├── ollama_test_setup.ipynb
└── PolyMarket_scrapper_v2.ipynb
```


## License & Attribution

This README was auto-generated from notebook contents. Each model/dataset retains its own license and usage terms. Check provider pages (e.g., Hugging Face model cards, Ollama model pages, Polymarket terms) before use.
