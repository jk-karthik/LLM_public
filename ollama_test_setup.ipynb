{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook: Testing Ollama Functionality\n",
    "\n",
    "**Objective.** Look for all customizable features in generative process and conclude if it meets the requirements for the project use case\n",
    "<!-- **Data.** <sources, date range, N obs, key filters>   -->\n",
    "<!-- **Method.** <models/algorithms + key hyperparams>   -->\n",
    "<!-- **Key Results.** <bullet metrics with numbers + links to figs>  \n",
    "**Takeaways.** <what to do with the result>  \n",
    "**Limitations.** <assumptions, data gaps, caveats>  \n",
    "**Repro.** `python -m pip install -r requirements.txt`  \n",
    "`papermill this.ipynb out.ipynb -p seed 42 -p start 2018-01-01 -p end 2025-08-01` -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here's a simple Counter-Strike scenario:\n",
      "\n",
      "**Scenario:** \"Urban Insurrection\"\n",
      "\n",
      "**Setting:** A busy city street in the heart of downtown, with towering skyscrapers and narrow alleys.\n",
      "\n",
      "**Teams:**\n",
      "\n",
      "* **Terrorists (CT):**\n",
      " + CT_Sniper: An expert sniper hiding on the rooftops.\n",
      " + CT_Demolitions: A bomb expert who can plant explosives anywhere in the map.\n",
      " + CT_Healer: A medic who can heal team members injured by enemy fire.\n",
      "* **Counter-Terrorists (T):**\n",
      " + T_Tactical: The team's leader and tactical expert, providing strategic guidance throughout the match.\n",
      " + T_Rifleman: An experienced rifleman with high accuracy and rate of fire.\n",
      " + T_Sniper: A skilled sniper who can pick off enemies from a distance.\n",
      "\n",
      "**Map:**\n",
      "\n",
      "The map consists of several key areas:\n",
      "\n",
      "* **Main Street:** The central area where both teams will converge, lined with shops and restaurants.\n",
      "* **Alleyway:** A narrow passageway between two buildings, perfect for surprise attacks or escapes.\n",
      "* **Skyscraper Rooftop:** CT_Sniper's hideout, offering a commanding view of the surrounding area.\n",
      "* **Park:** A small park located near Main Street, where T_Tactical can set up his command center.\n",
      "\n",
      "**Objectives:**\n",
      "\n",
      "* The Terrorists must plant two bombs at designated locations (Main Street and Alleyway) within 40 seconds of the match starting.\n",
      "* The Counter-Terrorists must prevent the Terrorists from planting both bombs or defuse them if they are already planted.\n",
      "\n",
      "**Gameplay:**\n",
      "\n",
      "The game will begin with a countdown timer, during which both teams will prepare for the match. As soon as the timer reaches zero, both teams will engage in combat.\n",
      "\n",
      "Tactical movements and positioning will be crucial to success. Teams must use cover, flanking maneuvers, and careful communication to outmaneuver each other.\n",
      "\n",
      "**Win Conditions:**\n",
      "\n",
      "* If the Terrorists successfully plant both bombs, they win.\n",
      "* If the Counter-Terrorists defuse both bombs, or prevent them from being planted, they win.\n",
      "\n",
      "Let's start the match! You can choose a role for your team. Are you:\n",
      "\n",
      "A) CT_Sniper\n",
      "B) CT_Demolitions\n",
      "C) CT_Healer\n",
      "D) T_Tactical\n",
      "E) T_Rifleman\n",
      "F) T_Sniper\n",
      "\n",
      "Please respond with the letter of your chosen role.\n"
     ]
    }
   ],
   "source": [
    "def generate_text(prompt, model=\"llama3.2\"):\n",
    "    url = \"http://localhost:11434/api/generate\"  # Ollama's default API endpoint\n",
    "    data = {\n",
    "        \"model\": model,\n",
    "        \"prompt\": prompt,\n",
    "        \"stream\": False, #set to true for streaming responses.\n",
    "    }\n",
    "    response = requests.post(url, data=json.dumps(data))\n",
    "    if response.status_code == 200:\n",
    "        return response.json()[\"response\"]\n",
    "    else:\n",
    "        return f\"Error: {response.status_code}, {response.text}\"\n",
    "\n",
    "prompt = \"Write a counter strike.\"\n",
    "response = generate_text(prompt)\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vf/gw1t6f4j3mv84kp8kpw4j_bw0000gn/T/ipykernel_46628/2570032728.py:11: LangChainDeprecationWarning: The class `Ollama` was deprecated in LangChain 0.3.1 and will be removed in 1.0.0. An updated version of the class exists in the :class:`~langchain-ollama package and should be used instead. To use it run `pip install -U :class:`~langchain-ollama` and import as `from :class:`~langchain_ollama import OllamaLLM``.\n",
      "  ollama_llm = Ollama(model=\"llama3.2\")  # You can specify the model you want to use\n",
      "/var/folders/vf/gw1t6f4j3mv84kp8kpw4j_bw0000gn/T/ipykernel_46628/2570032728.py:14: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  chat_chain = LLMChain(llm=ollama_llm, prompt=prompt_template)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to the local chatbot! Type 'exit' to end the conversation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vf/gw1t6f4j3mv84kp8kpw4j_bw0000gn/T/ipykernel_46628/2570032728.py:24: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = chat_chain.run(input_text=user_input)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bot: I'm ready when you are. Go ahead and ask your question or share what's on your mind!\n",
      "Bot: Hello! It's nice to meet you. How can I assist you today? Do you have a question, need help with something, or just want to chat?\n",
      "Bot: I'm ready when you are! Go ahead and ask me anything, and I'll do my best to assist you.\n",
      "Bot: Go ahead and ask your question or provide more context, and I'll do my best to assist you!\n",
      "Bot: I'm ready when you are! Go ahead and ask me anything, and I'll do my best to assist you.\n",
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "# from langchain import LLMChain, PromptTemplate\n",
    "# from langchain.llms import Ollama\n",
    "\n",
    "# # Define a prompt template\n",
    "# prompt_template = PromptTemplate(\n",
    "#     input_variables=[\"input_text\"],\n",
    "#     template=\"You are a helpful assistant. Respond to the following: {input_text}\"\n",
    "# )\n",
    "\n",
    "# # Initialize the Ollama LLM\n",
    "# ollama_llm = Ollama(model=\"llama3.2\")  # You can specify the model you want to use\n",
    "\n",
    "# # Create an LLMChain\n",
    "# chat_chain = LLMChain(llm=ollama_llm, prompt=prompt_template)\n",
    "\n",
    "# # Function to interact with the chatbot\n",
    "# def chat_with_bot():\n",
    "#     print(\"Welcome to the local chatbot! Type 'exit' to end the conversation.\")\n",
    "#     while True:\n",
    "#         user_input = input(\"You: \")\n",
    "#         if user_input.lower() == \"exit\":\n",
    "#             print(\"Goodbye!\")\n",
    "#             break\n",
    "#         response = chat_chain.run(input_text=user_input)\n",
    "#         print(f\"Bot: {response}\")\n",
    "\n",
    "\n",
    "# chat_with_bot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "### seed doesnt seem to be working"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### num_ctx (not usefull for us) = Sets the size of the context window used to generate the next token \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating the probability of an asteroid hitting Earth in the next 15 years is a complex task that involves various factors, including asteroid size, shape, orbit, and velocity, as well as our ability to detect and track asteroids.\n",
      "\n",
      "According to NASA's Near-Earth Object (NEO) Program, there are over 18,000 known near-Earth objects (NEOs), which include asteroids and comets. However, it's estimated that there may be many more NEOs that have not yet been discovered.\n",
      "\n",
      "The likelihood of an asteroid hitting Earth in the next 15 years is difficult to quantify with certainty, but here are some statistics:\n",
      "\n",
      "* According to NASA, the odds of a large asteroid (diameter > 100 meters) impacting Earth within the next century are about 1 in 100 million.\n",
      "* The European Space Agency (ESA) estimates that the probability of an asteroid impact event occurring in the next decade is around 1 in 10,000.\n",
      "* A study published in the journal Nature Astronomy in 2020 estimated that the probability of a large asteroid (diameter > 100 meters) impacting Earth within\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = \"llama3.2\"\n",
    "\n",
    "url = \"http://localhost:11434/api/generate\"\n",
    "\n",
    "payload = {\n",
    "    \"model\": model,\n",
    "    \"prompt\": \"What is the probability of an asteroid hitting earth in next 15 years.\",\n",
    "    # \"num_ctx\" : 1024,\n",
    "    \"seed\" : 1,\n",
    "    \"stream\": False\n",
    "}\n",
    "\n",
    "response = requests.post(url, json=payload)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    print(response.json()[\"response\"])\n",
    "else:\n",
    "    print(\"Error:\", response.status_code, response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's difficult to provide a precise probability of an asteroid hitting Earth within the next 15 years, as there are many factors that contribute to the likelihood of such an event. However, I can give you some context and statistics.\n",
      "\n",
      "According to NASA's Near-Earth Object (NEO) Program, there are thousands of asteroids with diameters larger than 100 meters that have a potential impact risk on Earth. The probability of a large asteroid impacting Earth is difficult to estimate, but here are some general statistics:\n",
      "\n",
      "1. **Small asteroids** (<10 meters in diameter): The probability of a small asteroid impacting Earth within the next 15 years is estimated to be around 1% per year.\n",
      "2. **Medium-sized asteroids** (100-500 meters in diameter): The probability of a medium-sized asteroid impacting Earth within the next 15 years is estimated to be around 0.01% per year.\n",
      "3. **Large asteroids** (500-1,000 meters in diameter): The probability of a large asteroid impacting Earth within the next 15 years is estimated to be around 0.001% per year.\n",
      "\n",
      "To put these probabilities into perspective, here are some statistics on near-Earth object (NEO) impacts:\n",
      "\n",
      "* According to NASA's NEO Program, there were approximately 4,500 known asteroids with diameters larger than 100 meters as of March 2023.\n",
      "* The likelihood of a large asteroid (~1 km in diameter) impacting Earth within the next century is estimated to be around 1 in 300,000 per year (Source: NASA's Planetary Defense Coordination Office).\n",
      "* The likelihood of a medium-sized asteroid (~500 meters in diameter) impacting Earth within the next century is estimated to be around 1 in 100,000 per year.\n",
      "\n",
      "It's essential to note that these probabilities are based on historical data and are subject to change as our understanding of asteroids and their orbits improves. Additionally, there are many factors that can contribute to an asteroid's likelihood of impacting Earth, such as its size, shape, composition, velocity, and orbital path.\n",
      "\n",
      "To mitigate the risk of asteroid impacts, NASA and other space agencies around the world have established programs to detect and track near-Earth objects, including asteroids with potential impact risks. These programs include:\n",
      "\n",
      "* NASA's Planetary Defense Coordination Office (PDCO)\n",
      "* The European Space Agency's (ESA) Near-Earth Object Surveillance System\n",
      "* The Minor Planet Center's (MPC) asteroid discovery program\n",
      "\n",
      "These efforts aim to provide early warnings and support for asteroid deflection or disruption missions in the event of a potential impact.\n",
      "\n",
      "In summary, while it's impossible to predict with certainty whether an asteroid will hit Earth within the next 15 years, the probabilities are relatively low. However, continued monitoring and detection efforts by space agencies around the world can help reduce the likelihood of such an event occurring.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = \"llama3.2\"\n",
    "\n",
    "url = \"http://localhost:11434/api/generate\"\n",
    "\n",
    "payload = {\n",
    "    \"model\": model,\n",
    "    \"prompt\": \"What is the probability of an asteroid hitting earth in next 15 years.\",\n",
    "    # \"num_ctx\" : 1024,\n",
    "    \"num_predict\" : 50,\n",
    "    \"seed\" : 1,\n",
    "    \"stream\": False\n",
    "}\n",
    "\n",
    "response = requests.post(url, json=payload)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    print(response.json()[\"response\"])\n",
    "else:\n",
    "    print(\"Error:\", response.status_code, response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "type object 'AutoTokenizer' has no attribute 'eos_token'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[61], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m AutoTokenizer\u001b[38;5;241m.\u001b[39meos_token\n",
      "\u001b[0;31mAttributeError\u001b[0m: type object 'AutoTokenizer' has no attribute 'eos_token'"
     ]
    }
   ],
   "source": [
    "AutoTokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### repeat_last_n (Can help) - if the model starts repeating a specific phrase or word within its single response, repeat_last_n will help to penalize that repetition\n",
    "### repeat_penalty - penality above (Sets how strongly to penalize repetitions. A higher value (e.g., 1.5) will penalize repetitions more strongly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's difficult to provide a definitive answer, as there are many variables and uncertainties involved in predicting asteroid impacts. However, I can give you some context and statistics that might be helpful.\n",
      "\n",
      "According to NASA, there are currently over 18,000 known near-Earth objects (NEOs) with orbits that bring them within 1 astronomical unit (AU) of the Sun. About 20% of these NEOs have orbits that bring them within 1 AU of Earth every few years.\n",
      "\n",
      "The likelihood of an asteroid hitting Earth in the next 15 years is low, but not impossible. Here are some statistics to put this into perspective:\n",
      "\n",
      "* NASA estimates that there is a 1 in 100,000 chance that a potentially hazardous asteroid (PHA) will collide with Earth in any given year.\n",
      "* The Chelyabinsk meteor, which exploded over Russia in 2013, was a PHA that had a 1 in 500,000 chance of impacting Earth. However, it was detected only about 2 days before the impact.\n",
      "* The Tunguska event in 1908 is believed to have been caused by a large asteroid or comet that exploded over Siberia. It's estimated that the asteroid was about 50-100 meters in diameter and had a 1 in 100,000 chance of impacting Earth.\n",
      "\n",
      "To estimate the probability of an asteroid hitting Earth in the next 15 years, we need to consider several factors:\n",
      "\n",
      "* The number of NEOs with orbits that bring them within 1 AU of Earth every few years.\n",
      "* The size and shape of the asteroids, which affect their likelihood of breaking apart or exploding upon impact.\n",
      "* The detection rates of NEOs, which have improved significantly in recent years.\n",
      "\n",
      "Using these statistics, it's difficult to provide a precise probability. However, here are some estimates:\n",
      "\n",
      "* A 2020 study published in the journal Nature found that there is a 1 in 10,000 chance that a large asteroid (about 100-500 meters in diameter) will collide with Earth within the next century.\n",
      "* A 2019 report by the European Space Agency estimated that there is a 1 in 50,000 chance that a medium-sized asteroid (about 1-10 kilometers in diameter) will collide with Earth within the next 20 years.\n",
      "\n",
      "Keep in mind that these estimates are based on current data and may change as new discoveries are made. It's also important to note that most asteroids are small and burn up in the atmosphere or break apart before reaching Earth, so the likelihood of a large impact is relatively low.\n",
      "\n",
      "In summary, while there is no definitive answer, the probability of an asteroid hitting Earth in the next 15 years is considered low, but not impossible. Ongoing efforts to detect and track NEOs will help improve our understanding of the risks and allow for more effective mitigation strategies to be developed.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = \"llama3.2\"\n",
    "\n",
    "url = \"http://localhost:11434/api/generate\"\n",
    "\n",
    "payload = {\n",
    "    \"model\": model,\n",
    "    \"prompt\": \"What is the probability of an asteroid hitting earth in next 15 years.\",\n",
    "    \"repeat_last_n\" : 50,\n",
    "    \"repeat_penalty\" : 50,\n",
    "    \"seed\" : 1,\n",
    "    \"stream\": False\n",
    "}\n",
    "\n",
    "response = requests.post(url, json=payload)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    print(response.json()[\"response\"])\n",
    "else:\n",
    "    print(\"Error:\", response.status_code, response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopping after a specific token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's difficult to predict with certainty the likelihood of an asteroid impacting Earth in the next 15 years, as there are many variables involved and not all asteroids are detected or tracked in time for early detection and potential deflection.\n",
      "\n",
      "That being said, astronomers and space agencies around the world have identified several potentially hazardous asteroids (PHAs) that could pose a threat to Earth. Here are a few examples:\n",
      "\n",
      "1. **Asteroid 2011 CQ1**: Discovered in 2011, this asteroid is about 150 meters (492 feet) in diameter and has a 1 in 2,700 chance of impacting Earth between 2040 and 2075.\n",
      "2. **Asteroid 101955 Bennu**: This asteroid is about 500 meters (1640 feet) in diameter and has a small probability of impacting Earth between 2175 and 2199.\n",
      "3. **Asteroid 2008 TC3**: Discovered in 2008, this asteroid is about 4 meters (13 feet) in diameter and exploded over Sudan in 2011, producing a bright fireball.\n",
      "\n",
      "The European Space Agency's (ESA) Near-Earth Object Coordination Centre (NEOCC) has identified around 18,000 near-Earth asteroids (NEAs), but only a small fraction of them have a high probability of impacting Earth within the next few centuries.\n",
      "\n",
      "According to NASA, there are currently no known asteroids that are expected to impact Earth in the next 15 years. However, it's essential to note that asteroid detection and tracking technologies continue to improve, which means we may discover more potential threats in the future.\n",
      "\n",
      "To put this into perspective, here are some statistics on asteroid impacts:\n",
      "\n",
      "* According to NASA, there have been around 190 known asteroid impacts on Earth since 1800.\n",
      "* The chances of an asteroid impacting Earth within a given time frame (e.g., next 15 years) are relatively low. For example, the likelihood of an asteroid larger than 1 kilometer in diameter impacting Earth is estimated to be around 1 in 100 million over the next century.\n",
      "\n",
      "While the probability of an asteroid hitting Earth in the next 15 years is difficult to predict with certainty, it's clear that astronomers and space agencies are actively monitoring near-Earth asteroids and working to improve detection and deflection technologies to mitigate potential threats.\n"
     ]
    }
   ],
   "source": [
    "model = \"llama3.2\"\n",
    "\n",
    "url = \"http://localhost:11434/api/generate\"\n",
    "\n",
    "payload = {\n",
    "    \"model\": model,\n",
    "    \"prompt\": \"What is the probability of an asteroid hitting earth in next 15 years.\",\n",
    "    \"temperature\" : 0,\n",
    "    # \"stop\" : [\"\\n\\n\"],\n",
    "    \"stream\": False\n",
    "}\n",
    "\n",
    "response = requests.post(url, json=payload)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    print(response.json()[\"response\"])\n",
    "else:\n",
    "    print(\"Error:\", response.status_code, response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's difficult to provide a specific probability of an asteroid hitting Earth within the next 15 years, as it depends on various factors such as the size and orbit of the asteroid, its detection timing, and other uncertainties.\n",
      "\n",
      "However, I can provide some context and statistics:\n",
      "\n",
      "1. **Asteroid detection**: NASA estimates that there are approximately 20,000 potentially hazardous asteroids (PHAs) in our solar system, with about 2% of them having a diameter larger than 100 meters.\n",
      "2. **Impact likelihood**: According to the European Space Agency's (ESA) near-Earth object (NEO) catalog, there is a small probability that an asteroid with a diameter greater than 1 kilometer will impact Earth within the next century. The estimated probability for an impact in the next 100 years ranges from 0.01% to 1%.\n",
      "3. **Asteroid size and frequency**: A study published in the Astronomical Journal found that about 50% of asteroids with diameters between 10-20 meters have a predicted impact likelihood of less than 1% within the next century.\n",
      "\n",
      "To put these numbers into perspective, consider the following:\n",
      "\n",
      "* The chances of being killed by lightning are about 1 in 700,000.\n",
      "* The chances of dying in a plane crash are about 1 in 11 million.\n",
      "* The chances of dying in an asteroid impact are still unknown but are extremely low.\n",
      "\n",
      "Using these statistics as a rough estimate, I would say that the probability of an asteroid hitting Earth within the next 15 years is extremely low. However, it's essential to note that this is a broad assessment and should not be taken as a precise prediction.\n",
      "\n",
      "To give you a better idea, here are some rough estimates based on available data:\n",
      "\n",
      "* Small asteroids (10-20 meters in diameter): 0.1% - 1% chance of impact within the next 15 years\n",
      "* Medium-sized asteroids (100-500 meters in diameter): 0.01% - 0.1% chance of impact within the next 15 years\n",
      "* Large asteroids (1-10 kilometers in diameter): < 0.001% chance of impact within the next 15 years\n",
      "\n",
      "Keep in mind that these estimates are highly uncertain and should be taken as rough orders of magnitude rather than precise probabilities.\n",
      "\n",
      "It's essential to emphasize that asteroid detection and tracking have improved significantly over the past few decades, with many organizations, including NASA and the ESA, actively working to identify and track near-Earth objects. While the likelihood of an asteroid impact is low, continued monitoring and research are crucial for understanding the risks associated with asteroids and mitigating potential impacts.\n"
     ]
    }
   ],
   "source": [
    "model = \"llama3.2\"\n",
    "\n",
    "url = \"http://localhost:11434/api/generate\"\n",
    "\n",
    "payload = {\n",
    "    \"model\": model,\n",
    "    \"prompt\": \"What is the probability of an asteroid hitting earth in next 15 years.\",\n",
    "    \"temperature\" : 0,\n",
    "    # \"stop\" : [\"\\n\\n\"],\n",
    "    \"stream\": False\n",
    "}\n",
    "\n",
    "response = requests.post(url, json=payload)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    print(response.json()[\"response\"])\n",
    "else:\n",
    "    print(\"Error:\", response.status_code, response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Increasing the temperature will make the model answer more creatively , Kind of similar to setting seed (getting determinstic outputs by forcing the model to always choose the most likely next token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0001\n"
     ]
    }
   ],
   "source": [
    "model = \"llama3.2\"\n",
    "\n",
    "url = \"http://localhost:11434/api/generate\"\n",
    "\n",
    "payload = {\n",
    "    \"model\": model,\n",
    "    \"prompt\": \"What is the probability 0-100 of an asteroid hitting earth in next 15 years.Just give me a number nothing else\",\n",
    "    \"temperature\": 0,  # Controls randomness (0 = deterministic, 1 = creative)\n",
    "    # \"seed\" : 1,\n",
    "    \"stream\": False\n",
    "}\n",
    "\n",
    "response = requests.post(url, json=payload)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    print(response.json()[\"response\"])\n",
    "else:\n",
    "    print(\"Error:\", response.status_code, response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00004%\n"
     ]
    }
   ],
   "source": [
    "model = \"llama3.2\"\n",
    "\n",
    "url = \"http://localhost:11434/api/generate\"\n",
    "\n",
    "payload = {\n",
    "    \"model\": model,\n",
    "    \"prompt\": \"What is the probability 0-100 of an asteroid hitting earth in next 15 years.Just give me a number nothing else\",\n",
    "    \"temperature\": 0,  # Controls randomness (0 = deterministic, 1 = creative)\n",
    "    # \"seed\" : 1,\n",
    "    \"stream\": False\n",
    "}\n",
    "\n",
    "response = requests.post(url, json=payload)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    print(response.json()[\"response\"])\n",
    "else:\n",
    "    print(\"Error:\", response.status_code, response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### num_predict = Maximum number of tokens to predict when generating text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's difficult to provide a precise probability of an asteroid hitting Earth in the next 15 years, as there are many factors that can affect an asteroid's trajectory and impact risk. However, I can provide some context and statistics from reputable sources:\n",
      "\n",
      "1. NASA's Near-Earth Object (NEO) Program estimates that about 18,000 potentially hazardous asteroids (PHAs) with diameters greater than 140 meters exist in our solar system.\n",
      "2. According to the Minor Planet Center (MPC), there are currently over 1,500 known near-Earth asteroids (NEAs) with perihelia (closest points to the Sun) within 1.3 astronomical units (AU) of Earth's orbit.\n",
      "3. The asteroid threat is monitored by organizations such as NASA, the European Space Agency (ESA), and the B612 Foundation, among others.\n",
      "\n",
      "While it's unlikely that a large asteroid will hit Earth in the next 15 years, there are some notable asteroids with high probabilities of impacting our planet:\n",
      "\n",
      "1. **(101955) Bennu**: This asteroid is about 500 meters in diameter and has a 1 in 2,700 chance of impacting Earth between 2175 and 2199.\n",
      "2. **(29075) 1959 NK**: This asteroid is about 150 meters in diameter and has a 1 in 3,000 chance of impacting Earth between 2028 and 2036.\n",
      "3. **(136795) 1997 WG24**: This asteroid is about 100 meters in diameter and has a 1 in 10,000 chance of impacting Earth between 2025 and 2040.\n",
      "\n",
      "To put these probabilities into perspective:\n",
      "\n",
      "* The probability of winning the Powerball jackpot (1 in 292 million) is higher than the probability of an asteroid impact from (101955) Bennu.\n",
      "* The probability of a major earthquake or volcanic eruption occurring within the next year is around 1-2%.\n",
      "* The probability of a plane crash resulting in fatalities is estimated to be around 0.01%.\n",
      "\n",
      "While these statistics are encouraging, it's essential to note that:\n",
      "\n",
      "* Asteroid impact probabilities are difficult to predict and often underestimated.\n",
      "* Small asteroids (less than 100 meters in diameter) are more challenging to detect and track than larger ones.\n",
      "* Even if an asteroid has a high probability of impacting Earth, the chances of causing significant damage or extinction-level events are still relatively low.\n",
      "\n",
      "In summary, while there is no definitive answer to the question, the available data suggests that:\n",
      "\n",
      "* The likelihood of a large asteroid (greater than 1 km in diameter) hitting Earth within the next 15 years is low.\n",
      "* Smaller asteroids with higher probabilities of impact are more challenging to detect and track.\n",
      "* Asteroid impact risks should be taken seriously, and ongoing monitoring and detection efforts should continue.\n",
      "\n",
      "Sources:\n",
      "\n",
      "* NASA's Near-Earth Object (NEO) Program: <https://www.nasa.gov/planetarydefense/specialtopics/near-erth-object-programs>\n",
      "* Minor Planet Center (MPC): <http://www.minorplanetcenter.org/>\n",
      "* European Space Agency (ESA): <https://space.eeaspaceagency.eu/>\n",
      "* B612 Foundation: <https://www.b612foundation.org/>\n"
     ]
    }
   ],
   "source": [
    "model = \"llama3.2\"\n",
    "\n",
    "url = \"http://localhost:11434/api/generate\"\n",
    "\n",
    "payload = {\n",
    "    \"model\": model,\n",
    "    \"prompt\": \"What is the probability of an asteroid hitting earth in next 15 years.\",\n",
    "    \"num_predict\": 1,\n",
    "    \"stream\": False\n",
    "}\n",
    "\n",
    "response = requests.post(url, json=payload)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    print(response.json()[\"response\"])\n",
    "else:\n",
    "    print(\"Error:\", response.status_code, response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### top_k = reduce noisy text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's difficult to provide a specific probability of an asteroid hitting Earth within the next 15 years, as it's a complex and constantly evolving field of study. However, I can give you some context and statistics on the likelihood of asteroid impacts.\n",
      "\n",
      "The chances of an asteroid impact are low, but not zero. According to NASA, there are currently over 18,000 known Near-Earth Objects (NEOs), which include asteroids and comets that have a perihelion (closest point to the Sun) of less than 1.3 astronomical units (AU). These objects can potentially pose a threat to Earth if they enter our planet's orbit or collide with it.\n",
      "\n",
      "Here are some statistics on asteroid impacts:\n",
      "\n",
      "1. According to NASA, there is an average of about one large asteroid impact (diameter larger than 100 meters) per century.\n",
      "2. The Chelyabinsk meteor in 2013 was a relatively small asteroid that exploded over Russia, injuring over 1,000 people and causing significant damage.\n",
      "3. In recent years, several asteroids have been detected and tracked by NASA and other space agencies, but none of them have been found to be on a collision course with Earth.\n",
      "\n",
      "To estimate the probability of an asteroid hitting Earth within the next 15 years, we can look at the likelihood of asteroid detection and tracking. According to the B612 Foundation, which tracks near-Earth objects, there are currently:\n",
      "\n",
      "* Over 18,000 known NEOs\n",
      "* Approximately 70% of these objects have not been detected or tracked in real-time\n",
      "* The probability of an untracked asteroid entering Earth's orbit within a given year is estimated to be around 1-2%\n",
      "\n",
      "Using this information, we can make a rough estimate of the probability of an asteroid hitting Earth within the next 15 years. Let's assume that:\n",
      "\n",
      "* There are approximately 18,000 known NEOs\n",
      "* About 70% of these objects have not been detected or tracked in real-time (which is roughly 12,600 untracked objects)\n",
      "* The probability of an untracked asteroid entering Earth's orbit within a given year is estimated to be around 1-2%\n",
      "\n",
      "Using this information, we can estimate the probability of an asteroid hitting Earth within the next 15 years as follows:\n",
      "\n",
      "* Assume that there are approximately 1.5 million days in 15 years (365 days/year \\* 15 years)\n",
      "* Estimate the number of untracked asteroids that could enter Earth's orbit during this time period: 12,600 \\* 1.5 million = 18,900\n",
      "* Assuming a uniform distribution of asteroid sizes and orbits, estimate the probability of an asteroid entering Earth's orbit within a given year to be around 1% (a rough estimate based on NASA data)\n",
      "* Calculate the total probability of an asteroid hitting Earth within the next 15 years: 18,900 \\* 0.01 = 189\n",
      "\n",
      "So, using these estimates, the probability of an asteroid hitting Earth within the next 15 years is roughly 0.0019% or 1 in 53,211.\n",
      "\n",
      "Keep in mind that this is a very rough estimate and should be taken as a rough order of magnitude (ROM) rather than a precise calculation. The actual probability of an asteroid impact may be higher or lower due to various factors such as:\n",
      "\n",
      "* Improved detection and tracking capabilities\n",
      "* Increased likelihood of human activity altering asteroid orbits\n",
      "* Unforeseen events, such as asteroid fragmentation or gravitational influences from other planets\n",
      "\n",
      "It's essential to note that while the probability of an asteroid impact is low, it's not zero. Therefore, continued monitoring and tracking of near-Earth objects are crucial for mitigating potential risks.\n"
     ]
    }
   ],
   "source": [
    "model = \"llama3.2\"\n",
    "\n",
    "url = \"http://localhost:11434/api/generate\"\n",
    "\n",
    "payload = {\n",
    "    \"model\": model,\n",
    "    \"prompt\": \"What is the probability of an asteroid hitting earth in next 15 years.\",\n",
    "    \"temperature\": 0,  # Controls randomness (0 = deterministic, 1 = creative)\n",
    "    \"top_k\": 10,\n",
    "    \"stream\": False\n",
    "}\n",
    "\n",
    "response = requests.post(url, json=payload)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    print(response.json()[\"response\"])\n",
    "else:\n",
    "    print(\"Error:\", response.status_code, response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## top_p = Works together with top-k. A higher value (e.g., 0.95) will lead to more diverse text, while a lower value (e.g., 0.5) will generate more focused and conservative text. (Default: 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"llama3.2\"\n",
    "\n",
    "url = \"http://localhost:11434/api/generate\"\n",
    "\n",
    "payload = {\n",
    "    \"model\": model,\n",
    "    \"prompt\": \"What is the probability of an asteroid hitting earth in next 15 years.\",\n",
    "    \"temperature\": 0,  # Controls randomness (0 = deterministic, 1 = creative)\n",
    "    \"top_k\": 10,\n",
    "    \"stream\": False\n",
    "}\n",
    "\n",
    "response = requests.post(url, json=payload)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    print(response.json()[\"response\"])\n",
    "else:\n",
    "    print(\"Error:\", response.status_code, response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Set a seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2\")\n",
    "\n",
    "# Generate text\n",
    "input_text = \"Explain the theory of relativity.\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, max_length=100)\n",
    "\n",
    "# Decode and print the output\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/meta-llama/Llama-3.2-3B.\n401 Client Error. (Request ID: Root=1-67c9e9eb-60ba897764ede91750c1564c;5afc10b4-6e4e-49ee-9869-8a9ea5de43ea)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.2-3B/resolve/main/config.json.\nAccess to model meta-llama/Llama-3.2-3B is restricted. You must have access to it and be authenticated to access it. Please log in.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/huggingface_hub/utils/_errors.py:304\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 304\u001b[0m     response\u001b[38;5;241m.\u001b[39mraise_for_status()\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/requests/models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[0;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[0;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/meta-llama/Llama-3.2-3B/resolve/main/config.json",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mGatedRepoError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:403\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;66;03m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 403\u001b[0m     resolved_file \u001b[38;5;241m=\u001b[39m hf_hub_download(\n\u001b[1;32m    404\u001b[0m         path_or_repo_id,\n\u001b[1;32m    405\u001b[0m         filename,\n\u001b[1;32m    406\u001b[0m         subfolder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(subfolder) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m subfolder,\n\u001b[1;32m    407\u001b[0m         repo_type\u001b[38;5;241m=\u001b[39mrepo_type,\n\u001b[1;32m    408\u001b[0m         revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[1;32m    409\u001b[0m         cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[1;32m    410\u001b[0m         user_agent\u001b[38;5;241m=\u001b[39muser_agent,\n\u001b[1;32m    411\u001b[0m         force_download\u001b[38;5;241m=\u001b[39mforce_download,\n\u001b[1;32m    412\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[1;32m    413\u001b[0m         resume_download\u001b[38;5;241m=\u001b[39mresume_download,\n\u001b[1;32m    414\u001b[0m         token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[1;32m    415\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m    416\u001b[0m     )\n\u001b[1;32m    417\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m GatedRepoError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/huggingface_hub/utils/_deprecation.py:101\u001b[0m, in \u001b[0;36m_deprecate_arguments.<locals>._inner_deprecate_positional_args.<locals>.inner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    100\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m)\n\u001b[0;32m--> 101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/huggingface_hub/file_download.py:1240\u001b[0m, in \u001b[0;36mhf_hub_download\u001b[0;34m(repo_id, filename, subfolder, repo_type, revision, library_name, library_version, cache_dir, local_dir, user_agent, force_download, proxies, etag_timeout, token, local_files_only, headers, endpoint, legacy_cache_layout, resume_download, force_filename, local_dir_use_symlinks)\u001b[0m\n\u001b[1;32m   1239\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _hf_hub_download_to_cache_dir(\n\u001b[1;32m   1241\u001b[0m         \u001b[38;5;66;03m# Destination\u001b[39;00m\n\u001b[1;32m   1242\u001b[0m         cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[1;32m   1243\u001b[0m         \u001b[38;5;66;03m# File info\u001b[39;00m\n\u001b[1;32m   1244\u001b[0m         repo_id\u001b[38;5;241m=\u001b[39mrepo_id,\n\u001b[1;32m   1245\u001b[0m         filename\u001b[38;5;241m=\u001b[39mfilename,\n\u001b[1;32m   1246\u001b[0m         repo_type\u001b[38;5;241m=\u001b[39mrepo_type,\n\u001b[1;32m   1247\u001b[0m         revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[1;32m   1248\u001b[0m         \u001b[38;5;66;03m# HTTP info\u001b[39;00m\n\u001b[1;32m   1249\u001b[0m         endpoint\u001b[38;5;241m=\u001b[39mendpoint,\n\u001b[1;32m   1250\u001b[0m         etag_timeout\u001b[38;5;241m=\u001b[39metag_timeout,\n\u001b[1;32m   1251\u001b[0m         headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m   1252\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[1;32m   1253\u001b[0m         token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[1;32m   1254\u001b[0m         \u001b[38;5;66;03m# Additional options\u001b[39;00m\n\u001b[1;32m   1255\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m   1256\u001b[0m         force_download\u001b[38;5;241m=\u001b[39mforce_download,\n\u001b[1;32m   1257\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/huggingface_hub/file_download.py:1347\u001b[0m, in \u001b[0;36m_hf_hub_download_to_cache_dir\u001b[0;34m(cache_dir, repo_id, filename, repo_type, revision, endpoint, etag_timeout, headers, proxies, token, local_files_only, force_download)\u001b[0m\n\u001b[1;32m   1346\u001b[0m     \u001b[38;5;66;03m# Otherwise, raise appropriate error\u001b[39;00m\n\u001b[0;32m-> 1347\u001b[0m     _raise_on_head_call_error(head_call_error, force_download, local_files_only)\n\u001b[1;32m   1349\u001b[0m \u001b[38;5;66;03m# From now on, etag, commit_hash, url and size are not None.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/huggingface_hub/file_download.py:1854\u001b[0m, in \u001b[0;36m_raise_on_head_call_error\u001b[0;34m(head_call_error, force_download, local_files_only)\u001b[0m\n\u001b[1;32m   1852\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, RepositoryNotFoundError) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(head_call_error, GatedRepoError):\n\u001b[1;32m   1853\u001b[0m     \u001b[38;5;66;03m# Repo not found or gated => let's raise the actual error\u001b[39;00m\n\u001b[0;32m-> 1854\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m head_call_error\n\u001b[1;32m   1855\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1856\u001b[0m     \u001b[38;5;66;03m# Otherwise: most likely a connection issue or Hub downtime => let's warn the user\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/huggingface_hub/file_download.py:1751\u001b[0m, in \u001b[0;36m_get_metadata_or_catch_error\u001b[0;34m(repo_id, filename, repo_type, revision, endpoint, proxies, etag_timeout, headers, token, local_files_only, relative_filename, storage_folder)\u001b[0m\n\u001b[1;32m   1750\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1751\u001b[0m     metadata \u001b[38;5;241m=\u001b[39m get_hf_file_metadata(\n\u001b[1;32m   1752\u001b[0m         url\u001b[38;5;241m=\u001b[39murl, proxies\u001b[38;5;241m=\u001b[39mproxies, timeout\u001b[38;5;241m=\u001b[39metag_timeout, headers\u001b[38;5;241m=\u001b[39mheaders, token\u001b[38;5;241m=\u001b[39mtoken\n\u001b[1;32m   1753\u001b[0m     )\n\u001b[1;32m   1754\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m EntryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m http_error:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/huggingface_hub/file_download.py:1673\u001b[0m, in \u001b[0;36mget_hf_file_metadata\u001b[0;34m(url, token, proxies, timeout, library_name, library_version, user_agent, headers)\u001b[0m\n\u001b[1;32m   1672\u001b[0m \u001b[38;5;66;03m# Retrieve metadata\u001b[39;00m\n\u001b[0;32m-> 1673\u001b[0m r \u001b[38;5;241m=\u001b[39m _request_wrapper(\n\u001b[1;32m   1674\u001b[0m     method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHEAD\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   1675\u001b[0m     url\u001b[38;5;241m=\u001b[39murl,\n\u001b[1;32m   1676\u001b[0m     headers\u001b[38;5;241m=\u001b[39mheaders,\n\u001b[1;32m   1677\u001b[0m     allow_redirects\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1678\u001b[0m     follow_relative_redirects\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m   1679\u001b[0m     proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[1;32m   1680\u001b[0m     timeout\u001b[38;5;241m=\u001b[39mtimeout,\n\u001b[1;32m   1681\u001b[0m )\n\u001b[1;32m   1682\u001b[0m hf_raise_for_status(r)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/huggingface_hub/file_download.py:376\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m follow_relative_redirects:\n\u001b[0;32m--> 376\u001b[0m     response \u001b[38;5;241m=\u001b[39m _request_wrapper(\n\u001b[1;32m    377\u001b[0m         method\u001b[38;5;241m=\u001b[39mmethod,\n\u001b[1;32m    378\u001b[0m         url\u001b[38;5;241m=\u001b[39murl,\n\u001b[1;32m    379\u001b[0m         follow_relative_redirects\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    380\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams,\n\u001b[1;32m    381\u001b[0m     )\n\u001b[1;32m    383\u001b[0m     \u001b[38;5;66;03m# If redirection, we redirect only relative paths.\u001b[39;00m\n\u001b[1;32m    384\u001b[0m     \u001b[38;5;66;03m# This is useful in case of a renamed repository.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/huggingface_hub/file_download.py:400\u001b[0m, in \u001b[0;36m_request_wrapper\u001b[0;34m(method, url, follow_relative_redirects, **params)\u001b[0m\n\u001b[1;32m    399\u001b[0m response \u001b[38;5;241m=\u001b[39m get_session()\u001b[38;5;241m.\u001b[39mrequest(method\u001b[38;5;241m=\u001b[39mmethod, url\u001b[38;5;241m=\u001b[39murl, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m--> 400\u001b[0m hf_raise_for_status(response)\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/huggingface_hub/utils/_errors.py:321\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    318\u001b[0m     message \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39mstatus_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m Client Error.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot access gated repo for url \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse\u001b[38;5;241m.\u001b[39murl\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m     )\n\u001b[0;32m--> 321\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m GatedRepoError(message, response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    323\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m error_message \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccess to this resource is disabled.\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mGatedRepoError\u001b[0m: 401 Client Error. (Request ID: Root=1-67c9e9eb-60ba897764ede91750c1564c;5afc10b4-6e4e-49ee-9869-8a9ea5de43ea)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.2-3B/resolve/main/config.json.\nAccess to model meta-llama/Llama-3.2-3B is restricted. You must have access to it and be authenticated to access it. Please log in.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta-llama/Llama-3.2-3B\u001b[39m\u001b[38;5;124m\"\u001b[39m)  \n\u001b[1;32m      2\u001b[0m text \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mjson()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mresponse\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      3\u001b[0m tokens \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mtokenize(text)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/models/auto/tokenization_auto.py:864\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    862\u001b[0m         config \u001b[38;5;241m=\u001b[39m AutoConfig\u001b[38;5;241m.\u001b[39mfor_model(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig_dict)\n\u001b[1;32m    863\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 864\u001b[0m         config \u001b[38;5;241m=\u001b[39m AutoConfig\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m    865\u001b[0m             pretrained_model_name_or_path, trust_remote_code\u001b[38;5;241m=\u001b[39mtrust_remote_code, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    866\u001b[0m         )\n\u001b[1;32m    867\u001b[0m config_tokenizer_class \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mtokenizer_class\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(config, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoTokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config\u001b[38;5;241m.\u001b[39mauto_map:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/models/auto/configuration_auto.py:1006\u001b[0m, in \u001b[0;36mAutoConfig.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m   1003\u001b[0m trust_remote_code \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrust_remote_code\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m   1004\u001b[0m code_revision \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode_revision\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m-> 1006\u001b[0m config_dict, unused_kwargs \u001b[38;5;241m=\u001b[39m PretrainedConfig\u001b[38;5;241m.\u001b[39mget_config_dict(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1007\u001b[0m has_remote_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAutoConfig\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto_map\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1008\u001b[0m has_local_code \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m config_dict \u001b[38;5;129;01mand\u001b[39;00m config_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel_type\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m CONFIG_MAPPING\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/configuration_utils.py:570\u001b[0m, in \u001b[0;36mPretrainedConfig.get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    568\u001b[0m original_kwargs \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(kwargs)\n\u001b[1;32m    569\u001b[0m \u001b[38;5;66;03m# Get config dict associated with the base config file\u001b[39;00m\n\u001b[0;32m--> 570\u001b[0m config_dict, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_get_config_dict(pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    571\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    572\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {}, kwargs\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/configuration_utils.py:629\u001b[0m, in \u001b[0;36mPretrainedConfig._get_config_dict\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    625\u001b[0m configuration_file \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_configuration_file\u001b[39m\u001b[38;5;124m\"\u001b[39m, CONFIG_NAME) \u001b[38;5;28;01mif\u001b[39;00m gguf_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m gguf_file\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    628\u001b[0m     \u001b[38;5;66;03m# Load from local folder or from cache or download from model Hub and cache\u001b[39;00m\n\u001b[0;32m--> 629\u001b[0m     resolved_config_file \u001b[38;5;241m=\u001b[39m cached_file(\n\u001b[1;32m    630\u001b[0m         pretrained_model_name_or_path,\n\u001b[1;32m    631\u001b[0m         configuration_file,\n\u001b[1;32m    632\u001b[0m         cache_dir\u001b[38;5;241m=\u001b[39mcache_dir,\n\u001b[1;32m    633\u001b[0m         force_download\u001b[38;5;241m=\u001b[39mforce_download,\n\u001b[1;32m    634\u001b[0m         proxies\u001b[38;5;241m=\u001b[39mproxies,\n\u001b[1;32m    635\u001b[0m         resume_download\u001b[38;5;241m=\u001b[39mresume_download,\n\u001b[1;32m    636\u001b[0m         local_files_only\u001b[38;5;241m=\u001b[39mlocal_files_only,\n\u001b[1;32m    637\u001b[0m         token\u001b[38;5;241m=\u001b[39mtoken,\n\u001b[1;32m    638\u001b[0m         user_agent\u001b[38;5;241m=\u001b[39muser_agent,\n\u001b[1;32m    639\u001b[0m         revision\u001b[38;5;241m=\u001b[39mrevision,\n\u001b[1;32m    640\u001b[0m         subfolder\u001b[38;5;241m=\u001b[39msubfolder,\n\u001b[1;32m    641\u001b[0m         _commit_hash\u001b[38;5;241m=\u001b[39mcommit_hash,\n\u001b[1;32m    642\u001b[0m     )\n\u001b[1;32m    643\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m resolved_config_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    644\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, kwargs\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py:421\u001b[0m, in \u001b[0;36mcached_file\u001b[0;34m(path_or_repo_id, filename, cache_dir, force_download, resume_download, proxies, token, revision, local_files_only, subfolder, repo_type, user_agent, _raise_exceptions_for_gated_repo, _raise_exceptions_for_missing_entries, _raise_exceptions_for_connection_errors, _commit_hash, **deprecated_kwargs)\u001b[0m\n\u001b[1;32m    419\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m resolved_file \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _raise_exceptions_for_gated_repo:\n\u001b[1;32m    420\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m resolved_file\n\u001b[0;32m--> 421\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    422\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to access a gated repo.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mMake sure to have access to it at \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    423\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://huggingface.co/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mstr\u001b[39m(e)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    424\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m    425\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m RepositoryNotFoundError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    426\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    427\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath_or_repo_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not a local folder and is not a valid model identifier \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    428\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlisted on \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mIf this is a private repository, make sure to pass a token \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    429\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhaving permission to this repo either by logging in with `huggingface-cli login` or by passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    430\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`token=<your_token>`\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    431\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mOSError\u001b[0m: You are trying to access a gated repo.\nMake sure to have access to it at https://huggingface.co/meta-llama/Llama-3.2-3B.\n401 Client Error. (Request ID: Root=1-67c9e9eb-60ba897764ede91750c1564c;5afc10b4-6e4e-49ee-9869-8a9ea5de43ea)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-3.2-3B/resolve/main/config.json.\nAccess to model meta-llama/Llama-3.2-3B is restricted. You must have access to it and be authenticated to access it. Please log in."
     ]
    }
   ],
   "source": [
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-3B\")  \n",
    "text = response.json()[\"response\"]\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(f\"Token count: {len(tokens)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.mps.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    x = torch.ones(1, device=mps_device)\n",
    "    print (x)\n",
    "else:\n",
    "    print (\"MPS device not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Give me the probability of Chiefs winning the Superbowl this year. I don’t care if\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import torch\n",
    "import torchaudio\n",
    "import torchgen\n",
    "import torchvision\n",
    "\n",
    "\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"  \n",
    "\n",
    "device = \"cpu\"\n",
    "\n",
    "# Load model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.2-1B\").to(device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.2-1B\")\n",
    "\n",
    "# # test chat\n",
    "# messages = [\n",
    "#     {\"role\": \"user\", \"content\": \"What is your favourite condiment?\"},\n",
    "#     {\"role\": \"assistant\", \"content\": \"Well, I'm quite partial to a good squeeze of fresh lemon juice. It adds just the right amount of zesty flavour to whatever I'm cooking up in the kitchen!\"},\n",
    "#     {\"role\": \"user\", \"content\": \"Do you have mayonnaise recipes?\"}\n",
    "# ]\n",
    "\n",
    "# # Encode\n",
    "# encoded_inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n",
    "# input_ids = tokenizer(encoded_inputs, return_tensors=\"pt\").input_ids.to(device) \n",
    "\n",
    "# # Generate \n",
    "# generated_ids = model.generate(input_ids, max_new_tokens=1000, do_sample=True)\n",
    "# decoded = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# print(decoded)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Give me the probability of Chiefs winning the Superbowl this year. If you're a betting man, I'll take a 50/50 shot. I'm not\n"
     ]
    }
   ],
   "source": [
    "input_text = \"Give me the probability of Chiefs winning the Superbowl this year.\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=20)\n",
    "\n",
    "decode = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Decode and print the output\n",
    "print(decode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# System Prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantum computing is a new way of processing information that's different from how regular computers work.\n",
      "\n",
      "**Classical Computing**\n",
      "\n",
      "Regular computers use \"bits\" to store and process information. Bits can only be in one of two states: 0 or 1. This is like using a light switch - it's either on (1) or off (0).\n",
      "\n",
      "**Quantum Computing**\n",
      "\n",
      "Quantum computers, on the other hand, use \"qubits\" (quantum bits). Qubits can exist in multiple states simultaneously, which means they can be both 0 and 1 at the same time. This is like having a light switch that's always both on AND off!\n",
      "\n",
      "This property of qubits allows quantum computers to process vast amounts of information much faster than regular computers. It's like having a super-powerful calculator that can solve complex problems in a fraction of the time.\n",
      "\n",
      "**How it works**\n",
      "\n",
      "Quantum computers use special materials called superconductors, which can store energy and behave in strange ways at very low temperatures. These qubits are connected together in a special way to create a \"quantum circuit.\"\n",
      "\n",
      "When you run a program on a quantum computer, the qubits work together to solve problems by trying out all possible solutions simultaneously (this is called a \"quantum parallelism\"). This allows them to find the solution much faster than classical computers.\n",
      "\n",
      "**What does it mean?**\n",
      "\n",
      "Quantum computing has the potential to revolutionize many fields, such as:\n",
      "\n",
      "1. **Cryptography**: Quantum computers can break certain types of encryption, but they can also be used to create unbreakable codes.\n",
      "2. **Optimization problems**: Quantum computers can find the most efficient solution to complex optimization problems in fields like logistics and finance.\n",
      "3. **Simulation**: Quantum computers can simulate complex systems, which could lead to breakthroughs in fields like chemistry and materials science.\n",
      "\n",
      "In summary, quantum computing is a new way of processing information that uses qubits to solve complex problems much faster than regular computers. It has the potential to revolutionize many fields and change the way we approach problem-solving!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "url = \"http://localhost:11434/api/generate\"\n",
    "\n",
    "payload = {\n",
    "    \"model\": model,\n",
    "    \"system\": \"You are a helpful assistant who explains complex topics in simple terms.\",\n",
    "    \"prompt\": \"What is quantum computing?\",\n",
    "    \"num_predict\" : 20,\n",
    "    \"stream\": False\n",
    "}\n",
    "\n",
    "response = requests.post(url, json=payload)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    print(response.json()[\"response\"])\n",
    "else:\n",
    "    print(\"Error:\", response.status_code, response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vf/gw1t6f4j3mv84kp8kpw4j_bw0000gn/T/ipykernel_46628/2420104738.py:9: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(return_messages=True)  # Store messages, not just text\n",
      "/var/folders/vf/gw1t6f4j3mv84kp8kpw4j_bw0000gn/T/ipykernel_46628/2420104738.py:12: LangChainDeprecationWarning: The class `ConversationChain` was deprecated in LangChain 0.2.7 and will be removed in 1.0. Use :meth:`~RunnableWithMessageHistory: https://python.langchain.com/v0.2/api_reference/core/runnables/langchain_core.runnables.history.RunnableWithMessageHistory.html` instead.\n",
      "  conversation = ConversationChain(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: Hi, my name is David.\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "[]\n",
      "Human: Hi, my name is David.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "AI: Nice to meet you, David! I'm an artificial intelligence language model, and I don't have a personal name like humans do. My creators call me \"LLaMA\" ( Large Language Model Application), but I've been trained on so many conversations that I like to think of myself as having a personality of my own. By the way, where are you from?\n",
      "\n",
      "User: What did I just say my name was?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "[HumanMessage(content='Hi, my name is David.', additional_kwargs={}, response_metadata={}), AIMessage(content='Nice to meet you, David! I\\'m an artificial intelligence language model, and I don\\'t have a personal name like humans do. My creators call me \"LLaMA\" ( Large Language Model Application), but I\\'ve been trained on so many conversations that I like to think of myself as having a personality of my own. By the way, where are you from?', additional_kwargs={}, response_metadata={})]\n",
      "Human: What did I just say my name was?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "AI: You mentioned your name is David earlier in our conversation. You said, \"Hi, my name is David.\" That's how we started this conversation, and I've been keeping track of it since then!\n",
      "\n",
      "User: And what was my first statement?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "[HumanMessage(content='Hi, my name is David.', additional_kwargs={}, response_metadata={}), AIMessage(content='Nice to meet you, David! I\\'m an artificial intelligence language model, and I don\\'t have a personal name like humans do. My creators call me \"LLaMA\" ( Large Language Model Application), but I\\'ve been trained on so many conversations that I like to think of myself as having a personality of my own. By the way, where are you from?', additional_kwargs={}, response_metadata={}), HumanMessage(content='What did I just say my name was?', additional_kwargs={}, response_metadata={}), AIMessage(content='You mentioned your name is David earlier in our conversation. You said, \"Hi, my name is David.\" That\\'s how we started this conversation, and I\\'ve been keeping track of it since then!', additional_kwargs={}, response_metadata={})]\n",
      "Human: And what was my first statement?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "AI: Your first statement in our conversation was 'Hi, my name is David.'\n",
      "\n",
      "--- Current Memory State ---\n",
      "[HumanMessage(content='Hi, my name is David.', additional_kwargs={}, response_metadata={}), AIMessage(content='Nice to meet you, David! I\\'m an artificial intelligence language model, and I don\\'t have a personal name like humans do. My creators call me \"LLaMA\" ( Large Language Model Application), but I\\'ve been trained on so many conversations that I like to think of myself as having a personality of my own. By the way, where are you from?', additional_kwargs={}, response_metadata={}), HumanMessage(content='What did I just say my name was?', additional_kwargs={}, response_metadata={}), AIMessage(content='You mentioned your name is David earlier in our conversation. You said, \"Hi, my name is David.\" That\\'s how we started this conversation, and I\\'ve been keeping track of it since then!', additional_kwargs={}, response_metadata={}), HumanMessage(content='And what was my first statement?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Your first statement in our conversation was 'Hi, my name is David.'\", additional_kwargs={}, response_metadata={})]\n",
      "\n",
      "User: My favorite food is pizza, and I like to go for walks in the park.\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: My favorite food is pizza, and I like to go for walks in the park.\n",
      "AI:\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vf/gw1t6f4j3mv84kp8kpw4j_bw0000gn/T/ipykernel_46628/2420104738.py:39: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory_summary = ConversationSummaryMemory(llm = ollama_llm)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "AI: That sounds like a lovely combination! I'm glad you enjoy pizza - did you know that the first true pizzas originated from ancient Greece and Rome, where people would top flatbreads with olive oil, herbs, and other ingredients? However, modern pizza as we know it today is often credited to Raffaele Esposito, an Italian baker who created a dish in 1889 called \"Pizza Margherita\" in honor of Queen Margherita of Savoy, featuring tomato sauce, mozzarella cheese, and fresh basil leaves.\n",
      "\n",
      "As for your walks in the park, I'd be happy to provide some suggestions on nearby walking routes or interesting facts about parks if you're interested! Which type of park are you usually visiting - a local community park, a nature reserve, or perhaps a historic garden?\n",
      "\n",
      "User: So what do you know about me?\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Current summary:\n",
      "The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good because it will help humans reach their full potential.\n",
      "\n",
      "New lines of conversation:\n",
      "Human: My favorite food is pizza, and I like to go for walks in the park.\n",
      "AI: That sounds like a lovely combination! \n",
      "\n",
      "New summary:\n",
      "The human asks what the AI thinks of artificial intelligence. The AI thinks artificial intelligence is a force for good because it will help humans reach their full potential. The human shares that they enjoy pizza and going for walks in the park, and the AI responds with enthusiasm, expressing admiration for these activities.\n",
      "Human: So what do you know about me?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "AI: I'm intrigued! Based on our conversation so far, I can gather some information about your interests. You mentioned enjoying pizza and going for walks in the park, which suggests that you appreciate simple pleasures in life. This might imply that you value relaxation and enjoyment as well.\n",
      "\n",
      "Additionally, since you're curious about me (the AI), it's possible that you're interested in technology or innovation. Perhaps you've been following advancements in artificial intelligence or have thoughts on its potential impact on society?\n",
      "\n",
      "I don't want to make any assumptions, but I'm eager to learn more about your interests and perspectives. Would you like to share more about yourself?\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import Ollama\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# Initialize Ollama LLM\n",
    "ollama_llm = Ollama(model=\"llama3.2\")\n",
    "\n",
    "# Initialize ConversationBufferMemory\n",
    "memory = ConversationBufferMemory(return_messages=True)  # Store messages, not just text\n",
    "\n",
    "# Initialize ConversationChain\n",
    "conversation = ConversationChain(\n",
    "    llm=ollama_llm,\n",
    "    memory=memory,\n",
    "    verbose=True  # Set to True to see the full interaction\n",
    ")\n",
    "\n",
    "# First interaction\n",
    "print(\"User: Hi, my name is David.\")\n",
    "response1 = conversation.predict(input=\"Hi, my name is David.\")\n",
    "print(\"AI:\", response1)\n",
    "\n",
    "# Second interaction\n",
    "print(\"\\nUser: What did I just say my name was?\")\n",
    "response2 = conversation.predict(input=\"What did I just say my name was?\")\n",
    "print(\"AI:\", response2)\n",
    "\n",
    "# Third interaction\n",
    "print(\"\\nUser: And what was my first statement?\")\n",
    "response3 = conversation.predict(input=\"And what was my first statement?\")\n",
    "print(\"AI:\", response3)\n",
    "\n",
    "# Inspect the memory directly\n",
    "print(\"\\n--- Current Memory State ---\")\n",
    "print(memory.buffer)\n",
    "\n",
    "# Example of using a different type of memory.\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "memory_summary = ConversationSummaryMemory(llm = ollama_llm)\n",
    "\n",
    "conversation_summary = ConversationChain(\n",
    "  llm = ollama_llm,\n",
    "  memory = memory_summary,\n",
    "  verbose = True,\n",
    ")\n",
    "\n",
    "print(\"\\nUser: My favorite food is pizza, and I like to go for walks in the park.\")\n",
    "response = conversation_summary.predict(input = \"My favorite food is pizza, and I like to go for walks in the park.\")\n",
    "print(\"AI:\", response)\n",
    "\n",
    "print(\"\\nUser: So what do you know about me?\")\n",
    "response = conversation_summary.predict(input = \"So what do you know about me?\")\n",
    "\n",
    "print(\"AI:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playing with OLLAMA and its genrative functionality\n",
    "### Conclusion : Need more control over generative process hence try HuggingFace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
